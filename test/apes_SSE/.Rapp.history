N
phy=mammstr
lambda = par[1]#
	mu = par[2]#
	# Extract the branching times from the tree#
	x <- c(NA, branching.times(phy))#
	# Get N, the number of tips#
	N = length(phy$tip.labels)#
	# r, Net Diversification Rate#
	r = lambda - mu#
	# a, Relative Death Rate#
	a = mu / lambda#
	# This is the log of Equation 21 from Nee et al. 1994#
	lnL = (lfactorial(N - 1) + (N - 2) * log(r) + r * sum(x[3:N]) + #
            N * log(1 - a) - 2 * sum(log(exp(r * x[2:N]) - a)))
lnL
lfactorial(N - 1)
N
length(phy$tip.label)
length(phy$tip.labels)
bd <- function(par, phy)#
	{#
	# Read in the parameters#
	lambda = par[1]#
	mu = par[2]#
	# Extract the branching times from the tree#
	x <- c(NA, branching.times(phy))#
	# Get N, the number of tips#
	N = length(phy$tip.label)#
	# r, Net Diversification Rate#
	r = lambda - mu#
	# a, Relative Death Rate#
	a = mu / lambda#
	# This is the log of Equation 21 from Nee et al. 1994#
	lnL = (lfactorial(N - 1) + (N - 2) * log(r) + r * sum(x[3:N]) + #
            N * log(1 - a) - 2 * sum(log(exp(r * x[2:N]) - a)))#
	return(lnL)#
	} # END of bd function#
#
bd(par=c(0.01, 0.0), phy=mammstr)
bd <- function(par, phy)#
	{#
	# Read in the parameters#
	lambda = par[1]#
	mu = par[2]#
	# Extract the branching times from the tree#
	x <- c(NA, branching.times(phy))#
	# Get N, the number of tips#
	N = length(phy$tip.label)#
	# r, Net Diversification Rate#
	r = lambda - mu#
	# a, Relative Death Rate#
	a = mu / lambda#
	# This is the log of Equation 21 from Nee et al. 1994#
	lnL = (lfactorial(N - 1) + (N - 2) * log(r) + r * sum(x[3:N]) + #
            N * log(1 - a) - 2 * sum(log(exp(r * x[2:N]) - a)))#
	# Text message to screen#
	txt = paste0("lambda=", lambda, ", mu=", mu, ", lnL=", lnL)#
	cat(txt)#
	cat("\n")#
	return(lnL)#
	} # END of bd function#
#
bd(par=c(0.01, 0.0), phy=mammstr)
bd <- function(par, phy)#
	{#
	# Read in the parameters#
	lambda = par[1]#
	mu = par[2]#
	# Extract the branching times from the tree#
	x <- c(NA, branching.times(phy))#
	# Get N, the number of tips#
	N = length(phy$tip.label)#
	# r, Net Diversification Rate#
	r = lambda - mu#
	# a, Relative Death Rate#
	a = mu / lambda#
	# This is the log of Equation 21 from Nee et al. 1994#
	lnL = (lfactorial(N - 1) + (N - 2) * log(r) + r * sum(x[3:N]) + #
            N * log(1 - a) - 2 * sum(log(exp(r * x[2:N]) - a)))#
	# Text message to screen#
	txt = paste0("lambda=", lambda, ", mu=", mu, ", lnL=", round(lnL, digits=6))#
	cat(txt)#
	cat("\n")#
	return(lnL)#
	} # END of bd function#
#
bd(par=c(0.01, 0.0), phy=mammstr)
lambda_values = seq(0.001, 1, by=0.001)#
mu_values = seq(0.001, 1, by=0.001)#
#
# Create a list of lnLs#
lnLs = NULL#
for (i in 1:length(lambda_values))#
	{#
	par = c(lambda_values[i], 0.0)#
	lnL = bd(par=par, phy=mammstr)#
	lnLs = c(lnLs, lnL)#
	}
plot(x=lambda_values, y=lnLs, xlab="Speciation rate", ylab="log-likelihood")
fn = "https://covid.ourworldindata.org/data/owid-covid-data.csv"#
data = read.csv(file=fn)#
dim(data)#
########################################################
# Exploring a data table#
# #
# A lot of work in R is done with "data frames". These#
# are just tables of data.#
##
# Data tables can be huge, so there are bunch of functions #
# that let you explore what is in a data table.#
# #
# Try these commands. Figure out what each one does.#
##
# Remember, you can do e.g. "?dim" to get the help page#
# for the function.#
# #
########################################################
#
dim(data)#
nrow(data)#
ncol(data)#
head(data)#
tail(data)#
names(data)#
class(data)#
########################################################
# Accessing particular columns of data#
########################################################
# With R data.frames, you can access individual columns#
# using "$columnname"#
#
# However, this dataset is much huger than in May 2020, such #
# that now it causes problems when printed to the screen#
# (so, DON'T try to print the whole dataset to screen, you#
# may get a frozen screen).#
#
# We can use the function "unique" to #
# get just one entry for each country#
#
unique(data$location)#
########################################################
# Plotting the data#
# Below, I have a script to plot the number of new#
# cases, per day, for New Zealand#
########################################################
#
# Specify the country name#
country_name = "New Zealand"#
#
# The "==" symbol gives TRUE or FALSE#
# Here, we are seeing, for each row, if it matches #
# "New Zealand".#
##
TF = data$location == country_name#
#
# R treats FALSE as 0, and TRUE as 1, so doing sum(TF)#
# gives us the number of TRUE values#
sum(TF)#
#
# We can subset the table "data" to a smaller table, "d",#
# by using square brackets. Square brackets indicate which #
# [rows,columns] of the data table you want, where blank =#
# "give everything".#
##
# So, this command subsets the table "data" to all rows#
# where location == "New Zealand"#
d = data[TF,]#
#
# How big is the new table?#
dim(d)#
#
# Let's plot date versus cases, for New Zealand#
plot(x=d$date, y=d$new_cases)#
#
# Why didn't that work?  Look at d$date:#
d$date#
class(d$date)#
#
# If the "class" of the data is "character", and if R can't easily #
# convert it to numbers (class "numeric"), then R gives an error.#
#
# We can fix this by forcing R to read the dates as datatype date,#
# using the "as.Date" function:#
#
plot(x=as.Date(d$date), y=d$new_cases)#
# You can see that R has plotted the dates on the x-axis, and #
# the new cases on the y-axis. R uses a bunch of defaults in its#
# plots, but we can do better.#
##
# I am just going to make a better plot. It is up to you to #
# READ this code, and look through the relevant help files,#
# ?plot and ?par, #
# to learn what these various inputs do.#
#
plot(x=as.Date(d$date), y=d$new_cases, xlab="Date", ylab="Number of new cases", pch=1, col="red", cex=0.5)#
titletxt = paste0("Number of new cases per day in: ", country_name)#
title(titletxt)#
lines(x=as.Date(d$date), y=d$new_cases, lty="solid", lwd=2, col="red")#
# Hmm, that line has some gaps, suggesting there are some days with no data.#
# What is going on?#
unique(d$new_cases)#
#
# Let's try replacing the "NA" values with 0:#
TF = is.na(d$new_cases)#
d$new_cases[TF] = 0#
#
# And, plot again:#
plot(x=as.Date(d$date), y=d$new_cases, xlab="Date", ylab="Number of new cases", pch=1, col="red", cex=0.5)#
titletxt = paste0("Number of new cases per day in: ", country_name)#
title(titletxt)#
lines(x=as.Date(d$date), y=d$new_cases, lty="solid", lwd=2, col="red")
TF = data$location == country_name#
d2 = data[TF, ]#
TF = is.na(d2$new_cases)#
d2$new_cases[TF] = 0#
#
# Keep a count of active cases, and recovered cases#
active_cases = rep(0.0, times=nrow(d2))#
recovered_cases = rep(0.0, times=nrow(d2))#
#
# Here, we do a "for-loop". It will leap through every row#
# of the table, and do a calculation.#
##
# NOTE: to make a for-loop run, you have to run#
# the WHOLE thing, from the "for" to the final closing#
# bracket "}", at once!#
# #
for (i in 1:nrow(d2))#
	{#
	# Keep track of the starting row number#
	if (i <= 14)#
		{#
		startrow = 1#
		endrow = i#
		recovered_cases[i] = 0#
		} else {#
		startrow = startrow + 1#
		endrow = i#
		sum_of_recovered_cases = sum(d2$new_cases[1:(startrow-1)])#
		recovered_cases[i] = sum_of_recovered_cases#
		}#
	# Add up the last 14 days of active cases#
	sum_of_active_cases_14days = sum(d2$new_cases[startrow:endrow])#
	# Store the result:#
	active_cases[i] = sum_of_active_cases_14days#
	}#
#
# Plot the active cases, AND recovered cases#
# Plot a plot of white dots (just to set the plot scale)#
xvals = c(as.Date(d$date), as.Date(d$date))#
yvals = c(active_cases, recovered_cases)#
plot(x=xvals, y=yvals, xlab="Date", ylab="Number of cases", pch=".", col="white", cex=0.5)#
titletxt = paste0("Number of active & recovered cases in: ", country_name)#
title(titletxt)
unique(data$location)#
########################################################
# Plotting the data#
# Below, I have a script to plot the number of new#
# cases, per day, for New Zealand#
########################################################
#
# Specify the country name#
country_name = "New Zealand"#
#
# The "==" symbol gives TRUE or FALSE#
# Here, we are seeing, for each row, if it matches #
# "New Zealand".#
##
TF = data$location == country_name#
#
# R treats FALSE as 0, and TRUE as 1, so doing sum(TF)#
# gives us the number of TRUE values#
sum(TF)#
#
# We can subset the table "data" to a smaller table, "d",#
# by using square brackets. Square brackets indicate which #
# [rows,columns] of the data table you want, where blank =#
# "give everything".#
##
# So, this command subsets the table "data" to all rows#
# where location == "New Zealand"#
d = data[TF,]#
#
# How big is the new table?#
dim(d)#
#
# Let's plot date versus cases, for New Zealand#
plot(x=d$date, y=d$new_cases)#
#
# Why didn't that work?  Look at d$date:#
d$date#
class(d$date)#
#
# If the "class" of the data is "character", and if R can't easily #
# convert it to numbers (class "numeric"), then R gives an error.#
#
# We can fix this by forcing R to read the dates as datatype date,#
# using the "as.Date" function:#
#
plot(x=as.Date(d$date), y=d$new_cases)#
# You can see that R has plotted the dates on the x-axis, and #
# the new cases on the y-axis. R uses a bunch of defaults in its#
# plots, but we can do better.#
##
# I am just going to make a better plot. It is up to you to #
# READ this code, and look through the relevant help files,#
# ?plot and ?par, #
# to learn what these various inputs do.#
#
plot(x=as.Date(d$date), y=d$new_cases, xlab="Date", ylab="Number of new cases", pch=1, col="red", cex=0.5)#
titletxt = paste0("Number of new cases per day in: ", country_name)#
title(titletxt)#
lines(x=as.Date(d$date), y=d$new_cases, lty="solid", lwd=2, col="red")#
# Hmm, that line has some gaps, suggesting there are some days with no data.#
# What is going on?#
unique(d$new_cases)#
#
# Let's try replacing the "NA" values with 0:#
TF = is.na(d$new_cases)#
d$new_cases[TF] = 0#
#
# And, plot again:#
plot(x=as.Date(d$date), y=d$new_cases, xlab="Date", ylab="Number of new cases", pch=1, col="red", cex=0.5)#
titletxt = paste0("Number of new cases per day in: ", country_name)#
title(titletxt)#
lines(x=as.Date(d$date), y=d$new_cases, lty="solid", lwd=2, col="red")#
########################################################
# That looks a little better, but it still seems like quite a jagged plot.#
# Some of this is randomness in the daily counts, and some is #
# probably errors in the data.#
##
# We might get a clearer picture if we had a smoother version of the data.#
##
# Also, for SIR models, we will want to know how many COVID#
# cases are active at any particular point in time.#
##
# The conventional wisdom is that once someone gets COVID, #
# they are infectious for about 14 days. This may not be#
# perfectly accurate, but it is a reasonable starting point.#
##
# The code below keeps a running count of cases for the #
# last 14 days. It also counts how many have recovered.#
########################################################
#
# Subset to your country, replace "NA" values with 0:#
TF = data$location == country_name#
d2 = data[TF, ]#
TF = is.na(d2$new_cases)#
d2$new_cases[TF] = 0#
#
# Keep a count of active cases, and recovered cases#
active_cases = rep(0.0, times=nrow(d2))#
recovered_cases = rep(0.0, times=nrow(d2))#
#
# Here, we do a "for-loop". It will leap through every row#
# of the table, and do a calculation.#
##
# NOTE: to make a for-loop run, you have to run#
# the WHOLE thing, from the "for" to the final closing#
# bracket "}", at once!#
# #
for (i in 1:nrow(d2))#
	{#
	# Keep track of the starting row number#
	if (i <= 14)#
		{#
		startrow = 1#
		endrow = i#
		recovered_cases[i] = 0#
		} else {#
		startrow = startrow + 1#
		endrow = i#
		sum_of_recovered_cases = sum(d2$new_cases[1:(startrow-1)])#
		recovered_cases[i] = sum_of_recovered_cases#
		}#
	# Add up the last 14 days of active cases#
	sum_of_active_cases_14days = sum(d2$new_cases[startrow:endrow])#
	# Store the result:#
	active_cases[i] = sum_of_active_cases_14days#
	}#
#
# Plot the active cases, AND recovered cases#
# Plot a plot of white dots (just to set the plot scale)#
xvals = c(as.Date(d$date), as.Date(d$date))#
yvals = c(active_cases, recovered_cases)#
plot(x=xvals, y=yvals, xlab="Date", ylab="Number of cases", pch=".", col="white", cex=0.5)#
titletxt = paste0("Number of active & recovered cases in: ", country_name)#
title(titletxt)
# Add points & line for active cases#
points(x=as.Date(d$date), y=active_cases, pch=1, col="red", cex=0.6)#
lines(x=as.Date(d$date), y=active_cases, lty="solid", lwd=2, col="red2")#
#
# Add points & line for recovered cases#
points(x=as.Date(d$date), y=recovered_cases, pch=2, col="green3", cex=0.5)#
lines(x=as.Date(d$date), y=recovered_cases, lty="solid", lwd=2, col="green4")#
#
# Add vertical line with abline, and text, for Level 4 lockdown start#
lockdown_time = as.Date("2020-03-26")#
abline(v=lockdown_time, col="black", lty="dashed", lwd=2)#
text(x=lockdown_time, y=0.99*max(yvals), labels="lockdown begins", pos=2, srt=90, cex=0.7)#
#
# Add vertical line with abline, and text, for Level 4 lockdown end#
lockdown_time = as.Date("2020-04-28")#
abline(v=lockdown_time, col="black", lty="dashed", lwd=2)#
text(x=lockdown_time, y=0.99*max(yvals), labels="lockdown ends", pos=2, srt=90, cex=0.7)#
# Add horizontal line with abline#
threshold = 1000#
abline(h=threshold, col="grey", lty="dotted", lwd=2)#
text_x = min(xvals) + 0.05*(max(xvals) - min(xvals))#
text(x=text_x, y=threshold, labels="(1000 cases)", pos=3, srt=0, cex=0.7, col="grey")#
#
# Add legend#
legend(x="right", legend=c("active cases", "recovered"), col=c("red2", "green4"), lwd=c(2,2), pch=c(1,2))
TF = data$location == country_name#
d2 = data[TF, ]#
TF = is.na(d2$new_cases)#
d2$new_cases[TF] = 0#
#
# Keep a count of active cases, and recovered cases#
active_cases = rep(0.0, times=nrow(d2))#
recovered_cases = rep(0.0, times=nrow(d2))#
#
# Here, we do a "for-loop". It will leap through every row#
# of the table, and do a calculation.#
##
# NOTE: to make a for-loop run, you have to run#
# the WHOLE thing, from the "for" to the final closing#
# bracket "}", at once!#
# #
for (i in 1:nrow(d2))#
	{#
	# Keep track of the starting row number#
	if (i <= 14)#
		{#
		startrow = 1#
		endrow = i#
		recovered_cases[i] = 0#
		} else {#
		startrow = startrow + 1#
		endrow = i#
		sum_of_recovered_cases = sum(d2$new_cases[1:(startrow-1)])#
		recovered_cases[i] = sum_of_recovered_cases#
		}#
	# Add up the last 14 days of active cases#
	sum_of_active_cases_14days = sum(d2$new_cases[startrow:endrow])#
	# Store the result:#
	active_cases[i] = sum_of_active_cases_14days#
	}#
#
# Plot the active cases, AND recovered cases#
# Plot a plot of white dots (just to set the plot scale)#
xvals = c(as.Date(d$date), as.Date(d$date))#
yvals = c(active_cases, recovered_cases)#
plot(x=xvals, y=yvals, xlab="Date", ylab="Number of cases", pch=".", col="white", cex=0.5)#
titletxt = paste0("Number of active & recovered cases in: ", country_name)#
title(titletxt)
# Add points & line for active cases#
points(x=as.Date(d$date), y=active_cases, pch=1, col="red", cex=0.6)#
lines(x=as.Date(d$date), y=active_cases, lty="solid", lwd=2, col="red2")#
#
# Add points & line for recovered cases#
points(x=as.Date(d$date), y=recovered_cases, pch=2, col="green3", cex=0.5)#
lines(x=as.Date(d$date), y=recovered_cases, lty="solid", lwd=2, col="green4")#
#
# Add vertical line with abline, and text, for Level 4 lockdown start#
lockdown_time = as.Date("2020-03-26")#
abline(v=lockdown_time, col="black", lty="dashed", lwd=2)#
text(x=lockdown_time, y=0.99*max(yvals), labels="lockdown begins", pos=2, srt=90, cex=0.7)#
#
# Add vertical line with abline, and text, for Level 4 lockdown end#
lockdown_time = as.Date("2020-04-28")#
abline(v=lockdown_time, col="black", lty="dashed", lwd=2)#
text(x=lockdown_time, y=0.99*max(yvals), labels="lockdown ends", pos=2, srt=90, cex=0.7)
# Add horizontal line with abline#
threshold = 1000#
abline(h=threshold, col="grey", lty="dotted", lwd=2)#
text_x = min(xvals) + 0.05*(max(xvals) - min(xvals))#
text(x=text_x, y=threshold, labels="(1000 cases)", pos=3, srt=0, cex=0.7, col="grey")#
#
# Add legend#
legend(x="right", legend=c("active cases", "recovered"), col=c("red2", "green4"), lwd=c(2,2), pch=c(1,2))
unique(data$location)#
########################################################
# Plotting the data#
# Below, I have a script to plot the number of new#
# cases, per day, for New Zealand#
########################################################
#
# Specify the country name#
country_name = "New Zealand"#
#
# Specify the end-date for your analysis#
# (This script was initially set up for the first 2020 wave. If you #
#  change the end-date, plots will look different!)#
end_date = as.date("2020-06-30")#
#
# The "==" symbol gives TRUE or FALSE (I call the results TF)#
# Here, we are seeing, for each row, if it matches #
# "New Zealand".#
##
TF1 = data$location == country_name#
#
# R treats FALSE as 0, and TRUE as 1, so doing sum(TF1)#
# gives us the number of TRUE values#
sum(TF1)#
#
# Let's also subset to end-date#
TF2 = as.date(d$date) <= end_date#
TF2#
#
# Let's combine TF1 and TF2 into a single list of TRUEs/FALSEs.#
# We will keep rows that are TRUE for both#
TF = (TF1 + TF2) == 2#
#
# We can subset the table "data" to a smaller table, "d",#
# by using square brackets. Square brackets indicate which #
# [rows,columns] of the data table you want, where blank =#
# "give everything".#
##
# So, this command subsets the table "data" to all rows#
# where location == "New Zealand"#
d = data[TF,]#
#
# How big is the new table?#
dim(d)#
#
# Let's plot date versus cases, for New Zealand#
plot(x=d$date, y=d$new_cases)#
#
# Why didn't that work?  Look at d$date:#
d$date#
class(d$date)#
#
# If the "class" of the data is "character", and if R can't easily #
# convert it to numbers (class "numeric"), then R gives an error.#
#
# We can fix this by forcing R to read the dates as datatype date,#
# using the "as.Date" function:#
#
plot(x=as.Date(d$date), y=d$new_cases)
TF2 = as.Date(d$date) <= end_date#
TF2#
#
# Let's combine TF1 and TF2 into a single list of TRUEs/FALSEs.#
# We will keep rows that are TRUE for both#
TF = (TF1 + TF2) == 2#
#
# We can subset the table "data" to a smaller table, "d",#
# by using square brackets. Square brackets indicate which #
# [rows,columns] of the data table you want, where blank =#
# "give everything".#
##
# So, this command subsets the table "data" to all rows#
# where location == "New Zealand"#
d = data[TF,]#
#
# How big is the new table?#
dim(d)#
#
# Let's plot date versus cases, for New Zealand#
plot(x=d$date, y=d$new_cases)#
#
# Why didn't that work?  Look at d$date:#
d$date#
class(d$date)#
#
# If the "class" of the data is "character", and if R can't easily #
# convert it to numbers (class "numeric"), then R gives an error.#
#
# We can fix this by forcing R to read the dates as datatype date,#
# using the "as.Date" function:#
#
plot(x=as.Date(d$date), y=d$new_cases)
sum(TF1)
sum(TF2)
TF2 = as.Date(d$date) <= end_date
country_name = "New Zealand"#
#
# Specify the end-date for your analysis#
# (This script was initially set up for the first 2020 wave. If you #
#  change the end-date, plots will look different!)#
end_date = as.Date("2020-06-30")#
#
# The "==" symbol gives TRUE or FALSE (I call the results TF)#
# Here, we are seeing, for each row, if it matches #
# "New Zealand".#
##
TF1 = data$location == country_name#
#
# R treats FALSE as 0, and TRUE as 1, so doing sum(TF1)#
# gives us the number of TRUE values#
sum(TF1)#
#
# Let's also subset to end-date#
TF2 = as.Date(d$date) <= end_date#
TF2#
#
# Let's combine TF1 and TF2 into a single list of TRUEs/FALSEs.#
# We will keep rows that are TRUE for both#
TF = (TF1 + TF2) == 2#
#
# We can subset the table "data" to a smaller table, "d",#
# by using square brackets. Square brackets indicate which #
# [rows,columns] of the data table you want, where blank =#
# "give everything".#
##
# So, this command subsets the table "data" to all rows#
# where location == "New Zealand"#
d = data[TF,]#
#
# How big is the new table?#
dim(d)#
#
# Let's plot date versus cases, for New Zealand#
plot(x=d$date, y=d$new_cases)#
#
# Why didn't that work?  Look at d$date:#
d$date#
class(d$date)#
#
# If the "class" of the data is "character", and if R can't easily #
# convert it to numbers (class "numeric"), then R gives an error.#
#
# We can fix this by forcing R to read the dates as datatype date,#
# using the "as.Date" function:#
#
plot(x=as.Date(d$date), y=d$new_cases)
sum(TF)
sum(TF2)
sum(TF1)
unique(data$location)#
########################################################
# Plotting the data#
# Below, I have a script to plot the number of new#
# cases, per day, for New Zealand#
########################################################
#
# Specify the country name#
country_name = "New Zealand"#
#
# Specify the end-date for your analysis#
# (This script was initially set up for the first 2020 wave. If you #
#  change the end-date, plots will look different!)#
end_date = as.Date("2020-06-30")#
#
# The "==" symbol gives TRUE or FALSE (I call the results TF)#
# Here, we are seeing, for each row, if it matches #
# "New Zealand".#
##
TF1 = data$location == country_name#
#
# R treats FALSE as 0, and TRUE as 1, so doing sum(TF1)#
# gives us the number of TRUE values#
sum(TF1)#
#
# Let's also subset to end-date#
TF2 = as.Date(data$date) <= end_date#
TF2#
#
# Let's combine TF1 and TF2 into a single list of TRUEs/FALSEs.#
# We will keep rows that are TRUE for both#
TF = (TF1 + TF2) == 2#
#
# We can subset the table "data" to a smaller table, "d",#
# by using square brackets. Square brackets indicate which #
# [rows,columns] of the data table you want, where blank =#
# "give everything".#
##
# So, this command subsets the table "data" to all rows#
# where location == "New Zealand"#
d = data[TF,]#
#
# How big is the new table?#
dim(d)#
#
# Let's plot date versus cases, for New Zealand#
plot(x=d$date, y=d$new_cases)#
#
# Why didn't that work?  Look at d$date:#
d$date#
class(d$date)#
#
# If the "class" of the data is "character", and if R can't easily #
# convert it to numbers (class "numeric"), then R gives an error.#
#
# We can fix this by forcing R to read the dates as datatype date,#
# using the "as.Date" function:#
#
plot(x=as.Date(d$date), y=d$new_cases)
plot(x=as.Date(d$date), y=d$new_cases, xlab="Date", ylab="Number of new cases", pch=1, col="red", cex=0.5)#
titletxt = paste0("Number of new cases per day in: ", country_name)#
title(titletxt)#
lines(x=as.Date(d$date), y=d$new_cases, lty="solid", lwd=2, col="red")#
# Hmm, that line has some gaps, suggesting there are some days with no data.#
# What is going on?#
unique(d$new_cases)#
#
# Let's try replacing the "NA" values with 0:#
TF = is.na(d$new_cases)#
d$new_cases[TF] = 0#
#
# And, plot again:#
plot(x=as.Date(d$date), y=d$new_cases, xlab="Date", ylab="Number of new cases", pch=1, col="red", cex=0.5)#
titletxt = paste0("Number of new cases per day in: ", country_name)#
title(titletxt)#
lines(x=as.Date(d$date), y=d$new_cases, lty="solid", lwd=2, col="red")
# Subset to your country, replace "NA" values with 0:#
TF = data$location == country_name#
d2 = data[TF, ]#
TF = is.na(d2$new_cases)#
d2$new_cases[TF] = 0#
#
# Keep a count of active cases, and recovered cases#
active_cases = rep(0.0, times=nrow(d2))#
recovered_cases = rep(0.0, times=nrow(d2))#
#
# Here, we do a "for-loop". It will leap through every row#
# of the table, and do a calculation.#
##
# NOTE: to make a for-loop run, you have to run#
# the WHOLE thing, from the "for" to the final closing#
# bracket "}", at once!#
# #
for (i in 1:nrow(d2))#
	{#
	# Keep track of the starting row number#
	if (i <= 14)#
		{#
		startrow = 1#
		endrow = i#
		recovered_cases[i] = 0#
		} else {#
		startrow = startrow + 1#
		endrow = i#
		sum_of_recovered_cases = sum(d2$new_cases[1:(startrow-1)])#
		recovered_cases[i] = sum_of_recovered_cases#
		}#
	# Add up the last 14 days of active cases#
	sum_of_active_cases_14days = sum(d2$new_cases[startrow:endrow])#
	# Store the result:#
	active_cases[i] = sum_of_active_cases_14days#
	}#
#
# Plot the active cases, AND recovered cases#
# Plot a plot of white dots (just to set the plot scale)#
xvals = c(as.Date(d$date), as.Date(d$date))#
yvals = c(active_cases, recovered_cases)#
plot(x=xvals, y=yvals, xlab="Date", ylab="Number of cases", pch=".", col="white", cex=0.5)#
titletxt = paste0("Number of active & recovered cases in: ", country_name)#
title(titletxt)#
#
# Notes:#
# for help on plots, ?plot#
# for help on point characters (pch), ?points#
# for other graphical parameters (e.g. lty = line type), ?par#
#
# Add points & line for active cases#
points(x=as.Date(d$date), y=active_cases, pch=1, col="red", cex=0.6)#
lines(x=as.Date(d$date), y=active_cases, lty="solid", lwd=2, col="red2")#
#
# Add points & line for recovered cases#
points(x=as.Date(d$date), y=recovered_cases, pch=2, col="green3", cex=0.5)#
lines(x=as.Date(d$date), y=recovered_cases, lty="solid", lwd=2, col="green4")#
#
# Add vertical line with abline, and text, for Level 4 lockdown start#
lockdown_time = as.Date("2020-03-26")#
abline(v=lockdown_time, col="black", lty="dashed", lwd=2)#
text(x=lockdown_time, y=0.99*max(yvals), labels="lockdown begins", pos=2, srt=90, cex=0.7)#
#
# Add vertical line with abline, and text, for Level 4 lockdown end#
lockdown_time = as.Date("2020-04-28")#
abline(v=lockdown_time, col="black", lty="dashed", lwd=2)#
text(x=lockdown_time, y=0.99*max(yvals), labels="lockdown ends", pos=2, srt=90, cex=0.7)#
# Add horizontal line with abline#
threshold = 1000#
abline(h=threshold, col="grey", lty="dotted", lwd=2)#
text_x = min(xvals) + 0.05*(max(xvals) - min(xvals))#
text(x=text_x, y=threshold, labels="(1000 cases)", pos=3, srt=0, cex=0.7, col="grey")#
#
# Add legend#
legend(x="right", legend=c("active cases", "recovered"), col=c("red2", "green4"), lwd=c(2,2), pch=c(1,2))
unique(data$location)#
########################################################
# Plotting the data#
# Below, I have a script to plot the number of new#
# cases, per day, for New Zealand#
########################################################
#
# Specify the country name#
country_name = "New Zealand"#
#
# Specify the end-date for your analysis#
# (This script was initially set up for the first 2020 wave. If you #
#  change the end-date, plots will look different!)#
end_date = as.Date("2020-06-30")#
#
# The "==" symbol gives TRUE or FALSE (I call the results TF)#
# Here, we are seeing, for each row, if it matches #
# "New Zealand".#
##
TF1 = data$location == country_name#
#
# R treats FALSE as 0, and TRUE as 1, so doing sum(TF1)#
# gives us the number of TRUE values#
sum(TF1)#
#
# Let's also subset to end-date#
TF2 = as.Date(data$date) <= end_date#
TF2#
#
# Let's combine TF1 and TF2 into a single list of TRUEs/FALSEs.#
# We will keep rows that are TRUE for both#
TF = (TF1 + TF2) == 2#
#
# We can subset the table "data" to a smaller table, "d",#
# by using square brackets. Square brackets indicate which #
# [rows,columns] of the data table you want, where blank =#
# "give everything".#
##
# So, this command subsets the table "data" to all rows#
# where location == "New Zealand"#
d = data[TF,]#
#
# How big is the new table?#
dim(d)#
#
# Let's plot date versus cases, for New Zealand#
plot(x=d$date, y=d$new_cases)#
#
# Why didn't that work?  Look at d$date:#
d$date#
class(d$date)#
#
# If the "class" of the data is "character", and if R can't easily #
# convert it to numbers (class "numeric"), then R gives an error.#
#
# We can fix this by forcing R to read the dates as datatype date,#
# using the "as.Date" function:#
#
plot(x=as.Date(d$date), y=d$new_cases)#
#
# We can also plot the cases, with the y-axis in log space.#
# This is useful when the counts range from dozens (e.g. the 2020#
# NZ wave) to tens of thousands (e.g. the 2022 wave).#
plot(x=as.Date(d$date), y=d$new_cases, log="y")
nzdata = data[TF1,]; nzdata$date = as.Date(nzdata$date)
fn = "https://covid.ourworldindata.org/data/owid-covid-data.csv"#
data = read.csv(file=fn)#
dim(data)#
########################################################
# Exploring a data table#
# #
# A lot of work in R is done with "data frames". These#
# are just tables of data.#
##
# Data tables can be huge, so there are bunch of functions #
# that let you explore what is in a data table.#
# #
# Try these commands. Figure out what each one does.#
##
# Remember, you can do e.g. "?dim" to get the help page#
# for the function.#
# #
########################################################
#
dim(data)#
nrow(data)#
ncol(data)#
head(data)#
tail(data)#
names(data)#
class(data)#
########################################################
# Accessing particular columns of data#
########################################################
# With R data.frames, you can access individual columns#
# using "$columnname"#
#
# However, this dataset is much huger than in May 2020, such #
# that now it causes problems when printed to the screen#
# (so, DON'T try to print the whole dataset to screen, you#
# may get a frozen screen).#
#
# We can use the function "unique" to #
# get just one entry for each country#
#
unique(data$location)#
########################################################
# Plotting the data#
# Below, I have a script to plot the number of new#
# cases, per day, for New Zealand#
########################################################
#
# Specify the country name#
country_name = "New Zealand"#
#
# Specify the end-date for your analysis#
# (This script was initially set up for the first 2020 wave. If you #
#  change the end-date, plots will look different!)#
end_date = as.Date("2020-06-30")#
#
# The "==" symbol gives TRUE or FALSE (I call the results TF)#
# Here, we are seeing, for each row, if it matches #
# "New Zealand".#
##
TF1 = data$location == country_name#
#
# R treats FALSE as 0, and TRUE as 1, so doing sum(TF1)#
# gives us the number of TRUE values#
sum(TF1)#
#
# Let's also subset to end-date#
TF2 = as.Date(data$date) <= end_date#
TF2#
#
# Let's combine TF1 and TF2 into a single list of TRUEs/FALSEs.#
# We will keep rows that are TRUE for both#
TF = (TF1 + TF2) == 2#
#
# We can subset the table "data" to a smaller table, "d",#
# by using square brackets. Square brackets indicate which #
# [rows,columns] of the data table you want, where blank =#
# "give everything".#
##
# So, this command subsets the table "data" to all rows#
# where location == "New Zealand"#
d = data[TF,]#
#
# (We will also keep all the NZ data for all dates)#
nzdata = data[TF1,]; nzdata$date = as.Date(nzdata$date)
fn = "https://covid.ourworldindata.org/data/owid-covid-data.csv"#
data = read.csv(file=fn)#
dim(data)#
########################################################
# Exploring a data table#
# #
# A lot of work in R is done with "data frames". These#
# are just tables of data.#
##
# Data tables can be huge, so there are bunch of functions #
# that let you explore what is in a data table.#
# #
# Try these commands. Figure out what each one does.#
##
# Remember, you can do e.g. "?dim" to get the help page#
# for the function.#
# #
########################################################
#
dim(data)#
nrow(data)#
ncol(data)#
head(data)#
tail(data)#
names(data)#
class(data)
unique(data$location)#
########################################################
# Plotting the data#
# Below, I have a script to plot the number of new#
# cases, per day, for New Zealand#
########################################################
#
# Specify the country name#
country_name = "New Zealand"#
#
# Specify the end-date for your analysis#
# (This script was initially set up for the first 2020 wave. If you #
#  change the end-date, plots will look different!)#
end_date = as.Date("2020-06-30")#
#
# The "==" symbol gives TRUE or FALSE (I call the results TF)#
# Here, we are seeing, for each row, if it matches #
# "New Zealand".#
##
TF1 = data$location == country_name#
#
# R treats FALSE as 0, and TRUE as 1, so doing sum(TF1)#
# gives us the number of TRUE values#
sum(TF1)#
#
# Let's also subset to end-date#
TF2 = as.Date(data$date) <= end_date#
sum(TF1)#
#
# Let's combine TF1 and TF2 into a single list of TRUEs/FALSEs.#
# We will keep rows that are TRUE for both#
TF = (TF1 + TF2) == 2
d = data[TF,]#
#
# (We will also keep all the NZ data for all dates)#
nzdata = data[TF1,]; nzdata$date = as.Date(nzdata$date)
dim(d)#
#
# Let's plot date versus cases, for New Zealand#
plot(x=d$date, y=d$new_cases)#
#
# Why didn't that work?  Look at d$date:#
d$date#
class(d$date)#
#
# If the "class" of the data is "character", and if R can't easily #
# convert it to numbers (class "numeric"), then R gives an error.#
#
# We can fix this by forcing R to read the dates as datatype date,#
# using the "as.Date" function:#
#
plot(x=as.Date(d$date), y=d$new_cases)#
#
# We can also plot the cases, with the y-axis in log space.#
# This is useful when the counts range from dozens (e.g. the 2020#
# NZ wave) to tens of thousands (e.g. the 2022 wave).#
plot(x=as.Date(d$date), y=d$new_cases, log="y")
plot(x=as.Date(nzdata$date), y=d$new_cases)#
plot(x=as.Date(nzdata$date), y=d$new_cases, log="y")
plot(x=as.Date(nzdata$date), y=nzdata$new_cases)#
plot(x=as.Date(nzdata$date), y=nzdata$new_cases, log="y")
xvals = c(as.Date(d$date), as.Date(d$date))#
yvals = c(active_cases, recovered_cases)#
plot(x=xvals, y=yvals, xlab="Date", ylab="Number of cases", pch=".", col="white", cex=0.5)#
titletxt = paste0("Number of active & recovered cases in: ", country_name)#
title(titletxt)#
#
plot(x=xvals, y=yvals, xlab="Date", ylab="Number of cases", pch=".", col="white", cex=0.5, log="y")#
titletxt = paste0("Number of active & recovered cases in: ", country_name, ("\ny-axis logged"))#
title(titletxt)
# Subset to your country, replace "NA" values with 0:#
TF = data$location == country_name#
d2 = data[TF, ]#
TF = is.na(d2$new_cases)#
d2$new_cases[TF] = 0#
#
# Keep a count of active cases, and recovered cases#
active_cases = rep(0.0, times=nrow(d2))#
recovered_cases = rep(0.0, times=nrow(d2))#
#
# Here, we do a "for-loop". It will leap through every row#
# of the table, and do a calculation.#
##
# NOTE: to make a for-loop run, you have to run#
# the WHOLE thing, from the "for" to the final closing#
# bracket "}", at once!#
# #
for (i in 1:nrow(d2))#
	{#
	# Keep track of the starting row number#
	if (i <= 14)#
		{#
		startrow = 1#
		endrow = i#
		recovered_cases[i] = 0#
		} else {#
		startrow = startrow + 1#
		endrow = i#
		sum_of_recovered_cases = sum(d2$new_cases[1:(startrow-1)])#
		recovered_cases[i] = sum_of_recovered_cases#
		}#
	# Add up the last 14 days of active cases#
	sum_of_active_cases_14days = sum(d2$new_cases[startrow:endrow])#
	# Store the result:#
	active_cases[i] = sum_of_active_cases_14days#
	}#
#
# Plot the active cases, AND recovered cases#
# Plot a plot of white dots (just to set the plot scale)#
xvals = c(as.Date(d$date), as.Date(d$date))#
yvals = c(active_cases, recovered_cases)#
plot(x=xvals, y=yvals, xlab="Date", ylab="Number of cases", pch=".", col="white", cex=0.5)#
titletxt = paste0("Number of active & recovered cases in: ", country_name)#
title(titletxt)#
#
plot(x=xvals, y=yvals, xlab="Date", ylab="Number of cases", pch=".", col="white", cex=0.5, log="y")#
titletxt = paste0("Number of active & recovered cases in: ", country_name, ("\ny-axis logged"))#
title(titletxt)
TF = data$location == country_name#
d2 = data[TF, ]#
TF = is.na(d2$new_cases)#
d2$new_cases[TF] = 0
active_cases = rep(0.0, times=nrow(d2))#
recovered_cases = rep(0.0, times=nrow(d2))#
#
# Here, we do a "for-loop". It will leap through every row#
# of the table, and do a calculation.#
##
# NOTE: to make a for-loop run, you have to run#
# the WHOLE thing, from the "for" to the final closing#
# bracket "}", at once!#
# #
for (i in 1:nrow(d2))#
	{#
	# Keep track of the starting row number#
	if (i <= 14)#
		{#
		startrow = 1#
		endrow = i#
		recovered_cases[i] = 0#
		} else {#
		startrow = startrow + 1#
		endrow = i#
		sum_of_recovered_cases = sum(d2$new_cases[1:(startrow-1)])#
		recovered_cases[i] = sum_of_recovered_cases#
		}#
	# Add up the last 14 days of active cases#
	sum_of_active_cases_14days = sum(d2$new_cases[startrow:endrow])#
	# Store the result:#
	active_cases[i] = sum_of_active_cases_14days#
	}
xvals = c(as.Date(d$date), as.Date(d$date))#
yvals = c(active_cases, recovered_cases)
plot(x=xvals, y=yvals, xlab="Date", ylab="Number of cases", pch=".", col="white", cex=0.5)#
titletxt = paste0("Number of active & recovered cases in: ", country_name)#
title(titletxt)
length(xvals)
length(yvals)
dim(d)
dim(d2)
xvals = c(as.Date(d2$date), as.Date(d2$date))#
yvals = c(active_cases, recovered_cases)#
plot(x=xvals, y=yvals, xlab="Date", ylab="Number of cases", pch=".", col="white", cex=0.5)#
titletxt = paste0("Number of active & recovered cases in: ", country_name)#
title(titletxt)
xvals = c(as.Date(d2$date), as.Date(d2$date))#
yvals = c(active_cases, recovered_cases)#
plot(x=xvals, y=yvals, xlab="Date", ylab="Number of cases", pch=".", col="white", cex=0.5)#
titletxt = paste0("Number of active & recovered cases in: ", country_name)#
title(titletxt)#
# Notes:#
# for help on plots, ?plot#
# for help on point characters (pch), ?points#
# for other graphical parameters (e.g. lty = line type), ?par#
#
# Add points & line for active cases#
points(x=as.Date(d$date), y=active_cases, pch=1, col="red", cex=0.6)#
lines(x=as.Date(d$date), y=active_cases, lty="solid", lwd=2, col="red2")#
#
# Add points & line for recovered cases#
points(x=as.Date(d$date), y=recovered_cases, pch=2, col="green3", cex=0.5)#
lines(x=as.Date(d$date), y=recovered_cases, lty="solid", lwd=2, col="green4")#
#
# Add vertical line with abline, and text, for Level 4 lockdown start#
lockdown_time = as.Date("2020-03-26")#
abline(v=lockdown_time, col="black", lty="dashed", lwd=2)#
text(x=lockdown_time, y=0.99*max(yvals), labels="lockdown begins", pos=2, srt=90, cex=0.7)#
#
# Add vertical line with abline, and text, for Level 4 lockdown end#
lockdown_time = as.Date("2020-04-28")#
abline(v=lockdown_time, col="black", lty="dashed", lwd=2)#
text(x=lockdown_time, y=0.99*max(yvals), labels="lockdown ends", pos=2, srt=90, cex=0.7)#
# Add horizontal line with abline#
threshold = 1000#
abline(h=threshold, col="grey", lty="dotted", lwd=2)#
text_x = min(xvals) + 0.05*(max(xvals) - min(xvals))#
text(x=text_x, y=threshold, labels="(1000 cases)", pos=3, srt=0, cex=0.7, col="grey")#
#
# Add legend#
legend(x="right", legend=c("active cases", "recovered"), col=c("red2", "green4"), lwd=c(2,2), pch=c(1,2))
points(x=as.Date(d2$date), y=active_cases, pch=1, col="red", cex=0.6)#
lines(x=as.Date(d2$date), y=active_cases, lty="solid", lwd=2, col="red2")#
#
# Add points & line for recovered cases#
points(x=as.Date(d2$date), y=recovered_cases, pch=2, col="green3", cex=0.5)#
lines(x=as.Date(d2$date), y=recovered_cases, lty="solid", lwd=2, col="green4")#
#
# Add vertical line with abline, and text, for Level 4 lockdown start#
lockdown_time = as.Date("2020-03-26")#
abline(v=lockdown_time, col="black", lty="dashed", lwd=2)#
text(x=lockdown_time, y=0.99*max(yvals), labels="lockdown begins", pos=2, srt=90, cex=0.7)#
#
# Add vertical line with abline, and text, for Level 4 lockdown end#
lockdown_time = as.Date("2020-04-28")#
abline(v=lockdown_time, col="black", lty="dashed", lwd=2)#
text(x=lockdown_time, y=0.99*max(yvals), labels="lockdown ends", pos=2, srt=90, cex=0.7)#
# Add horizontal line with abline#
threshold = 1000#
abline(h=threshold, col="grey", lty="dotted", lwd=2)#
text_x = min(xvals) + 0.05*(max(xvals) - min(xvals))#
text(x=text_x, y=threshold, labels="(1000 cases)", pos=3, srt=0, cex=0.7, col="grey")#
#
# Add legend#
legend(x="right", legend=c("active cases", "recovered"), col=c("red2", "green4"), lwd=c(2,2), pch=c(1,2))
xvals = c(as.Date(d2$date), as.Date(d2$date))#
yvals = c(active_cases, recovered_cases)#
plot(x=xvals, y=yvals, xlab="Date", ylab="Number of cases", pch=".", col="white", cex=0.5, log="y")#
titletxt = paste0("Number of active & recovered cases in: ", country_name)#
title(titletxt)#
# Notes:#
# for help on plots, ?plot#
# for help on point characters (pch), ?points#
# for other graphical parameters (e.g. lty = line type), ?par#
#
# Add points & line for active cases#
points(x=as.Date(d2$date), y=active_cases, pch=1, col="red", cex=0.6)#
lines(x=as.Date(d2$date), y=active_cases, lty="solid", lwd=2, col="red2")#
#
# Add points & line for recovered cases#
points(x=as.Date(d2$date), y=recovered_cases, pch=2, col="green3", cex=0.5)#
lines(x=as.Date(d2$date), y=recovered_cases, lty="solid", lwd=2, col="green4")#
#
# Add vertical line with abline, and text, for Level 4 lockdown start#
lockdown_time = as.Date("2020-03-26")#
abline(v=lockdown_time, col="black", lty="dashed", lwd=2)#
text(x=lockdown_time, y=0.99*max(yvals), labels="lockdown begins", pos=2, srt=90, cex=0.7)#
#
# Add vertical line with abline, and text, for Level 4 lockdown end#
lockdown_time = as.Date("2020-04-28")#
abline(v=lockdown_time, col="black", lty="dashed", lwd=2)#
text(x=lockdown_time, y=0.99*max(yvals), labels="lockdown ends", pos=2, srt=90, cex=0.7)#
# Add horizontal line with abline#
threshold = 1000#
abline(h=threshold, col="grey", lty="dotted", lwd=2)#
text_x = min(xvals) + 0.05*(max(xvals) - min(xvals))#
text(x=text_x, y=threshold, labels="(1000 cases)", pos=3, srt=0, cex=0.7, col="grey")#
#
# Add legend#
legend(x="right", legend=c("active cases", "recovered"), col=c("red2", "green4"), lwd=c(2,2), pch=c(1,2))
# (NON-LOG Y-AXIS VERSION)#
# Plot the active cases, AND recovered cases#
# Plot a plot of white dots (just to set the plot scale)#
xvals = c(as.Date(d2$date), as.Date(d2$date))#
yvals = c(active_cases, recovered_cases)#
plot(x=xvals, y=yvals, xlab="Date", ylab="Number of cases", pch=".", col="white", cex=0.5)#
titletxt = paste0("Number of active & recovered cases in: ", country_name)#
title(titletxt)#
# Notes:#
# for help on plots, ?plot#
# for help on point characters (pch), ?points#
# for other graphical parameters (e.g. lty = line type), ?par#
#
# Add points & line for active cases#
points(x=as.Date(d2$date), y=active_cases, pch=1, col="red", cex=0.6)#
lines(x=as.Date(d2$date), y=active_cases, lty="solid", lwd=2, col="red2")#
#
# Add points & line for recovered cases#
points(x=as.Date(d2$date), y=recovered_cases, pch=2, col="green3", cex=0.5)#
lines(x=as.Date(d2$date), y=recovered_cases, lty="solid", lwd=2, col="green4")#
#
# Add vertical line with abline, and text, for Level 4 lockdown start#
lockdown_time = as.Date("2020-03-26")#
abline(v=lockdown_time, col="black", lty="dashed", lwd=2)#
text(x=lockdown_time, y=0.99*max(yvals), labels="lockdown begins", pos=2, srt=90, cex=0.7)#
#
# Add vertical line with abline, and text, for Level 4 lockdown end#
lockdown_time = as.Date("2020-04-28")#
abline(v=lockdown_time, col="black", lty="dashed", lwd=2)#
text(x=lockdown_time, y=0.99*max(yvals), labels="lockdown ends", pos=2, srt=90, cex=0.7)#
# Add horizontal line with abline#
threshold = 1000#
abline(h=threshold, col="grey", lty="dotted", lwd=2)#
text_x = min(xvals) + 0.05*(max(xvals) - min(xvals))#
text(x=text_x, y=threshold, labels="(1000 cases)", pos=3, srt=0, cex=0.7, col="grey")#
#
# Add legend#
legend(x="right", legend=c("active cases", "recovered"), col=c("red2", "green4"), lwd=c(2,2), pch=c(1,2))
plot(x=as.Date(d$date), y=d$new_cases, xlab="Date", ylab="Number of new cases", pch=1, col="red", cex=0.5)#
titletxt = paste0("Number of new cases per day in: ", country_name)#
title(titletxt)#
lines(x=as.Date(d$date), y=d$new_cases, lty="solid", lwd=2, col="red")
xvals = c(as.Date(d$date), as.Date(d$date))#
yvals = c(active_cases, recovered_cases)#
plot(x=xvals, y=yvals, xlab="Date", ylab="Number of cases", pch=".", col="white", cex=0.5)#
titletxt = paste0("Number of active & recovered cases in: ", country_name)#
title(titletxt)
xvals = c(as.Date(d2$date), as.Date(d2$date))#
yvals = c(active_cases, recovered_cases)#
plot(x=xvals, y=yvals, xlab="Date", ylab="Number of cases", pch=".", col="white", cex=0.5)#
titletxt = paste0("Number of active & recovered cases in: ", country_name)#
title(titletxt)
pdffn = "number_of_active_recovered_cases_NZ.pdf"  # filename of PDF#
pdf(file=pdffn, width=10, height=8)    # pdf() opens the PDF for writing - won't close until dev.off()!!!#
#
# Code for your plot#
# Plot the active cases, AND recovered cases#
# Plot a plot of white dots (just to set the plot scale)#
xvals = c(as.Date(d2$date), as.Date(d2$date))#
yvals = c(active_cases, recovered_cases)#
plot(x=xvals, y=yvals, xlab="Date", ylab="Number of cases", pch=".", col="white", cex=0.5)#
titletxt = paste0("Number of active & recovered cases in: ", country_name)#
title(titletxt)#
#
# Notes:#
# for help on plots, ?plot#
# for help on point characters (pch), ?points#
# for other graphical parameters (e.g. lty = line type), ?par#
#
# Add points & line for active cases#
points(x=as.Date(d2$date), y=active_cases, pch=1, col="red", cex=0.6)#
lines(x=as.Date(d2$date), y=active_cases, lty="solid", lwd=2, col="red2")#
#
# Add points & line for recovered cases#
points(x=as.Date(d2$date), y=recovered_cases, pch=2, col="green3", cex=0.5)#
lines(x=as.Date(d2$date), y=recovered_cases, lty="solid", lwd=2, col="green4")#
#
# Add vertical line with abline, and text, for Level 4 lockdown start#
lockdown_time = as.Date("2020-03-26")#
abline(v=lockdown_time, col="black", lty="dashed", lwd=2)#
text(x=lockdown_time, y=0.99*max(yvals), labels="lockdown begins", pos=2, srt=90, cex=0.7)#
#
# Add vertical line with abline, and text, for Level 4 lockdown end#
lockdown_time = as.Date("2020-04-28")#
abline(v=lockdown_time, col="black", lty="dashed", lwd=2)#
text(x=lockdown_time, y=0.99*max(yvals), labels="lockdown ends", pos=2, srt=90, cex=0.7)#
# Add horizontal line with abline#
threshold = 1000#
abline(h=threshold, col="grey", lty="dotted", lwd=2)#
text_x = min(xvals) + 0.05*(max(xvals) - min(xvals))#
text(x=text_x, y=threshold, labels="(1000 cases)", pos=3, srt=0, cex=0.7, col="grey")#
#
# Add legend#
legend(x="right", legend=c("active cases", "recovered"), col=c("red2", "green4"), lwd=c(2,2), pch=c(1,2))#
#
dev.off()	# closes the PDF for writing; after this, new plots go to screen again#
cmdstr = paste0("open ", pdffn)  # create the command to open the PDF#
system(cmdstr)                   # open the PDF in the operating system
pdffn = "number_of_new_cases_NZ.pdf"  # filename of PDF#
pdf(file=pdffn, width=10, height=8)    # pdf() opens the PDF for writing - won't close until dev.off()!!!#
#
# Code for your plot#
plot(x=as.Date(d$date), y=d$new_cases, xlab="Date", ylab="Number of new cases", pch=1, col="red", cex=0.5)#
titletxt = paste0("Number of new cases per day in: ", country_name)#
title(titletxt)#
lines(x=as.Date(d$date), y=d$new_cases, lty="solid", lwd=2, col="red")#
#
dev.off()	# closes the PDF for writing; after this, new plots go to screen again#
cmdstr = paste0("open ", pdffn)  # create the command to open the PDF#
system(cmdstr)                   # open the PDF in the operating system
~/Downloads/Cetacea_Dangerous_MAP.tree.txt
library(ape)#
trfn = "~/Downloads/Cetacea_Dangerous_MAP.tree.txt"#
tr = read.tree(trfn)#
#
tr
tipnames = tr$tip.label
tipnames
sorted_tipnames = sort(tipnames)
sorted_tipnames
cat(sorted_tipnames, sep="\n")
library(BioGeoBEARS)
plot_BioGeoBEARS_results
# 2022 LAB NOTE: On laptops/home computers, make sure RStudio is up-to-date. #
# Older versions of Rstudio may give this error:#
# #
# "Warning message: R graphics engine version 14 is not supported by this #
#  version of Rstudio. The Plots tab will be disabled until a newer version #
#  of RStudio is installed."#
# #
# This means you can't plot with older versions like R 4.1.2. So, update R/RStudio!#
########################################################
# BIOSCI 220: QUANTITATIVE BIOLOGY#
# WEEK 11 LAB#
########################################################
#
########################################################
# SETUP:#
########################################################
# If this lab is being done online, you will need #
# access to R and Rstudio#
##
# You can use R/RStudio on #
# #
# (a) your own hard drive, or#
# #
# (b) via a web browser, with RStudio Cloud#
# #
#######################################################
# Instructions for your own hard drive:#
#######################################################
# #
# R is free and open source, so if you have a laptop#
# or desktop, you can download R and RStudio.  They #
# should work on virtually any computer (a very old#
# computer might be challenging, but even there#
# probably there are old versions of R that will work.#
# #
# Instructions: Google "download R" and "download Rstudio"#
# and follow the instructions.#
# #
################################################
# Instructions for using RStudio Cloud:#
################################################
# #
# 1. Go to https://rstudio.cloud/#
# #
# 2. Register for a free account, e.g. using a gmail address.#
# #
# 3. Wait awhile for your Workspace to load. Be patient.#
# #
# 4. Click the blue "New Project" button in the upper right.#
# #
# 5. Wait awhile for your new project to load. Be patient.#
# #
# 6. Click File -> New File -> R script#
# #
# 7. Paste or type your R code into this window. You should be able to #
#    run the code by either#
#    #
#    (a) highlighting the code and clicking the "Run" button#
#    #
#    (b) copy-pasting into the command-line Console and hitting the Return key#
#    #
#    (c) highlighting the code and typing Command-Return (Macs) or Control-Return (Windows)#
#    #
#   (hint: note that hitting the "Up" arrow in the Console will bring back #
#    previous commands. This makes editing and re-running commands much easier#
#######################################################
########################################################
# PART 1: ADVANTAGES/DISADVANTAGES OF MODELS#
########################################################
# #
# Please read the Asimov and O'Neill readings. Work to #
# get the basic points of each. (These will be on the #
# Quiz.)#
# #
########################################################
#
########################################################
# PART 2: INTRODUCTION TO ML INFERENCE#
#########################################################
# #
# We will begin our exploration of model-based inference#
# with a very simple dataset and model.#
# #
# Many biological problems, in many different fields,#
# from ecology to epidemiology, are concerned with #
# trying to estimate the prevalence or frequency of#
# something. #
##
# For example:#
##
# - Out of 1000 ponds in a region, how many contain a#
#   particular endangered frog species?#
##
# - Out of a million people, how many are resistant#
#   to measles (due to e.g. vaccination or previous #
#   infection)?#
##
# These examples (leaving out numerous real-world#
# complexities) boil down to this: there are 2 #
# possible outcomes, and we are interested in how#
# often each outcome occurs.#
##
# Another process with 2 outcomes is lizard-flipping.#
##
# Lizard-flipping, and any other 2-outcome process,#
# can be described with a one-parameter model, using#
# a binomial distribution ("bi-nomial" = "two names").#
##
# If a lizard dropped from a height of 1 meter lands#
# on its feet, with head up, we call that "Heads" (H).#
##
# If it lands on its back, with head down, we call that #
# "Tails" (T).#
##
# (We will use lizard-flipping as an example, but you can#
# mentally substitute any other binomial process if#
# you like.)#
##
# The single parameter describing a binomial process#
# is "p", the probability of one of the outcomes. The#
# probability of the other outcome is 1-p.#
##
# Because there are so many different probabilities and #
# "p" terms used in probability#
# and statistics, I will use "ProbHeads" - the probability#
# that a lizard will produce "Heads" on a single flip - to #
# describe this parameter.#
# #
#########################################################
#
# Let's consider some lizard-flip data.  #
##
# Here are 100 lizard flips:#
#
lizard_flips = c('H','T','H','T','H','H','T','H','H','H','T','H','H','T','T','T','T','H','H','H','H','H','H','H','H','H','H','H','H','H','H','H','H','T','T','T','H','T','T','T','H','T','T','T','H','H','H','T','T','H','H','H','T','H','H','H','T','T','H','H','H','H','H','H','H','T','T','H','H','H','H','T','T','H','H','H','T','T','H','H','H','H','H','H','T','T','T','H','H','H','H','H','H','T','H','T','H','H','T','T')#
#
# Look at the data#
lizard_flips#
#
# What is your guess at "ProbHeads", the probability of heads?#
##
#
# In the case of binomial data, we actually have a simple #
# formula to calculate the best estimate of ProbHeads:#
#
# Find the heads#
heads_TF = (lizard_flips == "H")#
heads_TF#
#
# Find the tails#
tails_TF = (lizard_flips == "T")#
tails_TF#
#
numHeads = sum(heads_TF)#
numHeads#
#
numTails = sum(tails_TF)#
numTails#
#
numTotal = length(lizard_flips)#
numTotal#
#
# Here's the formula:#
ProbHeads_ML_estimate = numHeads / numTotal#
ProbHeads_ML_estimate#
#
# PLEASE ANSWER Lab 11 QUESTION #1, NOW:#
##
# 1. What number (exactly) was reported for ProbHeads_ML_estimate?  #
# #
# (Obviously the answer is 0.65, but I want you to realize that #
# the computer will auto-grade this.  So, 0.65 will be marked correct, #
# but .65, 0.650, etc., will be marked incorrect.  Extra spaces may #
# also cause the answer to be marked incorrect, e.g. "0.65" is correct, #
# but " 0.65" or "0.65 " are incorrect. For future answers, when numbers #
# are asked for, be sure to report the exact number R gave, without #
# spaces etc.)#
# Well, duh, that seems pretty obvious.  At least it would have been, if we #
# weren't thinking of lizards, where we have a strong prior belief that the#
# lizard is probably fair.#
# #
# It turns out that this formula can be justified through a technique#
# known as Maximum Likelihood.#
##
# What does it mean to say we have a "maximum likelihood" estimate of ProbHeads?#
# #
# "Likelihood", in statistics, means "the probability of the data under the model"#
##
# So, a "Maximum Likelihood estimate" of ProbHeads is the value of the parameter#
# ProbHeads that maximizes the probability of seeing the data you saw, namely,#
# 65 Heads and 35 Tails.#
##
# Comparing likelihoods for different possible parameter values of ProbHeads#
# #
# Let's calculate the probability of these same lizard flip data under the #
# hypothesis/model that ProbHeads=0.5#
# #
# We'll be very inefficient, and use a for-loop, and#
# if/else statements#
#
# Loop through all 100 flips#
# Make a list of the probability of #
# each datum#
ProbHeads_guess = 0.5#
#
# Empty list of probabilities#
probs_list = rep(NA, times=length(lizard_flips))#
probs_list#
#
for (i in 1:length(lizard_flips))#
    {#
    # Print an update#
    cat("\nAnalysing lizard flip #", i, "/", length(lizard_flips), sep="")#
#
    # Get the current lizard flip#
    lizard_flip = lizard_flips[i]#
#
    # If the lizard flip is heads, give that datum#
    # probability ProbHeads_guess.#
    # If tails, give it (1-ProbHeads_guess)#
#
    if (lizard_flip == "H")#
        {#
        probs_list[i] = ProbHeads_guess#
        } # End if heads#
#
    if (lizard_flip == "T")        #
        {#
        probs_list[i] = (1-ProbHeads_guess)#
        } # End if tails#
    } # End for-loop#
#
# Look at the resulting probabilities#
probs_list#
#
# We get the probability of all the data by multiplying#
# all the probabilities, with the prod() function.#
likelihood_of_data_given_ProbHeads_guess1 = prod(probs_list)#
likelihood_of_data_given_ProbHeads_guess1#
#
# PLEASE ANSWER Lab 11, QUESTION #2, NOW:#
# 2. What number (exactly) was reported for likelihood_of_data_given_ProbHeads_guess1?#
# That's a pretty small number!  You'll see that it's #
# just 0.5^100:#
0.5^100#
#
# A probability of 0.5 is not small, but multiply it #
# 100 values of 0.5 together, and you get a small value.#
# That's the probability of that specific sequence of #
# heads/tails, given the hypothesis that the true#
# probability is ProbHeads_guess.#
#
# Let's try another parameter value:#
#
# Loop through all 100 flips#
# Make a list of the probability of #
# each datum#
ProbHeads_guess = 0.7#
#
# Empty list of probabilities#
probs_list = rep(NA, times=length(lizard_flips))#
probs_list#
#
for (i in 1:length(lizard_flips))#
    {#
    # Print an update#
    cat("\nAnalysing lizard flip #", i, "/", length(lizard_flips), sep="")#
#
    # Get the current lizard flip#
    lizard_flip = lizard_flips[i]#
#
    # If the lizard flip is heads, give that datum#
    # probability ProbHeads_guess.#
    # If tails, give it (1-ProbHeads_guess)#
#
    if (lizard_flip == "H")#
        {#
        probs_list[i] = ProbHeads_guess#
        } # End if heads#
#
    if (lizard_flip == "T")        #
        {#
        probs_list[i] = (1-ProbHeads_guess)#
        } # End if tails#
    } # End for-loop#
#
# Look at the resulting probabilities#
probs_list#
#
# We get the probability of all the data by multiplying#
# all the probabilities#
likelihood_of_data_given_ProbHeads_guess2 = prod(probs_list)#
likelihood_of_data_given_ProbHeads_guess2#
#
# PLEASE ANSWER Lab 11, QUESTION #3, NOW:#
# 3. What number (exactly) was reported for likelihood_of_data_given_ProbHeads_guess2?#
#
# We got a different likelihood. It's also very small.#
# But that's not important. What's important is, #
# how many times higher is it?#
#
likelihood_of_data_given_ProbHeads_guess2 / likelihood_of_data_given_ProbHeads_guess1#
#
# PLEASE ANSWER Lab 11, QUESTION #4, NOW:#
# 4. What number (exactly) was reported for #
# likelihood_of_data_given_ProbHeads_guess2 / likelihood_of_data_given_ProbHeads_guess1 ?#
# Whoa!  That's a lot higher!  This means the lizard flip data is 54 times more#
# probable under the hypothesis that ProbHeads=0.7 than under the #
# hypothesis that ProbHeads=0.5.#
#
# Maximum likelihood: Hopefully you can see that the data is in some sense#
# "best explained" by the parameter value (the valueof ProbHeads) that #
# maximizes the probability of the data. This is what we call the Maximum #
# Likelihood solution.#
#
# To try different values of ProbHeads, we could keep copying and pasting code, #
# but that seems annoying.  Let's define an R "function" instead.#
##
# Functions, in R, collect a number of commands, and run them when you#
# type the name of the function. Functions are simple programs. All of #
# the R commands you have ever run, e.g. mean(), prod(), etc. are all#
# just functions that R has pre-programmed.#
##
# NOTES ON FUNCTIONS IN R:#
# * To run, functions have to be followed by (). Usually, but not always,#
#   inputs go into the ().  One example of a function that needs no inputs#
#   is getwd(), which gets your current working directory (wd) and prints it#
#   to screen:#
#
getwd()#
#
# * To see the code that is running inside the function, you can often #
#   just type the name of the function, without the (). This works for#
#   most R functions except for "primitive"  ones (these are core R#
#   functions, like getwd, mean, length, etc., that are programmed#
#   a different way to be super-fast).#
##
# * Typing the names of functions and figuring out exactly what is #
#   happening in the code is a great way to learn R and learn a #
#   specific scientific field, since you can see *exactly* what #
#   some scientist is doing in their analysis.#
##
# * For functions that are part of packages, you can get the help#
#   page by type a ? before the function name. E.g., ?mean will open#
#   the help page for the "mean" function.  R help is often not #
#   written for beginners, but it is better than nothing.#
##
# * When you try to run a function and get an error, this usually#
#   means you left out a required input, or you put in the wrong#
#   kind of input (e.g., if the input is supposed to be a number,#
#   and you put in a word like "hello", you will get an error #
#   message.#
##
# When you type #
#
mean("hello")#
#
# ...what happens?#
#
# PLEASE ANSWER Lab 11, QUESTION #5, NOW:#
# 5. When you run 'mean("hello")', paste in the last part of the error message you get back. #
# Your paste should being with "argument" and end with "NA".#
# Let's define a function#
# #
# This function calculates the probability of a #
# sequence of lizard flip data, given a value of ProbHeads_guess#
calc_prob_lizard_flip_data <- function(ProbHeads_guess, lizard_flips)#
    {#
    # Empty list of probabilities#
    probs_list = rep(NA, times=length(lizard_flips))#
    probs_list#
#
    for (i in 1:length(lizard_flips))#
        {#
        # Print an update#
        #cat("\nAnalysing lizard flip #", i, "/", length(lizard_flips), sep="")#
#
        # Get the current lizard flip#
        lizard_flip = lizard_flips[i]#
#
        # If the lizard flip is heads, give that datum#
        # probability ProbHeads_guess.#
        # If tails, give it (1-ProbHeads_guess)#
#
        if (lizard_flip == "H")#
            {#
            probs_list[i] = ProbHeads_guess#
            } # End if heads#
#
        if (lizard_flip == "T")        #
            {#
            probs_list[i] = (1-ProbHeads_guess)#
            } # End if tails#
        } # End for-loop#
#
    # Look at the resulting probabilities#
    probs_list#
#
    # We get the probability of all the data by multiplying#
    # all the probabilities#
    likelihood_of_data_given_ProbHeads_guess = prod(probs_list)#
#
    # Return result#
    return(likelihood_of_data_given_ProbHeads_guess)#
    }#
#
# Now, we can just use this function:#
calc_prob_lizard_flip_data(ProbHeads_guess=0.5, lizard_flips=lizard_flips)#
calc_prob_lizard_flip_data(ProbHeads_guess=0.6, lizard_flips=lizard_flips)#
calc_prob_lizard_flip_data(ProbHeads_guess=0.7, lizard_flips=lizard_flips)#
#
# Look at that!  We did all of that work in a split-second.#
#
# PLEASE ANSWER Lab 11, QUESTION #6, NOW:#
# 6. What number (exactly) was reported for #
# calc_prob_lizard_flip_data(ProbHeads_guess=0.6, lizard_flips=lizard_flips) ?#
#
# In fact, we can make another for-loop, and search for the ML#
# value of ProbHeads by trying all of the values and plotting them.#
#
# Sequence of 50 possible values of ProbHeads between 0 and 1#
ProbHeads_values_to_try = seq(from=0, to=1, length.out=50)#
likelihoods = rep(NA, times=length(ProbHeads_values_to_try))#
#
for (i in 1:length(ProbHeads_values_to_try))#
    {#
    # Get the current guess at ProbHeads_guess#
    ProbHeads_guess = ProbHeads_values_to_try[i]#
#
    # Calculate likelihood of the lizard flip data under#
    # this value of ProbHeads#
    likelihood = calc_prob_lizard_flip_data(ProbHeads_guess=ProbHeads_guess, lizard_flips=lizard_flips)#
#
    # Store the likelihood value#
    likelihoods[i] = likelihood#
    } # End for-loop#
#
# Here are the resulting likelihoods:#
likelihoods#
#
# Let's try plotting the likelihoods to see if there's a peak#
plot(x=ProbHeads_values_to_try, y=likelihoods)#
lines(x=ProbHeads_values_to_try, y=likelihoods)#
#
# Whoa! That's quite a peak!  You can see that the likelihoods#
# vary over several orders of magnitude.#
##
# Partially because of this extreme variation, we often use the #
# logarithm of the likelihood, also known as #
##
# * log-likelihood#
# * lnL#
##
# In R, the log() function returns the natural logarithm, while#
# log10 returns the logarithms at base 10. I mention this because#
# on some calculators, the LN button returns the natural log,#
# and the LOG button returns the log10 logarithm.#
##
##
# There are several reasons scientists, and scientific papers,#
# often use the log-likelihood instead of the raw likelihood. #
# These include:#
# #
# * Computers have a minimum numerical precision. If a likelihood#
#   value gets too small, it gets rounded to 0. You can see this#
#   happen if you calculate the raw likelihood of over 1000 lizard-flips#
#   using the parameter ProbHeads = 0.5#
##
# Machine precision can vary between computers, so I will give you#
# the results on my machine (but, try it on yours):#
#
0.5^1073#
#
# ...equals 9.881313e-324, which is a very small number#
#
0.5^1074#
#
# ...equals 4.940656e-324, which is half the size of the previous number,#
# which makes sense, because we just multiplied it by 0.5#
##
# However, on my machine,#
#
0.5^1075#
#
# ...equals 0. Instead of one more lizard-flip causing the likelihood to be #
# halved again, zero is infinitely less than 4.940656e-324. So the #
# computer has made a "mistake" here. #
##
# Log-likelihoods do not have this problem. Compare:#
#
1073 * log(0.5)#
1074 * log(0.5)#
1075 * log(0.5)#
1076 * log(0.5)#
#
# We can convert log-likelihoods to raw-likelihood with exp(), which reverses#
# the log() operation.#
#
exp(1073 * log(0.5))#
exp(1074 * log(0.5))#
exp(1075 * log(0.5))#
exp(1076 * log(0.5))#
#
# You can see that, working in log-likelihood space, we have no #
# numerical precision issues, but with raw likelihoods, we do.#
#
# PLEASE ANSWER Lab 11, QUESTION #7, NOW:#
# 7. What number (exactly) was reported for 1075 * log(0.5) ?#
# Other reasons scientists use log-likelihoods instead of likelihoods:#
# #
# * The log-likelihood and raw-likelihood will have their maximums#
#   at the same parameter values.#
# #
# * Taking the logarithm of an equation converts multiplication to#
#   addition, which can make calculus much easier.#
##
# * Log-likelihoods are easier to read and interpret than raw #
#   likelihoods, once you get used to log-likelihoods. For any#
#   large dataset, the raw likelihood will be a very tiny#
#   number, hard to read/remember/interpret.#
##
# * It turns out that a number of fundamental statistical measures#
#   and tests use the log-likelihood as an input. These include#
#   the Likelihood Ratio Test (LRT) and Akaike Information Criterion (AIC).#
#   We will talk about these in future weeks.#
# PLOTTING THE LOG-LIKELIHOOD#
#
# We will repeat the code to calculate the likelihood#
# for a sequence of 50 possible values of ProbHeads between 0 and 1:#
#
ProbHeads_values_to_try = seq(from=0, to=1, length.out=50)#
likelihoods = rep(NA, times=length(ProbHeads_values_to_try))#
#
for (i in 1:length(ProbHeads_values_to_try))#
    {#
    # Get the current guess at ProbHeads_guess#
    ProbHeads_guess = ProbHeads_values_to_try[i]#
#
    # Calculate likelihood of the lizard flip data under#
    # this value of ProbHeads#
    likelihood = calc_prob_lizard_flip_data(ProbHeads_guess=ProbHeads_guess, lizard_flips=lizard_flips)#
#
    # Store the likelihood value#
    likelihoods[i] = likelihood#
    } # End for-loop#
#
# Here are the resulting likelihoods:#
likelihoods#
#
# Let's take the log (the natural log, i.e. the base is exp(1)).#
log_likelihoods = log(likelihoods, base=exp(1))#
#
plot(x=ProbHeads_values_to_try, y=log_likelihoods)#
lines(x=ProbHeads_values_to_try, y=log_likelihoods)#
#
# Let's plot the raw- and log-likelihood together:#
par(mfrow=c(2,1))#
plot(x=ProbHeads_values_to_try, y=likelihoods, main="Likelihood (L) of the data")#
lines(x=ProbHeads_values_to_try, y=likelihoods)#
#
plot(x=ProbHeads_values_to_try, y=log_likelihoods, main="Log-likelihood (LnL) of the data")#
lines(x=ProbHeads_values_to_try, y=log_likelihoods)#
#
# PLEASE ANSWER Lab 11, QUESTION #8, NOW:#
# 8. Compare the raw-likelihood curve and the log-likelihood curve. Does it look #
# like the top of each curve appears in about the same location on the #
# x-axis? (i.e., at the same parameter value for ProbHeads?)#
# #
# Maximum likelihood optimization#
# #
# You can see that the maximum likelihood of the data occurs when #
# ProbHeads is somewhere around 0.6 or 0.7.  What is it #
# exactly?#
##
# We could just keep trying more values until we find whatever#
# precision we desire.  But, R has a function for#
# maximum likelihood optimization!#
# #
# It's called optim().  Optim() takes a function as an input.#
# Fortunately, we've already written a function!#
##
# Let's modify our function a bit to return the log-likelihood,#
# and print the result:#
#
# Function that calculates the probability of lizard flip data#
# given a value of ProbHeads_guess#
calc_prob_lizard_flip_data2 <- function(ProbHeads_guess, lizard_flips)#
    {#
    # Empty list of probabilities#
    probs_list = rep(NA, times=length(lizard_flips))#
    probs_list#
#
    for (i in 1:length(lizard_flips))#
        {#
        # Print an update#
        #cat("\nAnalysing lizard flip #", i, "/", length(lizard_flips), sep="")#
#
        # Get the current lizard flip#
        lizard_flip = lizard_flips[i]#
#
        # If the lizard flip is heads, give that datum#
        # probability ProbHeads_guess.#
        # If tails, give it (1-ProbHeads_guess)#
#
        if (lizard_flip == "H")#
            {#
            probs_list[i] = ProbHeads_guess#
            } # End if heads#
#
        if (lizard_flip == "T")        #
            {#
            probs_list[i] = (1-ProbHeads_guess)#
            } # End if tails#
        } # End for-loop#
#
    # Look at the resulting probabilities#
    probs_list#
#
    # We get the probability of all the data by multiplying#
    # all the probabilities#
    likelihood_of_data_given_ProbHeads_guess = prod(probs_list)#
#
    # Get the log-likelihood#
    LnL = log(likelihood_of_data_given_ProbHeads_guess)#
    LnL#
#
    # Error correction: if -Inf, reset to a low value#
    if (is.finite(LnL) == FALSE)#
        {#
        LnL = -1000#
        }#
#
    # Print some output#
    print_txt = paste("\nWhen ProbHeads=", ProbHeads_guess, ", LnL=", LnL, sep="")#
    cat(print_txt)#
#
    # Return result#
    return(LnL)#
    }#
#
# Try the function out:#
LnL = calc_prob_lizard_flip_data2(ProbHeads_guess=0.1, lizard_flips=lizard_flips)#
LnL = calc_prob_lizard_flip_data2(ProbHeads_guess=0.2, lizard_flips=lizard_flips)#
LnL = calc_prob_lizard_flip_data2(ProbHeads_guess=0.3, lizard_flips=lizard_flips)#
#
# Looks like it works!  Let's use optim() to search for he #
# best ProbHeads value:#
#
# Set a starting value of ProbHeads#
starting_value = 0.1#
#
# Set the limits of the search#
limit_bottom = 0#
limit_top = 1#
#
optim_result = optim(par=starting_value, fn=calc_prob_lizard_flip_data2, lizard_flips=lizard_flips, method="L-BFGS-B", lower=limit_bottom, upper=limit_top, control=list(fnscale=-1))#
#
# You can see the search print out as it proceeds.#
#
# Let's see what ML search decided on:#
optim_result#
#
# Let's compare the LnL from ML search, with the binomial mean#
optim_result$par#
#
# Here's the formula:#
ProbHeads_ML_estimate = numHeads / numTotal#
ProbHeads_ML_estimate#
#
# PLEASE ANSWER Lab 11, QUESTION #9, NOW:#
# 9. Is the ML value of ProbHeads_guess found by numerical optimization #
# (found in optim_result$par ) very close to the ML value found #
# by analytical solution (ProbHeads_ML_estimate)?#
#
# PLEASE ANSWER Lab 11, QUESTION #10, NOW:#
# 10. What was the maximized log-likelihood found by optim (hint: look at#
# optim_result$value)?  Numerical optimization results might vary #
# slightly, so round this value to 2 decimal places in your answer.#
# Where does that formula -- the "analytic solution" for ProbHeads_ML_estimate #
# come from?  We will discuss this further in future lectures, but here is a summary.#
##
# It turns out that, in simple cases, we can use calculus to derive #
# "analytical solutions" for maximizing the likelihood.  Basically, this involves:#
##
# - taking the derivative of the likelihood function#
#   (the derivative is the slope of a function)#
# - setting the formula for the derivative to equal 0#
#   (because, when the derivative=0, you are at a maximum or minimum)#
# - solving for the parameter value that causes the derivative to #
#   equal 0.#
##
# This same basic calculus-based procedure provides a justification for#
# the formulas that are used in e.g. linear regression.#
##
# The nice things about calculus-derived formulas for maximizing likelihood #
# -- known as "analytic solutions" -- include:#
# #
# - it is very fast#
# - the solution can be found without a computer (this was useful back in #
#   the 20th century, when statistics was being developed without computers)#
# - there is no chance of optimization error (optimization errors can occur#
#   if likelihood surfaces are too flat, too bumpy, etc.)#
# #
# This raises the question:#
# Why would anyone ever go through all the optimization rigamarole, when #
# they could just calculate ProbHeads directly?#
##
# Well, only in simple cases do we have a formula for the maximum likelihood #
# estimate.  The optim() strategy works whether or not#
# there is a simple formula.#
##
# In real life science, with complex models of biological processes (such as #
# epidemiological models), ML optimization gets used A LOT, but most scientists#
# don't learn it until graduate school, if then.#
# #
# WHAT WE LEARNED#
##
# You can now see that there is nothing magical, or even particularly#
# difficult, about these concepts:#
##
# - likelihood#
# - log-likelihood#
# - maximum likelihood (ML)#
# - inferring a parameter through ML#
# #
##
# IMPLICATIONS#
##
# Despite their simplicity, it turns out that likelihood is one of #
# the core concepts in model-based inference, and statistics #
# generally.  Bayesian analyses have likelihood as a core component #
# (we will talk about this more later), and methods from introductory #
# statistics (linear models etc.)#
########################################################
########################################################
# PART 3: BUILDING BLOCKS OF MODELS IN R#
########################################################
##
# R has a number of functions that speed up the process #
# of building models, and simulating from them.#
##
# For example, for binomial processes, these are:#
##
# dbinom(x, size, prob, log) -- the probability density #
# (the likelihood) of x "heads"#
# #
# size = the number of tries (the number of lizard flips)#
##
# prob = the parameter value (e.g. ProbHeads)#
##
# log = should the likelihood be log-transformed when output?#
# #
##
# The other functions are:#
# #
# rbinom(n, size, prob) -- simulate n binomial outcomes,#
# using "size" and "prob" as above#
# #
# pbinom(q, ...) - used for calculating tail probabilities #
# (proportion of the distribution less than the value q)#
##
# qbinom(p, ...) - reverse of pbinom()#
##
##
# Any time you hear about "probability densities", "distributions"#
# "random number generators", "random draws", or "simulations"#
# in R, functions like these are being used.#
##
# For uniform distributions, the equivalent functions are:#
# #
# dunif, runif, punif, qunif#
##
# For normal (=Gaussian/bell-curve) distributions, the equivalents are:#
##
##
# dnorm, rnorm, pnorm, qnorm#
# Random seeds, and random number generators#
#
# Random numbers in computers are not truly random. To make random numbers#
# repeatable, a "seed" number is always used.#
#
# Compare:#
#
rnorm(n=1, mean=6, sd=1)#
rnorm(n=1, mean=6, sd=1)#
rnorm(n=1, mean=6, sd=1)#
#
set.seed(54321)#
rnorm(n=1, mean=6, sd=1)#
rnorm(n=1, mean=6, sd=1)#
#
set.seed(54321)#
rnorm(n=1, mean=6, sd=1)#
rnorm(n=1, mean=6, sd=1)#
#
set.seed(54321)#
rnorm(n=1, mean=6, sd=1)#
rnorm(n=1, mean=6, sd=1)#
#
# By re-setting the seed, we re-set the random number generator#
#
# PLEASE ANSWER Lab 11, QUESTION #11, NOW:#
# 11. If you set the random number seed to 54322, what is the first#
# random number generated by rnorm(n=1, mean=6, sd=1)?#
# Generate 1000 random numbers from a normal distribution, and #
# then plot a histogram of them, with this code:#
#
random_numbers = rnorm(n=1000, mean=6, sd=1)#
hist(random_numbers)#
#
# PLEASE ANSWER Lab 11, QUESTION #12, NOW:#
# 12. What shape do you see?#
# PLEASE ANSWER Lab 11, QUESTION #13, NOW:#
# 13. I am 6 feet tall. What is the raw likelihood of my height,#
# assuming that I am a random draw from a population that is #
# 7 feet tall, with a standard deviation of 0.5 feet?#
#
dnorm(x=6.0, mean=7.0, sd=0.5)#
#
# PLEASE ANSWER Lab 11, QUESTION #14, NOW:#
# 14. What is the log-likelihood of my height?#
log(dnorm(x=6.0, mean=7.0, sd=0.5))#
#
# PLEASE ANSWER Lab 11, QUESTION #15, NOW:#
# 15. If we change the model of my population from mean height=7 #
# feet to mean height=6 feet, will the likelihood of my 6-foot #
# height go up or down?#
#
# PLEASE ANSWER Lab 11, QUESTION #16, NOW:#
# 16. What is the raw likelihood of my height, assuming I am a#
# random draw from a population that is 6 feet tall, with #
# a standard deviation of 0.5 feet?#
#
dnorm(x=6.0, mean=6.0, sd=0.5)#
#
# NOTE: The "d" in "dnorm" refers to probability density. For continuous data,#
# the probability of any exact value is 0, because there are an infinite number#
# of possible values for a continuous variable. So we talk about the probability#
# density instead of the exact probability (or probability mass). Probability#
# densities work the same as probability masses for our purposes.#
# PLEASE ANSWER Lab 11, QUESTION #17, NOW:#
# 17. What is the corresponding log-likelihood?#
# PLEASE ANSWER Lab 11, QUESTION #18, NOW:#
# 18. What shape does dnorm() make, for a variety of#
# height values?  Try:#
##
height_values = seq(4, 8, 0.01)#
probability_densities = dnorm(x=height_values, mean=6.0, sd=0.5)#
plot(x=height_values, y=probability_densities)#
#
########################################################
# END OF Lab 11 WORKSHEET#
# DISCUSSION POST ASSIGNMENT CONTINUES BELOW#
########################################################
########################################################
# BIOSCI 220 Week 11 Lab Exercise#
# PART 4#
# Reading, plotting, and interpreting COVID data#
########################################################
#
# #
# NOTE: The OurWorldInData Covid case dataset grows #
# every term.  It now can take several minutes to download.#
# #
# BE PATIENT AND WAIT FOR THE COMMAND TO FINISH.#
# #
# If you get timeout #
# #
#
# The command below downloads the CSV (comma-separated value) file from:#
# #
# https://github.com/owid/covid-19-data/tree/master/public/data#
# #
# Specifically:#
# https://covid.ourworldindata.org/data/owid-covid-data.csv#
##
# Canvas backup of "owid-covid-data.csv" is:#
# https://canvas.auckland.ac.nz/files/5181566/download?download_frd=1#
# #
# Two methods to get the data:#
##
# 1. Save locally, then open in R. The tricky thing about this is that#
#    *you* have to figure out where you have saved the data, then get#
#    R to find that file.#
#
# If I saved the data to the directory #
# "/drives/GDrive/__classes/BIOSCI220/2020_term2/week02_lectures/"#
# I could store this "path" in "fn" (fn=filename), #
# then read it with read.csv():#
#
# fn = "/drives/GDrive/__classes/BIOSCI220/2020_term2/week02_lectures/owid-covid-data.csv"#
# data = read.csv(file=fn)#
# dim(data)#
#
# 2. Read the file straight off the internet (easier for now)#
##
fn = "https://covid.ourworldindata.org/data/owid-covid-data.csv"#
# BE PATIENT AND WAIT FOR THE read.csv COMMAND TO FINISH. #
# (A backup is available on Canvas if needed)#
data = read.csv(file=fn)#
dim(data)#
########################################################
# Exploring a data table#
# #
# A lot of work in R is done with "data frames". These#
# are just tables of data.#
##
# Data tables can be huge, so there are bunch of functions #
# that let you explore what is in a data table.#
# #
# Try these commands. Figure out what each one does.#
##
# Remember, you can do e.g. "?dim" to get the help page#
# for the function.#
# #
########################################################
#
dim(data)#
nrow(data)#
ncol(data)#
head(data)#
tail(data)#
names(data)#
class(data)#
########################################################
# Accessing particular columns of data#
########################################################
# With R data.frames, you can access individual columns#
# using "$columnname"#
#
# However, this dataset is much huger than in May 2020, such #
# that now it causes problems when printed to the screen#
# (so, DON'T try to print the whole dataset to screen, you#
# may get a frozen screen).#
#
# We can use the function "unique" to #
# get just one entry for each country#
#
unique(data$location)#
########################################################
# Plotting the data#
# Below, I have a script to plot the number of new#
# cases, per day, for New Zealand#
########################################################
#
# Specify the country name#
country_name = "New Zealand"#
#
# Specify the end-date for your analysis#
# (This script was initially set up for the first 2020 wave. If you #
#  change the end-date, plots will look different!)#
end_date = as.Date("2020-06-30")#
#
# The "==" symbol gives TRUE or FALSE (I call the results TF)#
# Here, we are seeing, for each row, if it matches #
# "New Zealand".#
##
TF1 = data$location == country_name#
#
# R treats FALSE as 0, and TRUE as 1, so doing sum(TF1)#
# gives us the number of TRUE values#
sum(TF1)#
#
# Let's also subset to end-date#
TF2 = as.Date(data$date) <= end_date#
sum(TF1)#
#
# Let's combine TF1 and TF2 into a single list of TRUEs/FALSEs.#
# We will keep rows that are TRUE for both#
TF = (TF1 + TF2) == 2#
#
# We can subset the table "data" to a smaller table, "d",#
# by using square brackets. Square brackets indicate which #
# [rows,columns] of the data table you want, where blank =#
# "give everything".#
##
# So, this command subsets the table "data" to all rows#
# where location == "New Zealand"#
d = data[TF,]#
#
# (We will also keep all the NZ data for all dates)#
nzdata = data[TF1,]; nzdata$date = as.Date(nzdata$date)#
# How big is the new table?#
dim(d)#
#
# Let's plot date versus cases, for New Zealand#
plot(x=d$date, y=d$new_cases)#
#
# Why didn't that work?  Look at d$date:#
d$date#
class(d$date)#
#
# If the "class" of the data is "character", and if R can't easily #
# convert it to numbers (class "numeric"), then R gives an error.#
#
# We can fix this by forcing R to read the dates as datatype date,#
# using the "as.Date" function:#
#
plot(x=as.Date(d$date), y=d$new_cases)#
#
# We can also plot the cases, with the y-axis in log space.#
# This is useful when the counts range from dozens (e.g. the 2020#
# NZ wave) to tens of thousands (e.g. the 2022 wave).#
plot(x=as.Date(d$date), y=d$new_cases, log="y")#
#
# That's not very useful for the 2020 NZ data, since our #
# case counts were so low. But when we look at the full#
# data covering 2020-present, the logged y-axis is very useful!#
# Compare visually:#
plot(x=as.Date(nzdata$date), y=nzdata$new_cases)#
plot(x=as.Date(nzdata$date), y=nzdata$new_cases, log="y")#
# You can see that R has plotted the dates on the x-axis, and #
# the new cases on the y-axis. R uses a bunch of defaults in its#
# plots, but we can do better.#
##
# I am just going to make a better plot. It is up to you to #
# READ this code, and look through the relevant help files,#
# ?plot and ?par, #
# to learn what these various inputs do.#
#
plot(x=as.Date(d$date), y=d$new_cases, xlab="Date", ylab="Number of new cases", pch=1, col="red", cex=0.5)#
titletxt = paste0("Number of new cases per day in: ", country_name)#
title(titletxt)#
lines(x=as.Date(d$date), y=d$new_cases, lty="solid", lwd=2, col="red")#
# Hmm, that line has some gaps, suggesting there are some days with no data.#
# What is going on?#
unique(d$new_cases)#
#
# Let's try replacing the "NA" values with 0:#
TF = is.na(d$new_cases)#
d$new_cases[TF] = 0#
#
# And, plot again:#
plot(x=as.Date(d$date), y=d$new_cases, xlab="Date", ylab="Number of new cases", pch=1, col="red", cex=0.5)#
titletxt = paste0("Number of new cases per day in: ", country_name)#
title(titletxt)#
lines(x=as.Date(d$date), y=d$new_cases, lty="solid", lwd=2, col="red")#
########################################################
# That looks a little better, but it still seems like quite a jagged plot.#
# Some of this is randomness in the daily counts, and some is #
# probably errors in the data.#
##
# We might get a clearer picture if we had a smoother version of the data.#
##
# Also, for SIR models, we will want to know how many COVID#
# cases are active at any particular point in time.#
##
# The conventional wisdom (in early 2020) was that once someone gets COVID, #
# they are infectious for about 14 days. This may not be#
# perfectly accurate, but it is a reasonable starting point.#
##
# The code below keeps a running count of cases for the #
# last 14 days. It also counts how many have recovered.#
########################################################
#
# Subset to your country, replace "NA" values with 0:#
TF = data$location == country_name#
d2 = data[TF, ]#
TF = is.na(d2$new_cases)#
d2$new_cases[TF] = 0#
#
# Keep a count of active cases, and recovered cases#
active_cases = rep(0.0, times=nrow(d2))#
recovered_cases = rep(0.0, times=nrow(d2))#
#
# Here, we do a "for-loop". It will leap through every row#
# of the table, and do a calculation.#
##
# NOTE: to make a for-loop run, you have to run#
# the WHOLE thing, from the "for" to the final closing#
# bracket "}", at once!#
# #
for (i in 1:nrow(d2))#
	{#
	# Keep track of the starting row number#
	if (i <= 14)#
		{#
		startrow = 1#
		endrow = i#
		recovered_cases[i] = 0#
		} else {#
		startrow = startrow + 1#
		endrow = i#
		sum_of_recovered_cases = sum(d2$new_cases[1:(startrow-1)])#
		recovered_cases[i] = sum_of_recovered_cases#
		}#
	# Add up the last 14 days of active cases#
	sum_of_active_cases_14days = sum(d2$new_cases[startrow:endrow])#
	# Store the result:#
	active_cases[i] = sum_of_active_cases_14days#
	}#
#
# (NON-LOG Y-AXIS VERSION)#
# Plot the active cases, AND recovered cases#
# Plot a plot of white dots (just to set the plot scale)#
xvals = c(as.Date(d2$date), as.Date(d2$date))#
yvals = c(active_cases, recovered_cases)#
plot(x=xvals, y=yvals, xlab="Date", ylab="Number of cases", pch=".", col="white", cex=0.5)#
titletxt = paste0("Number of active & recovered cases in: ", country_name)#
title(titletxt)#
# Notes:#
# for help on plots, ?plot#
# for help on point characters (pch), ?points#
# for other graphical parameters (e.g. lty = line type), ?par#
#
# Add points & line for active cases#
points(x=as.Date(d2$date), y=active_cases, pch=1, col="red", cex=0.6)#
lines(x=as.Date(d2$date), y=active_cases, lty="solid", lwd=2, col="red2")#
#
# Add points & line for recovered cases#
points(x=as.Date(d2$date), y=recovered_cases, pch=2, col="green3", cex=0.5)#
lines(x=as.Date(d2$date), y=recovered_cases, lty="solid", lwd=2, col="green4")#
#
# Add vertical line with abline, and text, for Level 4 lockdown start#
lockdown_time = as.Date("2020-03-26")#
abline(v=lockdown_time, col="black", lty="dashed", lwd=2)#
text(x=lockdown_time, y=0.99*max(yvals), labels="lockdown begins", pos=2, srt=90, cex=0.7)#
#
# Add vertical line with abline, and text, for Level 4 lockdown end#
lockdown_time = as.Date("2020-04-28")#
abline(v=lockdown_time, col="black", lty="dashed", lwd=2)#
text(x=lockdown_time, y=0.99*max(yvals), labels="lockdown ends", pos=2, srt=90, cex=0.7)#
# Add horizontal line with abline#
threshold = 1000#
abline(h=threshold, col="grey", lty="dotted", lwd=2)#
text_x = min(xvals) + 0.05*(max(xvals) - min(xvals))#
text(x=text_x, y=threshold, labels="(1000 cases)", pos=3, srt=0, cex=0.7, col="grey")#
#
# Add legend#
legend(x="right", legend=c("active cases", "recovered"), col=c("red2", "green4"), lwd=c(2,2), pch=c(1,2))#
# (LOG Y-AXIS VERSION)#
# Plot the active cases, AND recovered cases#
# Plot a plot of white dots (just to set the plot scale)#
xvals = c(as.Date(d2$date), as.Date(d2$date))#
yvals = c(active_cases, recovered_cases)#
plot(x=xvals, y=yvals, xlab="Date", ylab="Number of cases", pch=".", col="white", cex=0.5, log="y")#
titletxt = paste0("Number of active & recovered cases in: ", country_name)#
title(titletxt)#
# Notes:#
# for help on plots, ?plot#
# for help on point characters (pch), ?points#
# for other graphical parameters (e.g. lty = line type), ?par#
#
# Add points & line for active cases#
points(x=as.Date(d2$date), y=active_cases, pch=1, col="red", cex=0.6)#
lines(x=as.Date(d2$date), y=active_cases, lty="solid", lwd=2, col="red2")#
#
# Add points & line for recovered cases#
points(x=as.Date(d2$date), y=recovered_cases, pch=2, col="green3", cex=0.5)#
lines(x=as.Date(d2$date), y=recovered_cases, lty="solid", lwd=2, col="green4")#
#
# Add vertical line with abline, and text, for Level 4 lockdown start#
lockdown_time = as.Date("2020-03-26")#
abline(v=lockdown_time, col="black", lty="dashed", lwd=2)#
text(x=lockdown_time, y=0.99*max(yvals), labels="lockdown begins", pos=2, srt=90, cex=0.7)#
#
# Add vertical line with abline, and text, for Level 4 lockdown end#
lockdown_time = as.Date("2020-04-28")#
abline(v=lockdown_time, col="black", lty="dashed", lwd=2)#
text(x=lockdown_time, y=0.99*max(yvals), labels="lockdown ends", pos=2, srt=90, cex=0.7)#
# Add horizontal line with abline#
threshold = 1000#
abline(h=threshold, col="grey", lty="dotted", lwd=2)#
text_x = min(xvals) + 0.05*(max(xvals) - min(xvals))#
text(x=text_x, y=threshold, labels="(1000 cases)", pos=3, srt=0, cex=0.7, col="grey")#
#
# Add legend#
legend(x="right", legend=c("active cases", "recovered"), col=c("red2", "green4"), lwd=c(2,2), pch=c(1,2))#
########################################################
# Tasks:#
# #
# Work through the example R script, plotting New Zealands COVID history#
# #
# 1. Do the Week 11 Worksheet, if you haven't already#
##
# 2. Now that you have been shown how to make nice plots of covid active case data,#
#    make a nice plot, in R, using the COVID active case data for a country OTHER #
#    than New Zealand.#
# #
#    A nice plot means: title, labeled axes, legend if multiple colors are used, #
#     appropriate choice of logging the y-axis or not, etc.#
# #
#    If warranted, draw some vertical lines to represent major events (e.g. lockdowns starting and #
#     ending; use Google to find these. Yes, sometimes the story will be complex as #
#     some countries had regional lockdowns, etc.  Just try to label the biggest events#
#     influencing the curve. Your plot will be marked based on whether it communicates the #
#     basics of the Covid epidemic in the country, not on every last detail, so use your#
#     own best judgement about what to include.)#
# #
# 3. Screen capture your plot & post to Canvas, along with a paragraph interpreting #
# the plot, meaning: you provide a verbal model of what you think is going on in that country.#
# #
# 4. Comment on 2 other peoples posts  say what is interesting, ask a #
# question, etc. (make sure you are polite & professional)#
# #
# This assignment should be doable in lab.#
# #
# The main forum for help is the physical lab sessions. We will also monitor Piazza, #
# BUT DO NOT EXPECT PIAZZA HELP OUTSIDE OF WEEKDAY HOURS.#
# #
########################################################
#
########################################################
# HINTS#
########################################################
# We are deliberately throwing you into the deep end with the R script.#
# #
# Your job is to run the script, and, from reading the script, learn the basic R commands to plot data.  #
# #
# This is how everyone learns R!#
# #
# Yes, it is OK to copy and modify the R code#
# #
# Rubric: 10 points total#
# 2 - plot readable; y-axis appropriately logged or not#
#       * The plot MUST be viewable IN THE CANVAS POST, it MUST NOT be#
#         a separate attachment#
# 2 - plot colors, lines, style; plot has appropriate title, axis labels, legend if needed, etc.#
# 4 - paragraph interpretation#
# 2 - two polite, professional responses#
# #
########################################################
########################################################
# Plotting to PDF#
##
# Because the graphics window in RStudio can be different#
# sizes and shapes, depending on your setup, it can be#
# difficult to write code that makes a plot "look good",#
# especially in a small window.#
##
# One solution is to send your plots to a PDF. This will#
# *probably* work on your desktop/laptop -- the main thing that #
# is required is that the computer have a PDF viewer installed.#
##
# If it *doesn't* work, you can either (a) install a PDF viewer, or#
# (b) e.g. for a lab computer where you can't install things, you#
# can manually open the PDF in a web browser.#
########################################################
########################################################
# The first example plot, as a PDF#
########################################################
#
pdffn = "number_of_new_cases_NZ.pdf"  # filename of PDF#
pdf(file=pdffn, width=10, height=8)    # pdf() opens the PDF for writing - won't close until dev.off()!!!#
#
# Code for your plot#
plot(x=as.Date(d$date), y=d$new_cases, xlab="Date", ylab="Number of new cases", pch=1, col="red", cex=0.5)#
titletxt = paste0("Number of new cases per day in: ", country_name)#
title(titletxt)#
lines(x=as.Date(d$date), y=d$new_cases, lty="solid", lwd=2, col="red")#
#
dev.off()	# closes the PDF for writing; after this, new plots go to screen again#
cmdstr = paste0("open ", pdffn)  # create the command to open the PDF#
system(cmdstr)                   # open the PDF in the operating system#
########################################################
# The second example plot, as a PDF#
########################################################
#
pdffn = "number_of_active_recovered_cases_NZ.pdf"  # filename of PDF#
pdf(file=pdffn, width=10, height=8)    # pdf() opens the PDF for writing - won't close until dev.off()!!!#
#
# Code for your plot#
# Plot the active cases, AND recovered cases#
# Plot a plot of white dots (just to set the plot scale)#
xvals = c(as.Date(d2$date), as.Date(d2$date))#
yvals = c(active_cases, recovered_cases)#
plot(x=xvals, y=yvals, xlab="Date", ylab="Number of cases", pch=".", col="white", cex=0.5)#
titletxt = paste0("Number of active & recovered cases in: ", country_name)#
title(titletxt)#
#
# Notes:#
# for help on plots, ?plot#
# for help on point characters (pch), ?points#
# for other graphical parameters (e.g. lty = line type), ?par#
#
# Add points & line for active cases#
points(x=as.Date(d2$date), y=active_cases, pch=1, col="red", cex=0.6)#
lines(x=as.Date(d2$date), y=active_cases, lty="solid", lwd=2, col="red2")#
#
# Add points & line for recovered cases#
points(x=as.Date(d2$date), y=recovered_cases, pch=2, col="green3", cex=0.5)#
lines(x=as.Date(d2$date), y=recovered_cases, lty="solid", lwd=2, col="green4")#
#
# Add vertical line with abline, and text, for Level 4 lockdown start#
lockdown_time = as.Date("2020-03-26")#
abline(v=lockdown_time, col="black", lty="dashed", lwd=2)#
text(x=lockdown_time, y=0.99*max(yvals), labels="lockdown begins", pos=2, srt=90, cex=0.7)#
#
# Add vertical line with abline, and text, for Level 4 lockdown end#
lockdown_time = as.Date("2020-04-28")#
abline(v=lockdown_time, col="black", lty="dashed", lwd=2)#
text(x=lockdown_time, y=0.99*max(yvals), labels="lockdown ends", pos=2, srt=90, cex=0.7)#
# Add horizontal line with abline#
threshold = 1000#
abline(h=threshold, col="grey", lty="dotted", lwd=2)#
text_x = min(xvals) + 0.05*(max(xvals) - min(xvals))#
text(x=text_x, y=threshold, labels="(1000 cases)", pos=3, srt=0, cex=0.7, col="grey")#
#
# Add legend#
legend(x="right", legend=c("active cases", "recovered"), col=c("red2", "green4"), lwd=c(2,2), pch=c(1,2))#
#
dev.off()	# closes the PDF for writing; after this, new plots go to screen again#
cmdstr = paste0("open ", pdffn)  # create the command to open the PDF#
system(cmdstr)                   # open the PDF in the operating system
# Let's consider some lizard-flip data.  #
##
# Here are 100 lizard flips:#
#
lizard_flips = c('H','T','H','T','H','H','T','H','H','H','T','H','H','T','T','T','T','H','H','H','H','H','H','H','H','H','H','H','H','H','H','H','H','T','T','T','H','T','T','T','H','T','T','T','H','H','H','T','T','H','H','H','T','H','H','H','T','T','H','H','H','H','H','H','H','T','T','H','H','H','H','T','T','H','H','H','T','T','H','H','H','H','H','H','T','T','T','H','H','H','H','H','H','T','H','T','H','H','T','T')#
#
# Look at the data#
lizard_flips#
#
# What is your guess at "ProbHeads", the probability of heads?#
##
#
# In the case of binomial data, we actually have a simple #
# formula to calculate the best estimate of ProbHeads:#
#
# Find the heads#
heads_TF = (lizard_flips == "H")#
heads_TF#
#
# Find the tails#
tails_TF = (lizard_flips == "T")#
tails_TF#
#
numHeads = sum(heads_TF)#
numHeads#
#
numTails = sum(tails_TF)#
numTails#
#
numTotal = length(lizard_flips)#
numTotal#
#
# Here's the formula:#
ProbHeads_ML_estimate = numHeads / numTotal#
ProbHeads_ML_estimate#
#
# PLEASE ANSWER Lab 11 QUESTION #1, NOW:#
##
# 1. What number (exactly) was reported for ProbHeads_ML_estimate?  #
# #
# (Obviously the answer is 0.65, but I want you to realize that #
# the computer will auto-grade this.  So, 0.65 will be marked correct, #
# but .65, 0.650, etc., will be marked incorrect.  Extra spaces may #
# also cause the answer to be marked incorrect, e.g. "0.65" is correct, #
# but " 0.65" or "0.65 " are incorrect. For future answers, when numbers #
# are asked for, be sure to report the exact number R gave, without #
# spaces etc.)#
# Well, duh, that seems pretty obvious.  At least it would have been, if we #
# weren't thinking of lizards, where we have a strong prior belief that the#
# lizard is probably fair.#
# #
# It turns out that this formula can be justified through a technique#
# known as Maximum Likelihood.#
##
# What does it mean to say we have a "maximum likelihood" estimate of ProbHeads?#
# #
# "Likelihood", in statistics, means "the probability of the data under the model"#
##
# So, a "Maximum Likelihood estimate" of ProbHeads is the value of the parameter#
# ProbHeads that maximizes the probability of seeing the data you saw, namely,#
# 65 Heads and 35 Tails.#
##
# Comparing likelihoods for different possible parameter values of ProbHeads#
# #
# Let's calculate the probability of these same lizard flip data under the #
# hypothesis/model that ProbHeads=0.5#
# #
# We'll be very inefficient, and use a for-loop, and#
# if/else statements#
#
# Loop through all 100 flips#
# Make a list of the probability of #
# each datum#
ProbHeads_guess = 0.5#
#
# Empty list of probabilities#
probs_list = rep(NA, times=length(lizard_flips))#
probs_list#
#
for (i in 1:length(lizard_flips))#
    {#
    # Print an update#
    cat("\nAnalysing lizard flip #", i, "/", length(lizard_flips), sep="")#
#
    # Get the current lizard flip#
    lizard_flip = lizard_flips[i]#
#
    # If the lizard flip is heads, give that datum#
    # probability ProbHeads_guess.#
    # If tails, give it (1-ProbHeads_guess)#
#
    if (lizard_flip == "H")#
        {#
        probs_list[i] = ProbHeads_guess#
        } # End if heads#
#
    if (lizard_flip == "T")        #
        {#
        probs_list[i] = (1-ProbHeads_guess)#
        } # End if tails#
    } # End for-loop#
#
# Look at the resulting probabilities#
probs_list#
#
# We get the probability of all the data by multiplying#
# all the probabilities, with the prod() function.#
likelihood_of_data_given_ProbHeads_guess1 = prod(probs_list)#
likelihood_of_data_given_ProbHeads_guess1#
#
# PLEASE ANSWER Lab 11, QUESTION #2, NOW:#
# 2. What number (exactly) was reported for likelihood_of_data_given_ProbHeads_guess1?#
# That's a pretty small number!  You'll see that it's #
# just 0.5^100:#
0.5^100#
#
# A probability of 0.5 is not small, but multiply it #
# 100 values of 0.5 together, and you get a small value.#
# That's the probability of that specific sequence of #
# heads/tails, given the hypothesis that the true#
# probability is ProbHeads_guess.#
#
# Let's try another parameter value:#
#
# Loop through all 100 flips#
# Make a list of the probability of #
# each datum#
ProbHeads_guess = 0.7#
#
# Empty list of probabilities#
probs_list = rep(NA, times=length(lizard_flips))#
probs_list#
#
for (i in 1:length(lizard_flips))#
    {#
    # Print an update#
    cat("\nAnalysing lizard flip #", i, "/", length(lizard_flips), sep="")#
#
    # Get the current lizard flip#
    lizard_flip = lizard_flips[i]#
#
    # If the lizard flip is heads, give that datum#
    # probability ProbHeads_guess.#
    # If tails, give it (1-ProbHeads_guess)#
#
    if (lizard_flip == "H")#
        {#
        probs_list[i] = ProbHeads_guess#
        } # End if heads#
#
    if (lizard_flip == "T")        #
        {#
        probs_list[i] = (1-ProbHeads_guess)#
        } # End if tails#
    } # End for-loop#
#
# Look at the resulting probabilities#
probs_list#
#
# We get the probability of all the data by multiplying#
# all the probabilities#
likelihood_of_data_given_ProbHeads_guess2 = prod(probs_list)#
likelihood_of_data_given_ProbHeads_guess2#
#
# PLEASE ANSWER Lab 11, QUESTION #3, NOW:#
# 3. What number (exactly) was reported for likelihood_of_data_given_ProbHeads_guess2?#
#
# We got a different likelihood. It's also very small.#
# But that's not important. What's important is, #
# how many times higher is it?#
#
likelihood_of_data_given_ProbHeads_guess2 / likelihood_of_data_given_ProbHeads_guess1#
#
# PLEASE ANSWER Lab 11, QUESTION #4, NOW:#
# 4. What number (exactly) was reported for #
# likelihood_of_data_given_ProbHeads_guess2 / likelihood_of_data_given_ProbHeads_guess1 ?#
# Whoa!  That's a lot higher!  This means the lizard flip data is 54 times more#
# probable under the hypothesis that ProbHeads=0.7 than under the #
# hypothesis that ProbHeads=0.5.#
#
# Maximum likelihood: Hopefully you can see that the data is in some sense#
# "best explained" by the parameter value (the valueof ProbHeads) that #
# maximizes the probability of the data. This is what we call the Maximum #
# Likelihood solution.#
#
# To try different values of ProbHeads, we could keep copying and pasting code, #
# but that seems annoying.  Let's define an R "function" instead.#
##
# Functions, in R, collect a number of commands, and run them when you#
# type the name of the function. Functions are simple programs. All of #
# the R commands you have ever run, e.g. mean(), prod(), etc. are all#
# just functions that R has pre-programmed.#
##
# NOTES ON FUNCTIONS IN R:#
# * To run, functions have to be followed by (). Usually, but not always,#
#   inputs go into the ().  One example of a function that needs no inputs#
#   is getwd(), which gets your current working directory (wd) and prints it#
#   to screen:#
#
getwd()#
#
# * To see the code that is running inside the function, you can often #
#   just type the name of the function, without the (). This works for#
#   most R functions except for "primitive"  ones (these are core R#
#   functions, like getwd, mean, length, etc., that are programmed#
#   a different way to be super-fast).#
##
# * Typing the names of functions and figuring out exactly what is #
#   happening in the code is a great way to learn R and learn a #
#   specific scientific field, since you can see *exactly* what #
#   some scientist is doing in their analysis.#
##
# * For functions that are part of packages, you can get the help#
#   page by type a ? before the function name. E.g., ?mean will open#
#   the help page for the "mean" function.  R help is often not #
#   written for beginners, but it is better than nothing.#
##
# * When you try to run a function and get an error, this usually#
#   means you left out a required input, or you put in the wrong#
#   kind of input (e.g., if the input is supposed to be a number,#
#   and you put in a word like "hello", you will get an error #
#   message.#
##
# When you type #
#
mean("hello")#
#
# ...what happens?#
#
# PLEASE ANSWER Lab 11, QUESTION #5, NOW:#
# 5. When you run 'mean("hello")', paste in the last part of the error message you get back. #
# Your paste should being with "argument" and end with "NA".#
# Let's define a function#
# #
# This function calculates the probability of a #
# sequence of lizard flip data, given a value of ProbHeads_guess#
calc_prob_lizard_flip_data <- function(ProbHeads_guess, lizard_flips)#
    {#
    # Empty list of probabilities#
    probs_list = rep(NA, times=length(lizard_flips))#
    probs_list#
#
    for (i in 1:length(lizard_flips))#
        {#
        # Print an update#
        #cat("\nAnalysing lizard flip #", i, "/", length(lizard_flips), sep="")#
#
        # Get the current lizard flip#
        lizard_flip = lizard_flips[i]#
#
        # If the lizard flip is heads, give that datum#
        # probability ProbHeads_guess.#
        # If tails, give it (1-ProbHeads_guess)#
#
        if (lizard_flip == "H")#
            {#
            probs_list[i] = ProbHeads_guess#
            } # End if heads#
#
        if (lizard_flip == "T")        #
            {#
            probs_list[i] = (1-ProbHeads_guess)#
            } # End if tails#
        } # End for-loop#
#
    # Look at the resulting probabilities#
    probs_list#
#
    # We get the probability of all the data by multiplying#
    # all the probabilities#
    likelihood_of_data_given_ProbHeads_guess = prod(probs_list)#
#
    # Return result#
    return(likelihood_of_data_given_ProbHeads_guess)#
    }#
#
# Now, we can just use this function:#
calc_prob_lizard_flip_data(ProbHeads_guess=0.5, lizard_flips=lizard_flips)#
calc_prob_lizard_flip_data(ProbHeads_guess=0.6, lizard_flips=lizard_flips)#
calc_prob_lizard_flip_data(ProbHeads_guess=0.7, lizard_flips=lizard_flips)#
#
# Look at that!  We did all of that work in a split-second.#
#
# PLEASE ANSWER Lab 11, QUESTION #6, NOW:#
# 6. What number (exactly) was reported for #
# calc_prob_lizard_flip_data(ProbHeads_guess=0.6, lizard_flips=lizard_flips) ?#
#
# In fact, we can make another for-loop, and search for the ML#
# value of ProbHeads by trying all of the values and plotting them.#
#
# Sequence of 50 possible values of ProbHeads between 0 and 1#
ProbHeads_values_to_try = seq(from=0, to=1, length.out=50)#
likelihoods = rep(NA, times=length(ProbHeads_values_to_try))#
#
for (i in 1:length(ProbHeads_values_to_try))#
    {#
    # Get the current guess at ProbHeads_guess#
    ProbHeads_guess = ProbHeads_values_to_try[i]#
#
    # Calculate likelihood of the lizard flip data under#
    # this value of ProbHeads#
    likelihood = calc_prob_lizard_flip_data(ProbHeads_guess=ProbHeads_guess, lizard_flips=lizard_flips)#
#
    # Store the likelihood value#
    likelihoods[i] = likelihood#
    } # End for-loop#
#
# Here are the resulting likelihoods:#
likelihoods#
#
# Let's try plotting the likelihoods to see if there's a peak#
plot(x=ProbHeads_values_to_try, y=likelihoods)#
lines(x=ProbHeads_values_to_try, y=likelihoods)#
#
# Whoa! That's quite a peak!  You can see that the likelihoods#
# vary over several orders of magnitude.#
##
# Partially because of this extreme variation, we often use the #
# logarithm of the likelihood, also known as #
##
# * log-likelihood#
# * lnL#
##
# In R, the log() function returns the natural logarithm, while#
# log10 returns the logarithms at base 10. I mention this because#
# on some calculators, the LN button returns the natural log,#
# and the LOG button returns the log10 logarithm.#
##
##
# There are several reasons scientists, and scientific papers,#
# often use the log-likelihood instead of the raw likelihood. #
# These include:#
# #
# * Computers have a minimum numerical precision. If a likelihood#
#   value gets too small, it gets rounded to 0. You can see this#
#   happen if you calculate the raw likelihood of over 1000 lizard-flips#
#   using the parameter ProbHeads = 0.5#
##
# Machine precision can vary between computers, so I will give you#
# the results on my machine (but, try it on yours):#
#
0.5^1073#
#
# ...equals 9.881313e-324, which is a very small number#
#
0.5^1074#
#
# ...equals 4.940656e-324, which is half the size of the previous number,#
# which makes sense, because we just multiplied it by 0.5#
##
# However, on my machine,#
#
0.5^1075#
#
# ...equals 0. Instead of one more lizard-flip causing the likelihood to be #
# halved again, zero is infinitely less than 4.940656e-324. So the #
# computer has made a "mistake" here. #
##
# Log-likelihoods do not have this problem. Compare:#
#
1073 * log(0.5)#
1074 * log(0.5)#
1075 * log(0.5)#
1076 * log(0.5)#
#
# We can convert log-likelihoods to raw-likelihood with exp(), which reverses#
# the log() operation.#
#
exp(1073 * log(0.5))#
exp(1074 * log(0.5))#
exp(1075 * log(0.5))#
exp(1076 * log(0.5))#
#
# You can see that, working in log-likelihood space, we have no #
# numerical precision issues, but with raw likelihoods, we do.#
#
# PLEASE ANSWER Lab 11, QUESTION #7, NOW:#
# 7. What number (exactly) was reported for 1075 * log(0.5) ?#
# Other reasons scientists use log-likelihoods instead of likelihoods:#
# #
# * The log-likelihood and raw-likelihood will have their maximums#
#   at the same parameter values.#
# #
# * Taking the logarithm of an equation converts multiplication to#
#   addition, which can make calculus much easier.#
##
# * Log-likelihoods are easier to read and interpret than raw #
#   likelihoods, once you get used to log-likelihoods. For any#
#   large dataset, the raw likelihood will be a very tiny#
#   number, hard to read/remember/interpret.#
##
# * It turns out that a number of fundamental statistical measures#
#   and tests use the log-likelihood as an input. These include#
#   the Likelihood Ratio Test (LRT) and Akaike Information Criterion (AIC).#
#   We will talk about these in future weeks.#
# PLOTTING THE LOG-LIKELIHOOD#
#
# We will repeat the code to calculate the likelihood#
# for a sequence of 50 possible values of ProbHeads between 0 and 1:#
#
ProbHeads_values_to_try = seq(from=0, to=1, length.out=50)#
likelihoods = rep(NA, times=length(ProbHeads_values_to_try))#
#
for (i in 1:length(ProbHeads_values_to_try))#
    {#
    # Get the current guess at ProbHeads_guess#
    ProbHeads_guess = ProbHeads_values_to_try[i]#
#
    # Calculate likelihood of the lizard flip data under#
    # this value of ProbHeads#
    likelihood = calc_prob_lizard_flip_data(ProbHeads_guess=ProbHeads_guess, lizard_flips=lizard_flips)#
#
    # Store the likelihood value#
    likelihoods[i] = likelihood#
    } # End for-loop#
#
# Here are the resulting likelihoods:#
likelihoods#
#
# Let's take the log (the natural log, i.e. the base is exp(1)).#
log_likelihoods = log(likelihoods, base=exp(1))#
#
plot(x=ProbHeads_values_to_try, y=log_likelihoods)#
lines(x=ProbHeads_values_to_try, y=log_likelihoods)#
#
# Let's plot the raw- and log-likelihood together:#
par(mfrow=c(2,1))#
plot(x=ProbHeads_values_to_try, y=likelihoods, main="Likelihood (L) of the data")#
lines(x=ProbHeads_values_to_try, y=likelihoods)#
#
plot(x=ProbHeads_values_to_try, y=log_likelihoods, main="Log-likelihood (LnL) of the data")#
lines(x=ProbHeads_values_to_try, y=log_likelihoods)#
#
# PLEASE ANSWER Lab 11, QUESTION #8, NOW:#
# 8. Compare the raw-likelihood curve and the log-likelihood curve. Does it look #
# like the top of each curve appears in about the same location on the #
# x-axis? (i.e., at the same parameter value for ProbHeads?)#
# #
# Maximum likelihood optimization#
# #
# You can see that the maximum likelihood of the data occurs when #
# ProbHeads is somewhere around 0.6 or 0.7.  What is it #
# exactly?#
##
# We could just keep trying more values until we find whatever#
# precision we desire.  But, R has a function for#
# maximum likelihood optimization!#
# #
# It's called optim().  Optim() takes a function as an input.#
# Fortunately, we've already written a function!#
##
# Let's modify our function a bit to return the log-likelihood,#
# and print the result:#
#
# Function that calculates the probability of lizard flip data#
# given a value of ProbHeads_guess#
calc_prob_lizard_flip_data2 <- function(ProbHeads_guess, lizard_flips)#
    {#
    # Empty list of probabilities#
    probs_list = rep(NA, times=length(lizard_flips))#
    probs_list#
#
    for (i in 1:length(lizard_flips))#
        {#
        # Print an update#
        #cat("\nAnalysing lizard flip #", i, "/", length(lizard_flips), sep="")#
#
        # Get the current lizard flip#
        lizard_flip = lizard_flips[i]#
#
        # If the lizard flip is heads, give that datum#
        # probability ProbHeads_guess.#
        # If tails, give it (1-ProbHeads_guess)#
#
        if (lizard_flip == "H")#
            {#
            probs_list[i] = ProbHeads_guess#
            } # End if heads#
#
        if (lizard_flip == "T")        #
            {#
            probs_list[i] = (1-ProbHeads_guess)#
            } # End if tails#
        } # End for-loop#
#
    # Look at the resulting probabilities#
    probs_list#
#
    # We get the probability of all the data by multiplying#
    # all the probabilities#
    likelihood_of_data_given_ProbHeads_guess = prod(probs_list)#
#
    # Get the log-likelihood#
    LnL = log(likelihood_of_data_given_ProbHeads_guess)#
    LnL#
#
    # Error correction: if -Inf, reset to a low value#
    if (is.finite(LnL) == FALSE)#
        {#
        LnL = -1000#
        }#
#
    # Print some output#
    print_txt = paste("\nWhen ProbHeads=", ProbHeads_guess, ", LnL=", LnL, sep="")#
    cat(print_txt)#
#
    # Return result#
    return(LnL)#
    }#
#
# Try the function out:#
LnL = calc_prob_lizard_flip_data2(ProbHeads_guess=0.1, lizard_flips=lizard_flips)#
LnL = calc_prob_lizard_flip_data2(ProbHeads_guess=0.2, lizard_flips=lizard_flips)#
LnL = calc_prob_lizard_flip_data2(ProbHeads_guess=0.3, lizard_flips=lizard_flips)#
#
# Looks like it works!  Let's use optim() to search for he #
# best ProbHeads value:#
#
# Set a starting value of ProbHeads#
starting_value = 0.1#
#
# Set the limits of the search#
limit_bottom = 0#
limit_top = 1#
#
optim_result = optim(par=starting_value, fn=calc_prob_lizard_flip_data2, lizard_flips=lizard_flips, method="L-BFGS-B", lower=limit_bottom, upper=limit_top, control=list(fnscale=-1))#
#
# You can see the search print out as it proceeds.#
#
# Let's see what ML search decided on:#
optim_result#
#
# Let's compare the LnL from ML search, with the binomial mean#
optim_result$par#
#
# Here's the formula:#
ProbHeads_ML_estimate = numHeads / numTotal#
ProbHeads_ML_estimate
fn = "https://covid.ourworldindata.org/data/owid-covid-data.csv"#
# BE PATIENT AND WAIT FOR THE read.csv COMMAND TO FINISH. #
# (A backup is available on Canvas if needed)#
data = read.csv(file=fn)#
dim(data)
library(deSolve)
install.packages("deSolve")
library(deSolve)
likelihood_single_point <- function(data.point, model.point, log=TRUE)#
	{#
	# Get the probability density of the count, assuming#
	# the observed count is a poisson process with an #
	# average value matching the true process.#
	##
	# This allows for variation, e.g. if a case is reported #
	# days after it occurs.#
	likelihood = suppressWarnings(dpois(x=data.point, lambda=model.point, log=log))#
	# Return the likelihood value#
	return(likelihood)#
	} # END of likelihood_single_point#
#
likelihood_time_series <- function(data.points, model.points, log=TRUE)#
	{#
	# Error check - return a stop() message if check fails#
	if (length(data.points) != length(model.points))#
		{ stop("ERROR in likelihood_time_series(): the lengths of data.points and model.points must match.") }#
	# Calculate the likelihoods of the data.points#
	likelihoods = mapply(FUN=likelihood_single_point, data.point=data.points, model.point=model.points, MoreArgs=list(log=log))#
	# If you calculated log-likelihoods, then sum them#
	# If you calculated raw likelihoods, then multiply them#
	if (log == TRUE)#
		{#
		total_likelihood = sum(likelihoods)#
		} else {#
		total_likelihood = prod(likelihoods)  # prod=take the product#
		} # END if/else statement: if (log == TRUE)#
	# Return that value#
	return (total_likelihood)#
	} # END of likelihood_time_series
library(deSolve)#
#
# SIR_ode -- this function defines the ordinary differential equations of the SIR model#
#
SIR_ode <- function(time, state, parameters)#
	{#
	# The above inputs are:#
	# time = a time point#
	# state = an R list of the 3 state values, i.e. the #
	#         population sizes of S, I, and R#
	# parameters = an R list of the parameters R0 and D_inf#
	# Convert the input parameters "R0" and "D_inf" #
	# into the rates "beta" and "nu"#
	beta <- parameters[["R0"]] / parameters[["D_inf"]]#
	nu <- 1 / parameters[["D_inf"]]#
#
	# Extract the current values of the states S, I, and R#
	S <- state[["S"]]#
	I <- state[["I"]]#
	R <- state[["R"]]#
#
	# Calculate N, the total of S+I+R, #
	# i.e. the population size#
	N <- S + I + R#
	# Write out the system of #
	# Ordinary Differential Equations (ODEs)#
	dS <- -beta * I/N * S #
	dI <- beta * I/N * S - nu * I#
	dR <- nu * I#
	# Return the current rates of change of S, I, and R:#
	return(list(c(dS, dI, dR)))#
	} # End of function SIR_ode#
#
# simulate_SIR() -- this runs the simulation, with inputs:#
# * theta -- an R list, containing the parameters R0 and D_inf#
# * init.state -- the starting counts for S, I, and R individuals#
# * times -- the time-points at which to measure the epidemic#
#
simulate_SIR <- function(theta, init.state, times)#
	{#
	trajectory <- data.frame(ode(y = init.state,#
                times = times,#
                func = SIR_ode,#
                parms = theta,#
                method = "ode45"))#
	return(trajectory)#
	} # End of simulate_SIR()#
#
# simulate_SIR_changes() -- this runs an SIR model in which parameters can change at specific times#
# * thetas_states_df, is an R data.frame#
# containing the states and parameters at the start,#
# followed by rows describing new parameters at #
# any time points you like.#
# * times, are the times to record the epidemic trajectory at#
#
simulate_SIR_changes <- function(thetas_states_df, times)#
	{#
	# Loop through the rows#
	trajectory = NULL#
	projected_new_cases = NULL#
	projected_recoveries = NULL#
	for (rn in 1:nrow(thetas_states_df))#
		{#
		if (rn == 1)#
			{#
			init.state = c(S=thetas_states_df$S[rn], I=thetas_states_df$I[rn], R=thetas_states_df$R[rn])#
			theta = list(R0=as.numeric(thetas_states_df$R0[rn]), D_inf=as.numeric(thetas_states_df$D_inf[rn]))#
			} else {#
			last_row = trajectory[nrow(trajectory),]#
			init.state = c(S=last_row$S+thetas_states_df$S[rn], I=last_row$I+thetas_states_df$I[rn], R=last_row$R+thetas_states_df$R[rn])#
			theta = list(R0=as.numeric(thetas_states_df$R0[rn]), D_inf=as.numeric(thetas_states_df$D_inf[rn]))			#
			}#
		# Read in the parameters for the initial, versus later, time-bins#
		if (rn >= nrow(thetas_states_df))#
			{#
			start_time = thetas_states_df$time[rn]#
			end_time = max(times)#
			} else {#
			start_time = thetas_states_df$time[rn]#
			end_time = thetas_states_df$time[rn+1]#
			}#
		TF1 = times >= start_time#
		TF2 = times <= end_time#
		TF = (TF1 + TF2) == 2#
		tmp_times = times[TF]#
#
		tmp_trajectory = simulate_SIR(theta=theta, init.state=init.state, times=tmp_times)#
		if (rn == 1)#
			{#
			trajectory = rbind(trajectory, tmp_trajectory)#
			} else {#
			trajectory_minus_last_state = trajectory[-nrow(trajectory),]#
			trajectory = rbind(trajectory_minus_last_state, tmp_trajectory)#
			} # if (rn == 1)#
#
		} # END for (rn in 1:nrow(thetas_states_df))#
#
	# Add the projected net new cases#
	projected_net_new_cases	= trajectory$I[2:length(trajectory$I)] - trajectory$I[1:(length(trajectory$I)-1)]#
	projected_net_new_cases = c(thetas_states_df$I[1], projected_net_new_cases)#
	# Subtract the projected recoveries#
	projected_recoveries = trajectory$R[2:length(trajectory$R)] - trajectory$R[1:(length(trajectory$R)-1)]#
	projected_recoveries = c(0, projected_recoveries)#
	# Add back in the projected recoveries, so you get the total number of new cases#
	projected_new_cases = projected_net_new_cases + projected_recoveries#
	trajectory = cbind(trajectory, projected_net_new_cases, projected_recoveries, projected_new_cases)#
#
	return(trajectory)#
	} # End simulate_SIR_changes()#
# Function definition for params_to_likelihood_v2#
params_to_likelihood_v2 <- function(params, data.points, thetas_states_df, delay_val=0.0, printvals=TRUE, use_just_new_cases=TRUE, observed_recoveries=NULL)#
	{#
	if (nrow(thetas_states_df) < 2)#
		{#
		txt = "STOP ERROR in params_to_likelihood_v2(): Your input parameters table, 'thetas_states_df', must have 2 or more rows in it. Check you thetas_states_df input, and try again."#
		stop(txt)#
		}#
	# The times are just the length of the data.points#
	times = 1:length(data.points)#
#
	# (1) input the free parameters#
	thetas_states_temp = thetas_states_df#
	TF = thetas_states_temp == "free"#
	thetas_states_temp[TF] = params#
	# Ensure everything is numeric#
	thetas_states_temp$time = as.numeric(thetas_states_temp$time)#
	thetas_states_temp$R0 = as.numeric(thetas_states_temp$R0)#
	thetas_states_temp$D_inf = as.numeric(thetas_states_temp$D_inf)#
	thetas_states_temp$S = as.numeric(thetas_states_temp$S)#
	thetas_states_temp$I = as.numeric(thetas_states_temp$I)#
	thetas_states_temp$R = as.numeric(thetas_states_temp$R)#
	# You have to round the "time" variable to the nearest day#
	thetas_states_temp$time = round(thetas_states_temp$time, digits=0)#
	# Error checks, e.g. for "time" values out of order,#
	# "time" outside of min/max#
	time_inputs_valid = TRUE#
	for (i in 2:length(thetas_states_temp$time))#
		{#
		if (thetas_states_temp$time[i] <= thetas_states_temp$time[i-1])#
			{#
			time_inputs_valid = FALSE#
			break()#
			}#
		if (thetas_states_temp$time[i-1] < times[1])#
			{#
			time_inputs_valid = FALSE#
			break()			#
			}#
		if (thetas_states_temp$time[i-1] > times[length(times)])#
			{#
			time_inputs_valid = FALSE#
			break()			#
			}#
		if (thetas_states_temp$time[i] < times[1])#
			{#
			time_inputs_valid = FALSE#
			break()			#
			}#
		if (thetas_states_temp$time[i] > times[length(times)])#
			{#
			time_inputs_valid = FALSE#
			break()			#
			}#
		}#
	# If any of time inputs are invalid, return an absurdly low lnL#
	if (time_inputs_valid== FALSE)#
		{#
		lnL=-1e100#
#
		# Print, if desired#
		if (printvals == TRUE)#
			{#
			print(thetas_states_temp)#
			cat("log-likelihood = ", lnL, "\n", sep="")#
			}#
#
		return(lnL)#
		}#
	# Error check#
	if ((max(thetas_states_temp$time)+delay_val) > max(times))#
		{#
		stop("ERROR in params_to_likelihood_v2: there is a 'thetas_states_df$time'+delay_val greater than found in 'times'")#
		}#
	# Make sure everything in thetas_states_df is numeric, instead of character#
	for (i in 1:ncol(thetas_states_temp))#
		{#
		thetas_states_temp[,i] = as.numeric(thetas_states_temp[,i])#
		}#
	# Ad2 the delay (allows time for interventions to show up#
	# in the detections)#
	thetas_states_temp$time[2:length(thetas_states_temp$time)] = thetas_states_temp$time[2:length(thetas_states_temp$time)]#
	# Calculate the trajectory given the parameters, #
	# input into model.points#
	trajectory = simulate_SIR_changes(thetas_states_df=thetas_states_temp, times)#
	if (use_just_new_cases == TRUE)#
		{#
		# prob of observed new cases#
		model.points = trajectory$projected_new_cases#
		lnL1 = likelihood_time_series(data.points, model.points, log=TRUE)#
		# prob of recoveries#
		model.points = trajectory$projected_recoveries#
		lnL2 = likelihood_time_series(observed_recoveries, model.points, log=TRUE)#
		lnL = lnL1 + lnL2#
#
		} else {#
		# The original way -- modeling the number of active cases#
		model.points = trajectory$I#
		lnL = likelihood_time_series(data.points, model.points, log=TRUE)#
		} # END if (use_just_new_cases == TRUE)#
	if (is.finite(lnL)== FALSE)#
		{#
		lnL=-1e100#
		}#
	# Print, if desired#
	if (printvals == TRUE)#
		{#
		print(thetas_states_temp)#
		cat("log-likelihood = ", lnL, "\n", sep="")#
		}#
#
	return(lnL)#
	} # END params_to_likelihood_v2
########################################################
# LOAD & PROCESS DATA FROM OURWORLDINDATA#
########################################################
#
# Function to help plot your country's data#
# This function does the data-processing we did in the first week:#
# (1) converts "NA" values to 0.0#
# (2) adds up the total number of recovered and active #
#     cases each day.#
calc_active_recovered_cases <- function(d2, add_zeros_back_to=as.Date("2019-12-31"))#
	{#
	# Subset to your country, replace "NA" values with 0:#
	TF = is.na(d2$new_cases)#
	d2$new_cases[TF] = 0#
#
	# Keep a count of active cases, and recovered cases#
	active_cases = rep(0.0, times=nrow(d2))#
	recovered_cases = rep(0.0, times=nrow(d2))#
	net_new_cases = rep(0.0, times=nrow(d2))#
	num_recovered_cases = rep(0.0, times=nrow(d2))#
	# Here, we do a "for-loop". It will leap through every row#
	# of the table, and do a calculation.#
	##
	# NOTE: to make a for-loop run, you have to run#
	# the WHOLE thing, from the "for" to the final closing#
	# bracket "}", at once!#
	# #
	for (i in 1:nrow(d2))#
		{#
		# Keep track of the starting row number#
		if (i <= 6)#
			{#
			startrow = 1#
			endrow = i#
			recovered_cases[i] = 0#
			num_recovered_cases_today = 0#
			} else {#
			startrow = startrow + 1#
			endrow = i#
			sum_of_recovered_cases = sum(d2$new_cases[1:(startrow-1)])#
			recovered_cases[i] = sum_of_recovered_cases#
			num_recovered_cases_today = recovered_cases[i] - recovered_cases[i-1]#
			} # END if/else statement#
		# Add up the last 14 days of active cases#
		sum_of_active_cases_14days = sum(d2$new_cases[startrow:endrow])#
		# Store the result:#
		active_cases[i] = sum_of_active_cases_14days#
		# Calculate net new cases#
		net_new_cases[i] = d2$new_cases[endrow] - num_recovered_cases_today#
		num_recovered_cases[i] = num_recovered_cases_today#
		} # END for (i in 1:nrow(d2))#
	d3 = cbind(d2, active_cases, recovered_cases, net_new_cases, num_recovered_cases)#
	# Add 0s back to 2019-12-31 (like the original OurWorldInData had it)#
	zero_row = d3[1,]#
	zero_row$total_cases = 0#
	zero_row$new_cases = 0#
	zero_row$active_cases = 0#
	zero_row$recovered_cases = 0#
	zero_row$net_new_cases = 0#
	d3_with_zero_rows_df = d3#
	orig_starting_day = as.Date(zero_row$date)#
	current_day = orig_starting_day-1#
	if (is.na(add_zeros_back_to) == FALSE)#
		{#
		if (as.Date(d3$date[1]) > as.Date(add_zeros_back_to))#
			{#
			days_to_add = as.numeric(as.Date(d3$date[1]) - as.Date(add_zeros_back_to))#
			for (j in 1:days_to_add)#
				{#
				zero_row$date = current_day#
				d3_with_zero_rows_df = rbind(zero_row, d3_with_zero_rows_df)#
				current_day = current_day - 1#
				}#
			}#
		return(d3_with_zero_rows_df)#
		}#
	return(d3)#
	} # END calc_active_recovered_cases <- function(d2)#
# Load ODE-solving library#
library(deSolve)#
#
# Data setup#
fn = "https://covid.ourworldindata.org/data/owid-covid-data.csv"#
data = read.csv(file=fn)#
country_name = "New Zealand"#
TF = data$location == country_name#
d2 = data[TF, ]#
d3 = calc_active_recovered_cases(d2)#
head(d3)
plot(x=as.Date(d3$date), y=d3$active_cases, xlab="Date", ylab="Active cases", col="red", pch="+")#
titletxt = paste0("Active COVID-19 cases in ", country_name, "\n(from OurWorldInData)")#
title(titletxt)#
points(x=as.Date(d3$date), y=d3$recovered, col="green3")
dim(calc_active_recovered_cases(d2))#
maximum_date = as.Date("2020-08-15")#
TF = as.Date(calc_active_recovered_cases(d2)$date) < maximum_date  # TF = TRUE/FALSE result#
d3 = calc_active_recovered_cases(d2)[TF,]  # this takes just the rows where TF=TRUE#
dim(d3)#
#
# Plot the data:#
plot(x=as.Date(d3$date), y=d3$active_cases, xlab="Date", ylab="Active cases", col="red", pch="+")#
titletxt = paste0("Active COVID-19 cases in ", country_name, "\n(from OurWorldInData)")#
title(titletxt)#
points(x=as.Date(d3$date), y=d3$recovered, col="green3")
plot(x=as.Date(d3$date), y=d3$active_cases, xlab="Date", ylab="Active cases", col="red", pch="+")#
titletxt = paste0("Active COVID-19 cases in ", country_name, "\n(from OurWorldInData)")#
title(titletxt)#
points(x=as.Date(d3$date), y=d3$recovered, col="green3")
times = 1:nrow(d3)      # Number of days since first day#
#
# Set up regimes:#
# NZ Level 4 lockdown went in on March 26:#
# March 26 = Day 87, +1 for 31/21/2019, +8 for delay to see lockdown effect = 96#
# Allowing an earlier pre-lockdown slowdown due to social distancing & public health = 85#
# Allowing an uptick in July (in real life, these are imported cases)#
case1 = (1:nrow(d3))[d3$active_cases>0][1] # day of the first detected case#
time = c(1, case1) #
R0 = c(0.0, 2.7)    # initial guesses#
D_inf = c(6.0, 6.0) # seems to fit data#
S = c(as.numeric(d3$population[1]), 0) # putting in the country's population size#
I = c(0, 1) # the number of cases on day 1 is a "nuisance parameter"#
R = c(0, 0) # no vaccinations#
thetas_states_table = cbind(time, R0, D_inf, S, I, R)#
thetas_states_df = as.data.frame(thetas_states_table, stringsAsFactors=FALSE)#
thetas_states_df#
#
# Make some parameters into "free" parameters, to be inferred#
thetas_states_df$R0[2] = "free"#
thetas_states_df#
#
# Run the params_to_likelihood_v2() function once, to see your starting likelihood#
data_active_cases = d3$active_cases#
data.points = d3$new_cases#
observed_recoveries = d3$num_recovered_cases#
params = thetas_states_table[thetas_states_df=="free"] # starting parameter values#
lnL_result = params_to_likelihood_v2(params, data.points, thetas_states_df, delay_val=0, printvals=TRUE, use_just_new_cases=TRUE, observed_recoveries=observed_recoveries)#
lnL_result#
#
# Graphing the model model#
thetas_states_ML = thetas_states_df#
TF = thetas_states_ML == "free"#
thetas_states_ML[TF] = thetas_states_table[TF]#
# Conver to numeric#
for (i in 1:ncol(thetas_states_ML))#
	{ thetas_states_ML[,i] = as.numeric(thetas_states_ML[,i]) }#
#
# Plot the results#
trajectory = simulate_SIR_changes(thetas_states_df=thetas_states_ML, times=times)#
maxy = max(max(trajectory$I), max(data_active_cases))#
xvals = c(trajectory$time, times)#
yvals = c(trajectory$I, data_active_cases) + 0.1 # Adding +0.1 to prevent y-axis error with log(0)#
plot(x=xvals, y=yvals, pch=".", col="white", xlim=c(0, max(trajectory$time)), ylim=c(0.1, maxy), xlab="Day", ylab="Number of individuals (log scale)", log="y")#
lines(x=trajectory$time, y=trajectory$I, lwd=3, col="firebrick2")#
points(times, data_active_cases, col="red", pch="+")#
legend(x="topleft", legend=c("Active COVID-19 case count", 'ML-fitted projection of "I" (Infected)'), lty=c("blank", "solid"), lwd=c(1,3), pch=c("+", "."), col=c("red","firebrick2"), cex=0.8)#
#
titletxt = paste0("ML fit, active cases from: ", country_name, "\nM1 (a 2-regime model, R0 fixed to 2.7); max lnL=", round(lnL_result, 2))#
title(titletxt)
# Save this model's parameters and log-likelihood#
thetas_states_ML_model1 = thetas_states_ML#
total_lnL_Model1 = lnL_result
total_lnL_Model1
times = 1:nrow(d3)      # Number of days since first day#
#
# Set up regimes:#
case1 = (1:nrow(d3))[d3$active_cases>0][1] # day of the first detected case#
time = c(1, case1) #
R0 = c(0.0, 3.0)    # initial guesses#
D_inf = c(6.0, 6.0) # seems to fit data#
S = c(as.numeric(d3$population[1]), 0) # putting in the country's population size#
I = c(0, 1) # the number of cases on day 1 is a "nuisance parameter"#
R = c(0, 0) # no vaccinations#
thetas_states_table = cbind(time, R0, D_inf, S, I, R)#
thetas_states_df = as.data.frame(thetas_states_table, stringsAsFactors=FALSE)#
thetas_states_df#
#
# Make some parameters into "free" parameters, to be inferred#
thetas_states_df$R0[2] = "free"#
thetas_states_df#
#
# Run the params_to_likelihood_v2() function once, to see your starting likelihood#
data_active_cases = d3$active_cases#
data.points = d3$new_cases#
observed_recoveries = d3$num_recovered_cases#
params = thetas_states_table[thetas_states_df=="free"] # starting parameter values#
lnL_result = params_to_likelihood_v2(params, data.points, thetas_states_df, delay_val=0, printvals=TRUE, use_just_new_cases=TRUE, observed_recoveries=observed_recoveries)#
# Running the Maximum Likelihood search with the optim() function.#
# LOOK AT THE OUTPUT THAT PRINTS TO SCREEN!!#
ML_results = optim(par=params, fn=params_to_likelihood_v2, data.points=data.points, thetas_states_df=thetas_states_df, delay_val=0, printvals=TRUE, method="L-BFGS-B", lower=0.0, control=list(fnscale=-1), use_just_new_cases=TRUE, observed_recoveries=observed_recoveries)#
#
# Graphing the ML model#
# Take the learned parameters from "ML_results", put them#
# into a theta_states data.frame for simulation and plotting#
thetas_states_ML = thetas_states_df#
TF = thetas_states_ML == "free"#
thetas_states_ML[TF] = ML_results$par#
for (i in 1:ncol(thetas_states_ML))#
	{ thetas_states_ML[,i] = as.numeric(thetas_states_ML[,i]) }#
thetas_states_ML$time[2:length(thetas_states_ML$time)] = thetas_states_ML$time[2:length(thetas_states_ML$time)]#
# Plot the results#
trajectory = simulate_SIR_changes(thetas_states_df=thetas_states_ML, times=times)#
maxy = max(max(trajectory$I), max(data_active_cases))#
xvals = c(trajectory$time, times)#
yvals = c(trajectory$I, data_active_cases)#
plot(x=xvals, y=yvals, pch=".", col="white", xlim=c(0, max(trajectory$time)), ylim=c(0, maxy), xlab="Day", ylab="Number of individuals")#
lines(x=trajectory$time, y=trajectory$I, lwd=3, col="firebrick2")#
points(times, data_active_cases, col="red", pch="+")#
legend(x="topleft", legend=c("Active COVID-19 case count", 'ML-fitted projection of "I" (Infected)'), lty=c("blank", "solid"), lwd=c(1,3), pch=c("+", "."), col=c("red","firebrick2"), cex=0.8)#
#
titletxt = paste0("ML fit, active cases from: ", country_name, "\nM2 (a 2-regime, 1 parameter model), max lnL = ", round(ML_results$value, 2))#
title(titletxt)#
# Save this model's parameters and log-likelihood#
thetas_states_ML_model2 = thetas_states_ML#
total_lnL_Model2 = ML_results$value
total_lnL_Model2
########################################################
# Model M3: A 3-regime model (3 free parameters)#
########################################################
times = 1:nrow(d3)      # Number of days since first day#
#
# Set up regimes:#
# NZ Level 4 lockdown went in on March 26:#
# March 26 = Day 87, +1 for 31/21/2019, +8 for delay to see lockdown effect = 96#
case1 = (1:nrow(d3))[d3$active_cases>0][1] # day of the first detected case#
time = c(1, case1, 96) #
R0 = c(0.0, 3.0, 0.3)    # initial guesses#
D_inf = c(6.0, 6.0, 6.0) # seems to fit data#
S = c(as.numeric(d3$population[1]), 0, 0) # putting in the country's population size#
I = c(0, 1, 0) # the number of cases on day 1 is a "nuisance parameter"#
R = c(0, 0, 0) # no vaccinations#
thetas_states_table = cbind(time, R0, D_inf, S, I, R)#
thetas_states_df = as.data.frame(thetas_states_table, stringsAsFactors=FALSE)#
thetas_states_df#
#
# Make some parameters into "free" parameters, to be inferred#
thetas_states_df$I[2] = "free"#
thetas_states_df$R0[2] = "free"#
thetas_states_df$R0[3] = "free"#
thetas_states_df#
#
# Run the params_to_likelihood_v2() function once, to see your starting likelihood#
data_active_cases = d3$active_cases#
data.points = d3$new_cases#
observed_recoveries = d3$num_recovered_cases#
params = thetas_states_table[thetas_states_df=="free"] # starting parameter values#
lnL_result = params_to_likelihood_v2(params, data.points, thetas_states_df, delay_val=0, printvals=TRUE, use_just_new_cases=TRUE, observed_recoveries=observed_recoveries)#
# Running the Maximum Likelihood search with the optim() function.#
# LOOK AT THE OUTPUT THAT PRINTS TO SCREEN!!#
ML_results = optim(par=params, fn=params_to_likelihood_v2, data.points=data.points, thetas_states_df=thetas_states_df, delay_val=0, printvals=TRUE, method="L-BFGS-B", lower=0.0, control=list(fnscale=-1), use_just_new_cases=TRUE, observed_recoveries=observed_recoveries)#
#
# Graphing the ML model#
# Take the learned parameters from "ML_results", put them#
# into a theta_states data.frame for simulation and plotting#
thetas_states_ML = thetas_states_df#
TF = thetas_states_ML == "free"#
thetas_states_ML[TF] = ML_results$par#
for (i in 1:ncol(thetas_states_ML))#
	{ thetas_states_ML[,i] = as.numeric(thetas_states_ML[,i]) }#
thetas_states_ML$time[2:length(thetas_states_ML$time)] = thetas_states_ML$time[2:length(thetas_states_ML$time)]#
# Plot the results#
trajectory = simulate_SIR_changes(thetas_states_df=thetas_states_ML, times=times)#
maxy = max(max(trajectory$I), max(data_active_cases))#
xvals = c(trajectory$time, times)#
yvals = c(trajectory$I, data_active_cases)#
plot(x=xvals, y=yvals, pch=".", col="white", xlim=c(0, max(trajectory$time)), ylim=c(0, maxy), xlab="Day", ylab="Number of individuals")#
lines(x=trajectory$time, y=trajectory$I, lwd=3, col="firebrick2")#
points(times, data_active_cases, col="red", pch="+")#
legend(x="topleft", legend=c("Active COVID-19 case count", 'ML-fitted projection of "I" (Infected)'), lty=c("blank", "solid"), lwd=c(1,3), pch=c("+", "."), col=c("red","firebrick2"), cex=0.8)#
#
titletxt = paste0("ML fit, active cases from: ", country_name, "\nM3 (a 3-regime, 3 param model), max lnL = ", round(ML_results$value, 2))#
title(titletxt)#
# Save this model's parameters and log-likelihood#
thetas_states_ML_model3 = thetas_states_ML#
total_lnL_Model3 = ML_results$value
total_lnL_Model3
########################################################
# Model 4: A 4-regime model (4 free parameters)#
########################################################
times = 1:nrow(d3)      # Number of days since first day#
#
# Set up regimes:#
# NZ Level 4 lockdown went in on March 26:#
# March 26 = Day 87, +1 for 31/21/2019, +8 for delay to see lockdown effect = 96#
# Allowing an uptick in July (in real life, these are imported cases) = 140#
case1 = (1:nrow(d3))[d3$active_cases>0][1] # day of the first detected case#
time = c(1, case1, 96, 140) #
R0 = c(0.0, 3.0, 0.3, 1.5)    # initial guesses#
D_inf = c(6.0, 6.0, 6.0, 6.0) # seems to fit data#
S = c(as.numeric(d3$population[1]), 0, 0, 0) # putting in the country's population size#
I = c(0, 1, 0, 0) # the number of cases on day 1 is a "nuisance parameter"#
R = c(0, 0, 0, 0) # no vaccinations#
thetas_states_table = cbind(time, R0, D_inf, S, I, R)#
thetas_states_df = as.data.frame(thetas_states_table, stringsAsFactors=FALSE)#
thetas_states_df#
#
# Make some parameters into "free" parameters, to be inferred#
thetas_states_df$I[2] = "free"#
thetas_states_df$R0[2] = "free"#
thetas_states_df$R0[3] = "free"#
thetas_states_df$R0[4] = "free"#
thetas_states_df#
#
# Run the params_to_likelihood_v2() function once, to see your starting likelihood#
data_active_cases = d3$active_cases#
data.points = d3$new_cases#
observed_recoveries = d3$num_recovered_cases#
params = thetas_states_table[thetas_states_df=="free"] # starting parameter values#
lnL_result = params_to_likelihood_v2(params, data.points, thetas_states_df, delay_val=0, printvals=TRUE, use_just_new_cases=TRUE, observed_recoveries=observed_recoveries)#
# Running the Maximum Likelihood search with the optim() function.#
# LOOK AT THE OUTPUT THAT PRINTS TO SCREEN!!#
ML_results = optim(par=params, fn=params_to_likelihood_v2, data.points=data.points, thetas_states_df=thetas_states_df, delay_val=0, printvals=TRUE, method="L-BFGS-B", lower=0.0, control=list(fnscale=-1), use_just_new_cases=TRUE, observed_recoveries=observed_recoveries)#
#
# Graphing the ML model#
# Take the learned parameters from "ML_results", put them#
# into a theta_states data.frame for simulation and plotting#
thetas_states_ML = thetas_states_df#
TF = thetas_states_ML == "free"#
thetas_states_ML[TF] = ML_results$par#
for (i in 1:ncol(thetas_states_ML))#
	{ thetas_states_ML[,i] = as.numeric(thetas_states_ML[,i]) }#
thetas_states_ML$time[2:length(thetas_states_ML$time)] = thetas_states_ML$time[2:length(thetas_states_ML$time)]#
# Plot the results#
trajectory = simulate_SIR_changes(thetas_states_df=thetas_states_ML, times=times)#
maxy = max(max(trajectory$I), max(data_active_cases))#
xvals = c(trajectory$time, times)#
yvals = c(trajectory$I, data_active_cases)#
plot(x=xvals, y=yvals, pch=".", col="white", xlim=c(0, max(trajectory$time)), ylim=c(0, maxy), xlab="Day", ylab="Number of individuals")#
lines(x=trajectory$time, y=trajectory$I, lwd=3, col="firebrick2")#
points(times, data_active_cases, col="red", pch="+")#
legend(x="topleft", legend=c("Active COVID-19 case count", 'ML-fitted projection of "I" (Infected)'), lty=c("blank", "solid"), lwd=c(1,3), pch=c("+", "."), col=c("red","firebrick2"), cex=0.8)#
#
titletxt = paste0("ML fit, active cases from: ", country_name, "\nM4 (a 4-regime model, 4 free params) max lnL = ", round(ML_results$value, 2))#
title(titletxt)#
# Save this model's parameters and log-likelihood#
thetas_states_ML_model4 = thetas_states_ML#
total_lnL_Model4 = ML_results$value
########################################################
# Model M5: A 5-regime model, 5 free parameters#
########################################################
times = 1:nrow(d3)      # Number of days since first day#
#
# Set up regimes:#
# NZ Level 4 lockdown went in on March 26:#
# March 26 = Day 87, +1 for 31/21/2019, +8 for delay to see lockdown effect = 96#
# Allowing an earlier pre-lockdown slowdown due to social distancing & public health = 88#
# Allowing an uptick in July (in real life, these are imported cases) = 140#
case1 = (1:nrow(d3))[d3$active_cases>0][1] # day of the first detected case#
time = c(1, case1, 88, 96, 140) #
R0 = c(0.0, 3.0, 1.5, 0.3, 1.5)    # initial guesses#
D_inf = c(6.0, 6.0, 6.0, 6.0, 6.0) # seems to fit data#
S = c(as.numeric(d3$population[1]), 0, 0, 0, 0) # putting in the country's population size#
I = c(0, 1, 0, 0, 0) # the number of cases on day 1 is a "nuisance parameter"#
R = c(0, 0, 0, 0, 0) # no vaccinations#
thetas_states_table = cbind(time, R0, D_inf, S, I, R)#
thetas_states_df = as.data.frame(thetas_states_table, stringsAsFactors=FALSE)#
thetas_states_df#
#
# Make some parameters into "free" parameters, to be inferred#
thetas_states_df$I[2] = "free"#
thetas_states_df$R0[2] = "free"#
thetas_states_df$R0[3] = "free"#
thetas_states_df$R0[4] = "free"#
thetas_states_df$R0[5] = "free"#
thetas_states_df#
#
# Run the params_to_likelihood_v2() function once, to see your starting likelihood#
data_active_cases = d3$active_cases#
data.points = d3$new_cases#
observed_recoveries = d3$num_recovered_cases#
params = thetas_states_table[thetas_states_df=="free"] # starting parameter values#
lnL_result = params_to_likelihood_v2(params, data.points, thetas_states_df, delay_val=0, printvals=TRUE, use_just_new_cases=TRUE, observed_recoveries=observed_recoveries)#
# Running the Maximum Likelihood search with the optim() function.#
# LOOK AT THE OUTPUT THAT PRINTS TO SCREEN!!#
ML_results = optim(par=params, fn=params_to_likelihood_v2, data.points=data.points, thetas_states_df=thetas_states_df, delay_val=0, printvals=TRUE, method="L-BFGS-B", lower=0.0, control=list(fnscale=-1), use_just_new_cases=TRUE, observed_recoveries=observed_recoveries)#
#
# Graphing the ML model#
# Take the learned parameters from "ML_results", put them#
# into a theta_states data.frame for simulation and plotting#
thetas_states_ML = thetas_states_df#
TF = thetas_states_ML == "free"#
thetas_states_ML[TF] = ML_results$par#
for (i in 1:ncol(thetas_states_ML))#
	{ thetas_states_ML[,i] = as.numeric(thetas_states_ML[,i]) }#
thetas_states_ML$time[2:length(thetas_states_ML$time)] = thetas_states_ML$time[2:length(thetas_states_ML$time)]#
# Plot the results#
trajectory = simulate_SIR_changes(thetas_states_df=thetas_states_ML, times=times)#
maxy = max(max(trajectory$I), max(data_active_cases))#
xvals = c(trajectory$time, times)#
yvals = c(trajectory$I, data_active_cases)#
plot(x=xvals, y=yvals, pch=".", col="white", xlim=c(0, max(trajectory$time)), ylim=c(0, maxy), xlab="Day", ylab="Number of individuals")#
lines(x=trajectory$time, y=trajectory$I, lwd=3, col="firebrick2")#
points(times, data_active_cases, col="red", pch="+")#
legend(x="topleft", legend=c("Active COVID-19 case count", 'ML-fitted projection of "I" (Infected)'), lty=c("blank", "solid"), lwd=c(1,3), pch=c("+", "."), col=c("red","firebrick2"), cex=0.8)#
#
titletxt = paste0("ML fit, active cases from: ", country_name, "\nM5 (a 5-regime, 5 param model) max lnL = ", round(ML_results$value, 2))#
title(titletxt)#
#
# Save this model's parameters and log-likelihood#
thetas_states_ML_model5 = thetas_states_ML#
total_lnL_Model5 = ML_results$value
plot(x)
base::plot()
# Load the packages (after installation, see above).#
library(GenSA)    # GenSA is better than optimx (although somewhat slower)#
library(FD)       # for FD::maxent() (make sure this is up-to-date)#
library(snow)     # (if you want to use multicore functionality; some systems/R versions prefer library(parallel), try either)#
library(parallel)#
library(rexpokit)#
library(cladoRcpp)#
########################################################
# Installing BioGeoBEARS from scratch#
# 2018-10-10 update: I have been putting the #
# updates on CRAN/GitHub#
# You should use:#
# rexpokit version 0.26.6 from CRAN#
# cladoRcpp version 0.15 from CRAN#
# BioGeoBEARS version 1.1 from GitHub, install with:#
library(devtools)#
# devtools::install_github(repo="nmatzke/BioGeoBEARS")#
########################################################
########################################################
########################################################
# #
# #
# As it is easier to maintain#
##
# # Install BioGeoBEARS from CRAN 0-cloud:#
# install.packages("BioGeoBEARS", dependencies=TRUE, repos="http://cran.rstudio.com")#
##
########################################################
library(BioGeoBEARS)#
########################################################
# SETUP: YOUR WORKING DIRECTORY#
########################################################
# You will need to set your working directory to match your local system#
#
# Note these very handy functions!#
# Command "setwd(x)" sets your working directory#
# Command "getwd()" gets your working directory and tells you what it is.#
# Command "list.files()" lists the files in your working directory#
# To get help on any command, use "?".  E.g., "?list.files"#
#
# Set your working directory for output files#
# default here is your home directory ("~")#
# Change this as you like#
wd = np("~")#
setwd(wd)#
#
# Double-check your working directory with getwd()#
getwd()#
#
########################################################
# SETUP: Extension data directory#
########################################################
# When R packages contain extra files, they are stored in the "extdata" directory #
# inside the installed package.#
##
# BioGeoBEARS contains various example files and scripts in its extdata directory.#
# #
# Each computer operating system might install BioGeoBEARS in a different place, #
# depending on your OS and settings. #
# #
# However, you can find the extdata directory like this:#
extdata_dir = np(system.file("extdata", package="BioGeoBEARS"))#
extdata_dir#
list.files(extdata_dir)#
#
# "system.file" looks in the directory of a specified package (in this case BioGeoBEARS)#
# The function "np" is just a shortcut for normalizePath(), which converts the #
# path to the format appropriate for your system (e.g., Mac/Linux use "/", but #
# Windows uses "\\", if memory serves).#
#
# Even when using your own data files, you should KEEP these commands in your #
# script, since the plot_BioGeoBEARS_results function needs a script from the #
# extdata directory to calculate the positions of "corners" on the plot. This cannot#
# be made into a straight up BioGeoBEARS function because it uses C routines #
# from the package APE which do not pass R CMD check for some reason.#
#
########################################################
# SETUP: YOUR TREE FILE AND GEOGRAPHY FILE#
########################################################
# Example files are given below. To run your own data,#
# make the below lines point to your own files, e.g.#
# trfn = "/mydata/frogs/frogBGB/tree.newick"#
# geogfn = "/mydata/frogs/frogBGB/geog.data"#
#
########################################################
# Phylogeny file#
# Notes: #
# 1. Must be binary/bifurcating: no polytomies#
# 2. No negative branchlengths (e.g. BEAST MCC consensus trees sometimes have negative branchlengths)#
# 3. Be careful of very short branches, as BioGeoBEARS will interpret ultrashort branches as direct ancestors#
# 4. You can use non-ultrametric trees, but BioGeoBEARS will interpret any tips significantly below the #
#    top of the tree as fossils!  This is only a good idea if you actually do have fossils in your tree,#
#    as in e.g. Wood, Matzke et al. (2013), Systematic Biology.#
# 5. The default settings of BioGeoBEARS make sense for trees where the branchlengths are in units of #
#    millions of years, and the tree is 1-1000 units tall. If you have a tree with a total height of#
#    e.g. 0.00001, you will need to adjust e.g. the max values of d and e, or (simpler) multiply all#
#    your branchlengths to get them into reasonable units.#
# 6. DON'T USE SPACES IN SPECIES NAMES, USE E.G. "_"#
########################################################
# This is the example Newick file for Hawaiian Psychotria#
# (from Ree & Smith 2008)#
# "trfn" = "tree file name"#
trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
#
# Look at the raw Newick file:#
moref(trfn)#
#
# Look at your phylogeny (plots to a PDF, which avoids issues with multiple graphics in same window):#
pdffn = "tree.pdf"#
pdf(file=pdffn, width=9, height=12)#
#
tr = read.tree(trfn)#
tr#
plot(tr)#
title("Example Psychotria phylogeny from Ree & Smith (2008)")#
axisPhylo() # plots timescale#
#
dev.off()#
cmdstr = paste0("open ", pdffn)#
system(cmdstr)#
#
########################################################
# Geography file#
# Notes:#
# 1. This is a PHYLIP-formatted file. This means that in the #
#    first line, #
#    - the 1st number equals the number of rows (species)#
#    - the 2nd number equals the number of columns (number of areas)#
#    - after a tab, put the areas in parentheses, with spaces: (A B C D)#
##
# 1.5. Example first line:#
#    10    4    (A B C D)#
# #
# 2. The second line, and subsequent lines:#
#    speciesA    0110#
#    speciesB    0111#
#    speciesC    0001#
#         ...#
# #
# 2.5a. This means a TAB between the species name and the area 0/1s#
# 2.5b. This also means NO SPACE AND NO TAB between the area 0/1s.#
# #
# 3. See example files at:#
#    http://phylo.wikidot.com/biogeobears#files#
# #
# 4. Make you understand what a PLAIN-TEXT EDITOR is:#
#    http://phylo.wikidot.com/biogeobears#texteditors#
##
# 3. The PHYLIP format is the same format used for C++ LAGRANGE geography files.#
##
# 4. All names in the geography file must match names in the phylogeny file.#
##
# 5. DON'T USE SPACES IN SPECIES NAMES, USE E.G. "_"#
##
# 6. Operational taxonomic units (OTUs) should ideally be phylogenetic lineages, #
#    i.e. genetically isolated populations.  These may or may not be identical #
#    with species.  You would NOT want to just use specimens, as each specimen #
#    automatically can only live in 1 area, which will typically favor DEC+J #
#    models.  This is fine if the species/lineages really do live in single areas,#
#    but you wouldn't want to assume this without thinking about it at least. #
#    In summary, you should collapse multiple specimens into species/lineages if #
#    data indicates they are the same genetic population.#
#######################################################
#
# This is the example geography file for Hawaiian Psychotria#
# (from Ree & Smith 2008)#
geogfn = np(paste(addslash(extdata_dir), "Psychotria_geog.data", sep=""))#
#
# Look at the raw geography text file:#
moref(geogfn)#
#
# Look at your geographic range data:#
tipranges = getranges_from_LagrangePHYLIP(lgdata_fn=geogfn)#
tipranges#
#
# Maximum range size observed:#
max(rowSums(dfnums_to_numeric(tipranges@df)))#
#
# Set the maximum number of areas any species may occupy; this cannot be larger #
# than the number of areas you set up, but it can be smaller.#
max_range_size = 4#
#
#####################################################
#####################################################
# KEY HINT: The number of states (= number of different possible geographic ranges)#
# depends on (a) the number of areas and (b) max_range_size.#
# If you have more than about 500-600 states, the calculations will get REALLY slow,#
# since the program has to exponentiate a matrix of e.g. 600x600.  Often the computer#
# will just sit there and crunch, and never get through the calculation of the first#
# likelihood.#
# #
# (this is also what is usually happening when LAGRANGE hangs: you have too many states!)#
##
# To check the number of states for a given number of ranges, try:#
numstates_from_numareas(numareas=4, maxareas=4, include_null_range=TRUE)#
numstates_from_numareas(numareas=4, maxareas=4, include_null_range=FALSE)#
numstates_from_numareas(numareas=4, maxareas=3, include_null_range=TRUE)#
numstates_from_numareas(numareas=4, maxareas=2, include_null_range=TRUE)#
#
# Large numbers of areas have problems:#
numstates_from_numareas(numareas=10, maxareas=10, include_null_range=TRUE)#
#
# ...unless you limit the max_range_size:#
numstates_from_numareas(numareas=10, maxareas=2, include_null_range=TRUE)#
#####################################################
#####################################################
#
########################################################
########################################################
# DEC AND DEC+J ANALYSIS#
########################################################
########################################################
# NOTE: The BioGeoBEARS "DEC" model is identical with #
# the Lagrange DEC model, and should return identical#
# ML estimates of parameters, and the same #
# log-likelihoods, for the same datasets.#
##
# Ancestral state probabilities at nodes will be slightly #
# different, since BioGeoBEARS is reporting the #
# ancestral state probabilities under the global ML#
# model, and Lagrange is reporting ancestral state#
# probabilities after re-optimizing the likelihood#
# after fixing the state at each node. These will #
# be similar, but not identical. See Matzke (2014),#
# Systematic Biology, for discussion.#
##
# Also see Matzke (2014) for presentation of the #
# DEC+J model.#
########################################################
########################################################
#
########################################################
########################################################
#
########################################################
# Run DEC#
########################################################
#
# Intitialize a default model (DEC model)#
BioGeoBEARS_run_object = define_BioGeoBEARS_run()#
#
# Give BioGeoBEARS the location of the phylogeny Newick file#
BioGeoBEARS_run_object$trfn = trfn#
#
# Give BioGeoBEARS the location of the geography text file#
BioGeoBEARS_run_object$geogfn = geogfn#
#
# Input the maximum range size#
BioGeoBEARS_run_object$max_range_size = max_range_size#
#
BioGeoBEARS_run_object$min_branchlength = 0.000001    # Min to treat tip as a direct ancestor (no speciation event)#
BioGeoBEARS_run_object$include_null_range = TRUE    # set to FALSE for e.g. DEC* model, DEC*+J, etc.#
# (For DEC* and other "*" models, please cite: Massana, Kathryn A.; Beaulieu, #
#  Jeremy M.; Matzke, Nicholas J.; OMeara, Brian C. (2015). Non-null Effects of #
#  the Null Range in Biogeographic Models: Exploring Parameter Estimation in the #
#  DEC Model. bioRxiv,  http://biorxiv.org/content/early/2015/09/16/026914 )#
# Also: search script on "include_null_range" for other places to change#
#
# Set up a time-stratified analysis:#
# 1. Here, un-comment ONLY the files you want to use.#
# 2. Also un-comment "BioGeoBEARS_run_object = section_the_tree(...", below.#
# 3. For example files see (a) extdata_dir, #
#  or (b) http://phylo.wikidot.com/biogeobears#files#
#  and BioGeoBEARS Google Group posts for further hints)#
##
# Uncomment files you wish to use in time-stratified analyses:#
BioGeoBEARS_run_object$timesfn = np(paste(addslash(extdata_dir), "examples/Psychotria_M3strat/BGB/timeperiods.txt", sep=""))#
BioGeoBEARS_run_object$dispersal_multipliers_fn = np(paste(addslash(extdata_dir), "examples/Psychotria_M3strat/BGB/dispersal_multipliers.txt", sep=""))#
BioGeoBEARS_run_object$areas_allowed_fn = np(paste(addslash(extdata_dir), "examples/Psychotria_M3strat/BGB/areas_allowed.txt", sep=""))#
#BioGeoBEARS_run_object$areas_adjacency_fn = "areas_adjacency.txt"#
#BioGeoBEARS_run_object$distsfn = "distances_matrix.txt"#
# See notes on the distances model on PhyloWiki's BioGeoBEARS updates page.#
#
# Speed options and multicore processing if desired#
BioGeoBEARS_run_object$on_NaN_error = -1e50    # returns very low lnL if parameters produce NaN error (underflow check)#
BioGeoBEARS_run_object$speedup = TRUE          # shorcuts to speed ML search; use FALSE if worried (e.g. >3 params)#
BioGeoBEARS_run_object$use_optimx = "GenSA"    # if FALSE, use optim() instead of optimx()#
BioGeoBEARS_run_object$num_cores_to_use = 1#
# (use more cores to speed it up; this requires#
# library(parallel) and/or library(snow). The package "parallel" #
# is now default on Macs in R 3.0+, but apparently still #
# has to be typed on some Windows machines. Note: apparently #
# parallel works on Mac command-line R, but not R.app.#
# BioGeoBEARS checks for this and resets to 1#
# core with R.app)#
#
# Sparse matrix exponentiation is an option for huge numbers of ranges/states (600+)#
# I have experimented with sparse matrix exponentiation in EXPOKIT/rexpokit,#
# but the results are imprecise and so I haven't explored it further.#
# In a Bayesian analysis, it might work OK, but the ML point estimates are#
# not identical.#
# Also, I have not implemented all functions to work with force_sparse=TRUE.#
# Volunteers are welcome to work on it!!#
BioGeoBEARS_run_object$force_sparse = FALSE    # force_sparse=TRUE causes pathology & isn't much faster at this scale#
#
# This function loads the dispersal multiplier matrix etc. from the text files into the model object. Required for these to work!#
# (It also runs some checks on these inputs for certain errors.)#
BioGeoBEARS_run_object = readfiles_BioGeoBEARS_run(BioGeoBEARS_run_object)#
#
# Divide the tree up by timeperiods/strata (uncomment this for stratified analysis)#
BioGeoBEARS_run_object = section_the_tree(inputs=BioGeoBEARS_run_object, make_master_table=TRUE, plot_pieces=FALSE)#
# The stratified tree is described in this table:#
#BioGeoBEARS_run_object$master_table#
#
# Good default settings to get ancestral states#
BioGeoBEARS_run_object$return_condlikes_table = TRUE#
BioGeoBEARS_run_object$calc_TTL_loglike_from_condlikes_table = TRUE#
BioGeoBEARS_run_object$calc_ancprobs = TRUE    # get ancestral states from optim run#
#
# Set up DEC model#
# (nothing to do; defaults)#
#
# Look at the BioGeoBEARS_run_object; it's just a list of settings etc.#
BioGeoBEARS_run_object#
#
# This contains the model object#
BioGeoBEARS_run_object$BioGeoBEARS_model_object#
#
# This table contains the parameters of the model #
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table#
#
# Run this to check inputs. Read the error messages if you get them!#
check_BioGeoBEARS_run(BioGeoBEARS_run_object)#
#
# For a slow analysis, run once, then set runslow=FALSE to just #
# load the saved result.#
runslow = TRUE#
resfn = "Psychotria_DEC_M3_time-stratified_v1.Rdata"#
if (runslow)#
    {#
    res = bears_optim_run(BioGeoBEARS_run_object)#
    res    #
#
    save(res, file=resfn)#
    resDEC = res#
    } else {#
    # Loads to "res"#
    load(resfn)#
    resDEC = res#
    }
moref(BioGeoBEARS_run_object$timesfn)
moref(BioGeoBEARS_run_object$dispersal_multipliers_fn)
names(BioGeoBEARS_run_object)
BioGeoBEARS_run_object$tree_sections_list
length(BioGeoBEARS_run_object$tree_sections_list)
BioGeoBEARS_run_object$tree_sections_list[[1]]
BioGeoBEARS_run_object$tree_sections_list[[1]]$return_pieces_list
sum(unlist(BioGeoBEARS_run_object$tree_sections_list[[1]]$return_pieces_list))
BioGeoBEARS_run_object$tree_sections_list[[2]]$return_pieces_list
BioGeoBEARS_run_object$tree_sections_list[[2]]$return_pieces_list[[3]]
names(BioGeoBEARS_run_object$tree_sections_list[[2]]$return_pieces_list[[3]])
sum(BioGeoBEARS_run_object$tree_sections_list[[2]]$return_pieces_list[[3]]$edge.length)
sum(BioGeoBEARS_run_object$tree_sections_list[[2]]$return_pieces_list[[3]]$root.edge)
########################################################
# Example simulation and SSEsim#
########################################################
# Load the package (after installation, see above).#
#
library(optimx)         # You need to have some version of optimx available#
                        # as it is a BioGeoBEARS dependency; however, if you#
                        # don't want to use optimx, and use optim() (from R core) #
                        # you can set:#
                        # BioGeoBEARS_run_object$use_optimx = FALSE#
                        # ...everything should work either way -- NJM 2014-01-08#
library(FD)       # for FD::maxent() (make sure this is up-to-date)#
library(snow)     # (if you want to use multicore functionality; some systems/R versions prefer library(parallel), try either)#
library(parallel)#
library(BioGeoBEARS)#
library(GenSA)  # run install.packages("GenSA") if you need this#
# wd = "/drives/GDrive/__classes/BIOSCI395/lab/BGBlab/conifer_DEC+x_traits_models/"#
# setwd(wd)#
#
# Get 395 locations in GitHub install#
extdata_dir = np(system.file("extdata", package="BioGeoBEARS"))#
scriptdir = np(system.file("extdata/a_scripts", package="BioGeoBEARS"))#
extdata_dir#
list.files(extdata_dir)#
#
# Get the locations of the 395 lab files from GitHub install#
labdir = paste(extdata_dir, "examples/395lab/", sep="/")#
labpt2a = paste(extdata_dir, "examples/395lab/conifer_DEC_traits_models/", sep="/")#
labpt2b = paste(extdata_dir, "examples/395lab/conifer_DEC+x_traits_models/", sep="/")#
########################################################
# Inference#
########################################################
max_range_size = 3
# Traits-only model -- 1 rate#
########################################################
BioGeoBEARS_run_object = define_BioGeoBEARS_run()#
BioGeoBEARS_run_object$print_optim = TRUE#
BioGeoBEARS_run_object$calc_ancprobs=TRUE        # get ancestral states from optim run#
BioGeoBEARS_run_object$max_range_size = 1#
BioGeoBEARS_run_object$num_cores_to_use = 1#
BioGeoBEARS_run_object$use_optimx=TRUE#
BioGeoBEARS_run_object$speedup=TRUE#
BioGeoBEARS_run_object$geogfn = slashslash(paste(labpt2b, "geog_1area.data", sep="/"))#
BioGeoBEARS_run_object$trfn = slashslash(paste(labpt2b, "tree.newick", sep="/"))#
BioGeoBEARS_run_object = readfiles_BioGeoBEARS_run(BioGeoBEARS_run_object)#
BioGeoBEARS_run_object$return_condlikes_table = TRUE#
BioGeoBEARS_run_object$calc_TTL_loglike_from_condlikes_table = TRUE#
BioGeoBEARS_run_object$calc_ancprobs = TRUE#
BioGeoBEARS_run_object$on_NaN_error = -1000000#
BioGeoBEARS_run_object$force_sparse = FALSE  # works with kexpmv, but compare to dense,#
# time-stratify to break up long branches if you see major differences in lnL
# Look at the BioGeoBEARS_run_object; it's just a list of settings etc.#
BioGeoBEARS_run_object#
#
# This contains the model object#
BioGeoBEARS_run_object$BioGeoBEARS_model_object#
#
# This table contains the parameters of the model #
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table#
#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["a","type"] = "fixed"#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["a","init"] = 0.0#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["a","est"] = 0.0#
#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["d","type"] = "fixed"#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["d","init"] = 0.0#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["d","est"] = 0.0#
#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["e","type"] = "fixed"#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["e","init"] = 0.0#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["e","est"] = 0.0#
#
# Set up BAYAREALIKE model#
# No subset sympatry#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["s","type"] = "fixed"#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["s","init"] = 0.0#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["s","est"] = 0.0#
#
# No vicariance#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["v","type"] = "fixed"#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["v","init"] = 0.0#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["v","est"] = 0.0#
#
# No jump dispersal/founder-event speciation#
# BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["j","type"] = "free"#
# BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["j","init"] = 0.01#
# BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["j","est"] = 0.01#
#
# Adjust linkage between parameters#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["ysv","type"] = "1-j"#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["ys","type"] = "ysv*1/1"#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["y","type"] = "1-j"#
#
# Only sympatric/range-copying (y) events allowed, and with #
# exact copying (both descendants always the same size as the ancestor)#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["mx01y","type"] = "fixed"#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["mx01y","init"] = 0.9999#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["mx01y","est"] = 0.9999
tr = read.tree(BioGeoBEARS_run_object$trfn)#
#plot(tr); axisPhylo()#
#
trait_fn = slashslash(paste(labpt2b, "trait.data", sep="/"))#
geog_values = getranges_from_LagrangePHYLIP(trait_fn)#
#
trait_fn = slashslash(paste(labpt2b, "trait.data", sep="/"))#
trait_values = getranges_from_LagrangePHYLIP(lgdata_fn=trait_fn)#
trait_values
BioGeoBEARS_run_object = add_trait_to_BioGeoBEARS_run_object(BioGeoBEARS_run_object, trait_fn=trait_fn)
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["t12", "type"] = "free"#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["t12", "init"] = 0.001#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["t12", "est"] = 0.001#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["t12", "min"] = 0.00001#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["t12", "max"] = 1#
#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["t21", "type"] = "t12"#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["t21", "init"] = 0.001#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["t21", "est"] = 0.001#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["t21", "min"] = 0.00001#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["t21", "max"] = 1#
#
# No multipliers on geog (set m1 and m2 to 1)#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["m1", "type"] = "fixed"#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["m2", "type"] = "fixed"#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["m1", "init"] = 1.0#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["m2", "init"] = 1.0#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["m1", "est"] = 1.0#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["m2", "est"] = 1.0#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["m1", "desc"] = "trait-based dispersal rate multipliers m1"#
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table["m2", "desc"] = "trait-based dispersal rate multipliers m2"#
#
# Run this to check inputs. Read the error messages if you get them!#
BioGeoBEARS_run_object = fix_BioGeoBEARS_params_minmax(BioGeoBEARS_run_object)#
check_BioGeoBEARS_run(BioGeoBEARS_run_object)
# For a slow analysis, run once, then set runslow=FALSE to just #
# load the saved result.#
runslow = TRUE#
resfn = "sim_traitsOnly_1rate_v2.Rdata"#
if (runslow)#
		{#
		res = bears_optim_run(BioGeoBEARS_run_object)#
		res    #
#
		save(res, file=resfn)#
		resTrait_1rate_v2 = res#
		} else {#
		# Loads to "res"#
		load(resfn)#
		resTrait_1rate_v2 = res#
		} # END if (runslow)
ltrfn ="/GitHub/PhyBEARS.jl/sims/sunkNZ_v1/living_tree.newick"
ltr = read.tree(ltrfn)
plot(ltr)
axisPhylo()
ftrfn="/GitHub/PhyBEARS.jl/sims/sunkNZ_v1/tree_wFossils.newick "
ftrfn="/GitHub/PhyBEARS.jl/sims/sunkNZ_v1/tree_wFossils.newick"
ftr = read.tree(ftrfn)
plot(ftr); axisPhylo()
tr
ftr
tree_pieces = section_the_tree(tr, make_master_table=TRUE, plot_pieces=FALSE, cut_fossils=FALSE)
?section_the_tree
section_the_tree
inputs
write.tree(tr, file="")
BioGeoBEARS_run_object$trfn
np(paste(addslash(extdata_dir), "timeperiods.txt", sep=""))
moref(np(paste(addslash(extdata_dir), "timeperiods.txt", sep="")))
times_fn = np(paste(addslash(extdata_dir), "examples/Psychotria_M3strat/BGB/timeperiods.txt", sep=""))#
moref(times_fn)
inputs$trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
inputs$timeperiods = c(0.5, 1.9, 3.7, 5.1, 10)#
#
tree_pieces = section_the_tree(inputs, make_master_table=TRUE, plot_pieces=FALSE, cut_fossils=FALSE)
inputs = NULL#
inputs$trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
inputs$timeperiods = c(0.5, 1.9, 3.7, 5.1, 10)
tree_pieces = section_the_tree(inputs, make_master_table=TRUE, plot_pieces=FALSE, cut_fossils=FALSE)
tree_pieces
length(tree_pieces)
i=1
tree_piece = tree_pieces[[i]]
tree_piece
tree_pieces
names(tree_pieces)
tree_piece = tree_pieces$tree_sections_list[[i]]
tree_piece
j=1
class(tree_piece$return_pieces_list[[j]])
class("hey")
timeperiods = c(0.5, 1.9, 3.7, 5.1, 10)#
trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
tr = read.tree(trfn)#
sum_nodes_branchlengths_by_timeperiod(tr, timeperiods)#
sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
	{#
	# Check if "tr" is useable#
	if (("phylo" %in%" class(tr)) == TRUE)#
		{#
		trfn = "temp_tree.newick"#
		write.tree(tr, file=trfn)#
		} else if (("character" %in%" class(tr)) == TRUE) {#
		trfn = tr#
		} else {#
		txt = "STOP ERROR in sum_nodes_branchlengths_by_timeperiod(): argument 'tr' must be either a 'phylo' tree object, or a 'character' filename."#
		stop(txt)#
		}#
	inputs = NULL#
	inputs$trfn = trfn#
	#inputs$timeperiods = timeperiods#
#
	tree_pieces = section_the_tree(inputs, make_master_table=TRUE, plot_pieces=FALSE, cut_fossils=FALSE)#
#
	tree_pieces#
#
	nodes_sums = NULL#
	branchlength_sums = NULL#
#
	for (i in 1:length(tree_pieces))#
		{#
		tree_piece = tree_pieces$tree_sections_list[[i]]#
		for (j in 1:length(tree_piece$return_pieces_list))#
			{#
			branchlength_sum = 0#
			nodes_sum = 0#
			if ("numeric" %in% class(tree_piece$return_pieces_list[[j]]))#
				{#
				branchlength_sum = branchlength_sum + tree_piece$return_pieces_list[[j]]#
				} else {#
				tmptr = tree_piece$return_pieces_list[[j]]#
				nodes_sum = nodes_sum + tmptr$Nnode#
				branchlength_sum = branchlength_sum + sum(tmptr$edge.length)#
				} # END if ("numeric" %in% class#
			nodes_sums = c(nodes_sums, )#
			branchlength_sums = c(branchlength_sums, branchlength_sum)#
			} # END for (j in 1:length(tree_piece$return_pieces_list))#
		} # END for (i in 1:length(tree_pieces))#
	} # END sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)
# Check if "tr" is useable#
	if (("phylo" %in%" class(tr)) == TRUE)#
		{#
		trfn = "temp_tree.newick"#
		write.tree(tr, file=trfn)#
		} else if (("character" %in%" class(tr)) == TRUE) {#
		trfn = tr#
		} else {#
		txt = "STOP ERROR in sum_nodes_branchlengths_by_timeperiod(): argument 'tr' must be either a 'phylo' tree object, or a 'character' filename."#
		stop(txt)#
		}#
	inputs = NULL#
	inputs$trfn = trfn#
	#inputs$timeperiods = timeperiods#
#
	tree_pieces = section_the_tree(inputs, make_master_table=TRUE, plot_pieces=FALSE, cut_fossils=FALSE)
"phylo" %in%" class(tr)
tr
# Check if "tr" is useable#
	if (("phylo" %in% class(tr)) == TRUE)#
		{#
		trfn = "temp_tree.newick"#
		write.tree(tr, file=trfn)#
		} else if (("character" %in% class(tr)) == TRUE) {#
		trfn = tr#
		} else {#
		txt = "STOP ERROR in sum_nodes_branchlengths_by_timeperiod(): argument 'tr' must be either a 'phylo' tree object, or a 'character' filename."#
		stop(txt)#
		}
class(tr)
sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
	{#
	# Check if "tr" is useable#
	if (("phylo" %in% class(tr)) == TRUE)#
		{#
		trfn = "temp_tree.newick"#
		write.tree(tr, file=trfn)#
		} else if (("character" %in% class(tr)) == TRUE) {#
		trfn = tr#
		} else {#
		txt = "STOP ERROR in sum_nodes_branchlengths_by_timeperiod(): argument 'tr' must be either a 'phylo' tree object, or a 'character' filename."#
		stop(txt)#
		}#
	inputs = NULL#
	inputs$trfn = trfn#
	#inputs$timeperiods = timeperiods#
#
	tree_pieces = section_the_tree(inputs, make_master_table=TRUE, plot_pieces=FALSE, cut_fossils=FALSE)#
#
	tree_pieces#
#
	nodes_sums = NULL#
	branchlength_sums = NULL#
#
	for (i in 1:length(tree_pieces))#
		{#
		tree_piece = tree_pieces$tree_sections_list[[i]]#
		for (j in 1:length(tree_piece$return_pieces_list))#
			{#
			branchlength_sum = 0#
			nodes_sum = 0#
			if ("numeric" %in% class(tree_piece$return_pieces_list[[j]]))#
				{#
				branchlength_sum = branchlength_sum + tree_piece$return_pieces_list[[j]]#
				} else {#
				tmptr = tree_piece$return_pieces_list[[j]]#
				nodes_sum = nodes_sum + tmptr$Nnode#
				branchlength_sum = branchlength_sum + sum(tmptr$edge.length)#
				} # END if ("numeric" %in% class#
			nodes_sums = c(nodes_sums, )#
			branchlength_sums = c(branchlength_sums, branchlength_sum)#
			} # END for (j in 1:length(tree_piece$return_pieces_list))#
		} # END for (i in 1:length(tree_pieces))#
	} # END sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)
names(tree_pieces)
tree_pieces$timeperiods
sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
	{#
	# Check if "tr" is useable#
	if (("phylo" %in% class(tr)) == TRUE)#
		{#
		trfn = "temp_tree.newick"#
		write.tree(tr, file=trfn)#
		} else if (("character" %in% class(tr)) == TRUE) {#
		trfn = tr#
		} else {#
		txt = "STOP ERROR in sum_nodes_branchlengths_by_timeperiod(): argument 'tr' must be either a 'phylo' tree object, or a 'character' filename."#
		stop(txt)#
		}#
	inputs = NULL#
	inputs$trfn = trfn#
	#inputs$timeperiods = timeperiods#
#
	tree_pieces = section_the_tree(inputs, make_master_table=TRUE, plot_pieces=FALSE, cut_fossils=FALSE)#
#
	tree_pieces#
#
	node_counts = NULL#
	branchlength_sums = NULL#
#
	for (i in 1:length(tree_pieces))#
		{#
		tree_piece = tree_pieces$tree_sections_list[[i]]#
		for (j in 1:length(tree_piece$return_pieces_list))#
			{#
			branchlength_sum = 0#
			nodes_sum = 0#
			if ("numeric" %in% class(tree_piece$return_pieces_list[[j]]))#
				{#
				branchlength_sum = branchlength_sum + tree_piece$return_pieces_list[[j]]#
				} else {#
				tmptr = tree_piece$return_pieces_list[[j]]#
				nodes_sum = nodes_sum + tmptr$Nnode#
				branchlength_sum = branchlength_sum + sum(tmptr$edge.length)#
				} # END if ("numeric" %in% class#
			node_counts = c(node_counts, )#
			branchlength_sums = c(branchlength_sums, branchlength_sum)#
			} # END for (j in 1:length(tree_piece$return_pieces_list))#
		} # END for (i in 1:length(tree_pieces))#
	time_tops = c(0.0, timeperiods[1:(length(timeperiods-1))])#
	time_bots = timeperiods#
	tmpmat = cbind(time_tops, time_bots, node_counts, branchlength_sums)#
	counts_df = as.data.frame(tmpmat, stringsAsFactors=FALSE)#
	names(counts_df) = c("time_tops", "time_bots", "node_counts", "branchlength_sums")#
	counts_df#
	} # END sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)
timeperiods = c(0.5, 1.9, 3.7, 5.1, 10)#
trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
tr = read.tree(trfn)#
counts_df = sum_nodes_branchlengths_by_timeperiod(tr, timeperiods)#
counts_df
timeperiods
class(timeperiods)
section_the_tree
sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
	{#
	# Check if "tr" is useable#
	if (("phylo" %in% class(tr)) == TRUE)#
		{#
		trfn = "temp_tree.newick"#
		write.tree(tr, file=trfn)#
		} else if (("character" %in% class(tr)) == TRUE) {#
		trfn = tr#
		} else {#
		txt = "STOP ERROR in sum_nodes_branchlengths_by_timeperiod(): argument 'tr' must be either a 'phylo' tree object, or a 'character' filename."#
		stop(txt)#
		}#
	inputs = NULL#
	inputs$trfn = trfn#
	inputs$timeperiods = timeperiods#
#
	tree_pieces = section_the_tree(inputs, make_master_table=TRUE, plot_pieces=FALSE, cut_fossils=FALSE)#
#
	tree_pieces#
#
	node_counts = NULL#
	branchlength_sums = NULL#
#
	for (i in 1:length(tree_pieces))#
		{#
		tree_piece = tree_pieces$tree_sections_list[[i]]#
		for (j in 1:length(tree_piece$return_pieces_list))#
			{#
			branchlength_sum = 0#
			nodes_sum = 0#
			if ("numeric" %in% class(tree_piece$return_pieces_list[[j]]))#
				{#
				branchlength_sum = branchlength_sum + tree_piece$return_pieces_list[[j]]#
				} else {#
				tmptr = tree_piece$return_pieces_list[[j]]#
				nodes_sum = nodes_sum + tmptr$Nnode#
				branchlength_sum = branchlength_sum + sum(tmptr$edge.length)#
				} # END if ("numeric" %in% class#
			node_counts = c(node_counts, )#
			branchlength_sums = c(branchlength_sums, branchlength_sum)#
			} # END for (j in 1:length(tree_piece$return_pieces_list))#
		} # END for (i in 1:length(tree_pieces))#
	time_tops = c(0.0, timeperiods[1:(length(timeperiods-1))])#
	time_bots = timeperiods#
	tmpmat = cbind(time_tops, time_bots, node_counts, branchlength_sums)#
	counts_df = as.data.frame(tmpmat, stringsAsFactors=FALSE)#
	names(counts_df) = c("time_tops", "time_bots", "node_counts", "branchlength_sums")#
	counts_df#
	} # END sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)
timeperiods = c(0.5, 1.9, 3.7, 5.1, 10)#
trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
tr = read.tree(trfn)#
counts_df = sum_nodes_branchlengths_by_timeperiod(tr, timeperiods)#
counts_df
sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
	{#
	# Check if "tr" is useable#
	if (("phylo" %in% class(tr)) == TRUE)#
		{#
		trfn = "temp_tree.newick"#
		write.tree(tr, file=trfn)#
		} else if (("character" %in% class(tr)) == TRUE) {#
		trfn = tr#
		} else {#
		txt = "STOP ERROR in sum_nodes_branchlengths_by_timeperiod(): argument 'tr' must be either a 'phylo' tree object, or a 'character' filename."#
		stop(txt)#
		}#
	inputs = NULL#
	inputs$trfn = trfn#
	inputs$timeperiods = timeperiods#
#
	tree_pieces = section_the_tree(inputs, make_master_table=TRUE, plot_pieces=FALSE, cut_fossils=FALSE)#
#
	tree_pieces#
#
	node_counts = NULL#
	branchlength_sums = NULL#
#
	for (i in 1:length(tree_pieces))#
		{#
		tree_piece = tree_pieces$tree_sections_list[[i]]#
		for (j in 1:length(tree_piece$return_pieces_list))#
			{#
			branchlength_sum = 0#
			nodes_sum = 0#
			if ("numeric" %in% class(tree_piece$return_pieces_list[[j]]))#
				{#
				branchlength_sum = branchlength_sum + tree_piece$return_pieces_list[[j]]#
				} else {#
				tmptr = tree_piece$return_pieces_list[[j]]#
				nodes_sum = nodes_sum + tmptr$Nnode#
				branchlength_sum = branchlength_sum + sum(tmptr$edge.length)#
				} # END if ("numeric" %in% class#
			node_counts = c(node_counts, nodes_sum)#
			branchlength_sums = c(branchlength_sums, branchlength_sum)#
			} # END for (j in 1:length(tree_piece$return_pieces_list))#
		} # END for (i in 1:length(tree_pieces))#
	time_tops = c(0.0, timeperiods[1:(length(timeperiods-1))])#
	time_bots = timeperiods#
	tmpmat = cbind(time_tops, time_bots, node_counts, branchlength_sums)#
	counts_df = as.data.frame(tmpmat, stringsAsFactors=FALSE)#
	names(counts_df) = c("time_tops", "time_bots", "node_counts", "branchlength_sums")#
	counts_df#
	} # END sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)
timeperiods = c(0.5, 1.9, 3.7, 5.1, 10)#
trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
tr = read.tree(trfn)#
counts_df = sum_nodes_branchlengths_by_timeperiod(tr, timeperiods)#
counts_df
sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
	{#
	# Check if "tr" is useable#
	if (("phylo" %in% class(tr)) == TRUE)#
		{#
		trfn = "temp_tree.newick"#
		write.tree(tr, file=trfn)#
		} else if (("character" %in% class(tr)) == TRUE) {#
		trfn = tr#
		} else {#
		txt = "STOP ERROR in sum_nodes_branchlengths_by_timeperiod(): argument 'tr' must be either a 'phylo' tree object, or a 'character' filename."#
		stop(txt)#
		}#
	inputs = NULL#
	inputs$trfn = trfn#
	inputs$timeperiods = timeperiods#
#
	tree_pieces = section_the_tree(inputs, make_master_table=TRUE, plot_pieces=FALSE, cut_fossils=FALSE)#
#
	tree_pieces#
#
	node_counts = NULL#
	branchlength_sums = NULL#
#
	for (i in 1:length(tree_pieces))#
		{#
		tree_piece = tree_pieces$tree_sections_list[[i]]#
		for (j in 1:length(tree_piece$return_pieces_list))#
			{#
			branchlength_sum = 0#
			nodes_sum = 0#
			if ("numeric" %in% class(tree_piece$return_pieces_list[[j]]))#
				{#
				branchlength_sum = branchlength_sum + tree_piece$return_pieces_list[[j]]#
				} else {#
				tmptr = tree_piece$return_pieces_list[[j]]#
				nodes_sum = nodes_sum + tmptr$Nnode#
				branchlength_sum = branchlength_sum + sum(tmptr$edge.length)#
				} # END if ("numeric" %in% class#
			} # END for (j in 1:length(tree_piece$return_pieces_list))#
			node_counts = c(node_counts, nodes_sum)#
			branchlength_sums = c(branchlength_sums, branchlength_sum)	#
		} # END for (i in 1:length(tree_pieces))#
	time_tops = c(0.0, timeperiods[1:(length(timeperiods-1))])#
	time_bots = timeperiods#
	tmpmat = cbind(time_tops, time_bots, node_counts, branchlength_sums)#
	counts_df = as.data.frame(tmpmat, stringsAsFactors=FALSE)#
	names(counts_df) = c("time_tops", "time_bots", "node_counts", "branchlength_sums")#
	counts_df#
	} # END sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
#
timeperiods = c(0.5, 1.9, 3.7, 5.1, 10)#
trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
tr = read.tree(trfn)#
counts_df = sum_nodes_branchlengths_by_timeperiod(tr, timeperiods)#
counts_df
sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
	{#
	# Check if "tr" is useable#
	if (("phylo" %in% class(tr)) == TRUE)#
		{#
		trfn = "temp_tree.newick"#
		write.tree(tr, file=trfn)#
		} else if (("character" %in% class(tr)) == TRUE) {#
		trfn = tr#
		} else {#
		txt = "STOP ERROR in sum_nodes_branchlengths_by_timeperiod(): argument 'tr' must be either a 'phylo' tree object, or a 'character' filename."#
		stop(txt)#
		}#
	inputs = NULL#
	inputs$trfn = trfn#
	inputs$timeperiods = timeperiods#
#
	tree_pieces = section_the_tree(inputs, make_master_table=TRUE, plot_pieces=FALSE, cut_fossils=FALSE)#
#
	tree_pieces#
#
	node_counts = NULL#
	branchlength_sums = NULL#
#
	for (i in 1:length(tree_pieces))#
		{#
		tree_piece = tree_pieces$tree_sections_list[[i]]#
		for (j in 1:length(tree_piece$return_pieces_list))#
			{#
			branchlength_sum = 0#
			nodes_sum = 0#
			if ("numeric" %in% class(tree_piece$return_pieces_list[[j]]))#
				{#
				branchlength_sum = branchlength_sum + tree_piece$return_pieces_list[[j]]#
				} else {#
				tmptr = tree_piece$return_pieces_list[[j]]#
				nodes_sum = nodes_sum + tmptr$Nnode#
				branchlength_sum = branchlength_sum + sum(tmptr$edge.length)#
				} # END if ("numeric" %in% class#
			} # END for (j in 1:length(tree_piece$return_pieces_list))#
			node_counts = c(node_counts, nodes_sum)#
			branchlength_sums = c(branchlength_sums, branchlength_sum)	#
		} # END for (i in 1:length(tree_pieces))#
	time_tops = c(0.0, timeperiods[1:(length(timeperiods)-1)])#
	time_bots = timeperiods#
	tmpmat = cbind(time_tops, time_bots, node_counts, branchlength_sums)#
	counts_df = as.data.frame(tmpmat, stringsAsFactors=FALSE)#
	names(counts_df) = c("time_tops", "time_bots", "node_counts", "branchlength_sums")#
	counts_df#
	} # END sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
#
timeperiods = c(0.5, 1.9, 3.7, 5.1, 10)#
trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
tr = read.tree(trfn)#
counts_df = sum_nodes_branchlengths_by_timeperiod(tr, timeperiods)#
counts_df
tree_piece = tree_pieces$tree_sections_list[[i]]
tree_piece
sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
	{#
	# Check if "tr" is useable#
	if (("phylo" %in% class(tr)) == TRUE)#
		{#
		trfn = "temp_tree.newick"#
		write.tree(tr, file=trfn)#
		} else if (("character" %in% class(tr)) == TRUE) {#
		trfn = tr#
		} else {#
		txt = "STOP ERROR in sum_nodes_branchlengths_by_timeperiod(): argument 'tr' must be either a 'phylo' tree object, or a 'character' filename."#
		stop(txt)#
		}#
	inputs = NULL#
	inputs$trfn = trfn#
	inputs$timeperiods = timeperiods#
#
	tree_pieces = section_the_tree(inputs, make_master_table=TRUE, plot_pieces=FALSE, cut_fossils=FALSE)#
#
	tree_pieces#
#
	node_counts = NULL#
	branchlength_sums = NULL#
#
	for (i in 1:length(tree_pieces))#
		{#
		tree_piece = tree_pieces$tree_sections_list[[i]]#
		branchlength_sum = 0#
		nodes_sum = 0#
		for (j in 1:length(tree_piece$return_pieces_list))#
			{#
			if ("numeric" %in% class(tree_piece$return_pieces_list[[j]]))#
				{#
				branchlength_sum = branchlength_sum + tree_piece$return_pieces_list[[j]]#
				} else {#
				tmptr = tree_piece$return_pieces_list[[j]]#
				nodes_sum = nodes_sum + tmptr$Nnode#
				branchlength_sum = branchlength_sum + sum(tmptr$edge.length)#
				} # END if ("numeric" %in% class#
			} # END for (j in 1:length(tree_piece$return_pieces_list))#
		node_counts = c(node_counts, nodes_sum)#
		branchlength_sums = c(branchlength_sums, branchlength_sum)	#
		} # END for (i in 1:length(tree_pieces))#
	time_tops = c(0.0, timeperiods[1:(length(timeperiods)-1)])#
	time_bots = timeperiods#
	tmpmat = cbind(time_tops, time_bots, node_counts, branchlength_sums)#
	counts_df = as.data.frame(tmpmat, stringsAsFactors=FALSE)#
	names(counts_df) = c("time_tops", "time_bots", "node_counts", "branchlength_sums")#
	counts_df#
	} # END sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
#
timeperiods = c(0.5, 1.9, 3.7, 5.1, 10)#
trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
tr = read.tree(trfn)#
counts_df = sum_nodes_branchlengths_by_timeperiod(tr, timeperiods)#
counts_df
tr
sum(tr$edge.length)
sum(counts_df$branchlength_sums)
tr$Nnode
sum(counts_df$node_counts)
length(tree_pieces)
timeperiods
names(tree_pieces)
tree_pieces$tree_sections_list
names(tree_pieces)
names(tree_pieces$tree_sections_list)
length(tree_pieces$tree_sections_list)
timeperiods = c(0.5, 1.9, 3.7, 5.1, 10)#
trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
tr = read.tree(trfn)#
counts_df = sum_nodes_branchlengths_by_timeperiod(tr, timeperiods)#
counts_df#
sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
	{#
	# Check if "tr" is useable#
	if (("phylo" %in% class(tr)) == TRUE)#
		{#
		trfn = "temp_tree.newick"#
		write.tree(tr, file=trfn)#
		} else if (("character" %in% class(tr)) == TRUE) {#
		trfn = tr#
		} else {#
		txt = "STOP ERROR in sum_nodes_branchlengths_by_timeperiod(): argument 'tr' must be either a 'phylo' tree object, or a 'character' filename."#
		stop(txt)#
		}#
	inputs = NULL#
	inputs$trfn = trfn#
	inputs$timeperiods = timeperiods#
#
	tree_pieces = section_the_tree(inputs, make_master_table=TRUE, plot_pieces=FALSE, cut_fossils=FALSE)#
#
	tree_pieces#
#
	node_counts = NULL#
	branchlength_sums = NULL#
#
	for (i in 1:length(tree_pieces$tree_sections_list))#
		{#
		tree_piece = tree_pieces$tree_sections_list[[i]]#
		branchlength_sum = 0#
		nodes_sum = 0#
		for (j in 1:length(tree_piece$return_pieces_list))#
			{#
			if ("numeric" %in% class(tree_piece$return_pieces_list[[j]]))#
				{#
				branchlength_sum = branchlength_sum + tree_piece$return_pieces_list[[j]]#
				} else {#
				tmptr = tree_piece$return_pieces_list[[j]]#
				nodes_sum = nodes_sum + tmptr$Nnode#
				branchlength_sum = branchlength_sum + sum(tmptr$edge.length)#
				} # END if ("numeric" %in% class#
			} # END for (j in 1:length(tree_piece$return_pieces_list))#
		node_counts = c(node_counts, nodes_sum)#
		branchlength_sums = c(branchlength_sums, branchlength_sum)	#
		} # END for (i in 1:length(tree_pieces))#
	time_tops = c(0.0, timeperiods[1:(length(timeperiods)-1)])#
	time_bots = timeperiods#
	tmpmat = cbind(time_tops, time_bots, node_counts, branchlength_sums)#
	counts_df = as.data.frame(tmpmat, stringsAsFactors=FALSE)#
	names(counts_df) = c("time_tops", "time_bots", "node_counts", "branchlength_sums")#
	counts_df#
	} # END sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
#
timeperiods = c(0.5, 1.9, 3.7, 5.1, 10)#
trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
tr = read.tree(trfn)#
counts_df = sum_nodes_branchlengths_by_timeperiod(tr, timeperiods)#
counts_df
tr#
#
times_fn = np(paste(addslash(extdata_dir), "examples/Psychotria_M3strat/BGB/timeperiods.txt", sep=""))#
moref(times_fn)#
#
# #
timeperiods = c(0.5, 1.9, 3.7, 5.1, 10)#
trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
tr = read.tree(trfn)#
counts_df = sum_nodes_branchlengths_by_timeperiod(tr, timeperiods)#
counts_df#
sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
	{#
	# Check if "tr" is useable#
	if (("phylo" %in% class(tr)) == TRUE)#
		{#
		trfn = "temp_tree.newick"#
		write.tree(tr, file=trfn)#
		} else if (("character" %in% class(tr)) == TRUE) {#
		trfn = tr#
		} else {#
		txt = "STOP ERROR in sum_nodes_branchlengths_by_timeperiod(): argument 'tr' must be either a 'phylo' tree object, or a 'character' filename."#
		stop(txt)#
		}#
	inputs = NULL#
	inputs$trfn = trfn#
	inputs$timeperiods = timeperiods#
#
	tree_pieces = section_the_tree(inputs, make_master_table=TRUE, plot_pieces=FALSE, cut_fossils=FALSE)#
#
	tree_pieces#
#
	node_counts = NULL#
	branchlength_sums = NULL#
#
	for (i in 1:length(tree_pieces$tree_sections_list))#
		{#
		tree_piece = tree_pieces$tree_sections_list[[i]]#
		branchlength_sum = 0#
		nodes_sum = 0#
		for (j in 1:length(tree_piece$return_pieces_list))#
			{#
			if ("numeric" %in% class(tree_piece$return_pieces_list[[j]]))#
				{#
				branchlength_sum = branchlength_sum + tree_piece$return_pieces_list[[j]]#
				} else {#
				tmptr = tree_piece$return_pieces_list[[j]]#
				nodes_sum = nodes_sum + tmptr$Nnode#
				branchlength_sum = branchlength_sum + sum(tmptr$edge.length)#
				} # END if ("numeric" %in% class#
			} # END for (j in 1:length(tree_piece$return_pieces_list))#
		node_counts = c(node_counts, nodes_sum)#
		branchlength_sums = c(branchlength_sums, branchlength_sum)	#
		} # END for (i in 1:length(tree_pieces))#
	time_tops = c(0.0, timeperiods[1:(length(timeperiods)-1)])#
	time_bots = timeperiods#
	tmpmat = cbind(time_tops, time_bots, node_counts, branchlength_sums)#
	counts_df = as.data.frame(tmpmat, stringsAsFactors=FALSE)#
	names(counts_df) = c("time_tops", "time_bots", "node_counts", "branchlength_sums")#
	counts_df#
	} # END sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
#
timeperiods = c(0.5, 1.9, 3.7, 5.1, 10)#
trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
tr = read.tree(trfn)#
counts_df = sum_nodes_branchlengths_by_timeperiod(tr, timeperiods)#
counts_df#
#
sum(tr$edge.length)#
sum(counts_df$branchlength_sums)#
tr$Nnode#
sum(counts_df$node_counts)
library(BioGeoBEARS)#
#
tr#
#
times_fn = np(paste(addslash(extdata_dir), "examples/Psychotria_M3strat/BGB/timeperiods.txt", sep=""))#
moref(times_fn)#
#
# #
timeperiods = c(0.5, 1.9, 3.7, 5.1, 10)#
trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
tr = read.tree(trfn)#
counts_df = sum_nodes_branchlengths_by_timeperiod(tr, timeperiods)#
counts_df#
sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
	{#
	# Check if "tr" is useable#
	if (("phylo" %in% class(tr)) == TRUE)#
		{#
		trfn = "temp_tree.newick"#
		write.tree(tr, file=trfn)#
		} else if (("character" %in% class(tr)) == TRUE) {#
		trfn = tr#
		} else {#
		txt = "STOP ERROR in sum_nodes_branchlengths_by_timeperiod(): argument 'tr' must be either a 'phylo' tree object, or a 'character' filename."#
		stop(txt)#
		}#
	inputs = NULL#
	inputs$trfn = trfn#
	inputs$timeperiods = timeperiods#
#
	tree_pieces = section_the_tree(inputs, make_master_table=TRUE, plot_pieces=FALSE, cut_fossils=FALSE)#
#
	tree_pieces#
#
	node_counts = NULL#
	branchlength_sums = NULL#
#
	for (i in 1:length(tree_pieces$tree_sections_list))#
		{#
		tree_piece = tree_pieces$tree_sections_list[[i]]#
		branchlength_sum = 0#
		nodes_sum = 0#
		for (j in 1:length(tree_piece$return_pieces_list))#
			{#
			if ("numeric" %in% class(tree_piece$return_pieces_list[[j]]))#
				{#
				branchlength_sum = branchlength_sum + tree_piece$return_pieces_list[[j]]#
				} else {#
				tmptr = tree_piece$return_pieces_list[[j]]#
				nodes_sum = nodes_sum + tmptr$Nnode#
				branchlength_sum = branchlength_sum + sum(tmptr$edge.length)#
				} # END if ("numeric" %in% class#
			} # END for (j in 1:length(tree_piece$return_pieces_list))#
		node_counts = c(node_counts, nodes_sum)#
		branchlength_sums = c(branchlength_sums, branchlength_sum)	#
		} # END for (i in 1:length(tree_pieces))#
	time_tops = c(0.0, timeperiods[1:(length(timeperiods)-1)])#
	time_bots = timeperiods#
	tmpmat = cbind(time_tops, time_bots, node_counts, branchlength_sums)#
	counts_df = as.data.frame(tmpmat, stringsAsFactors=FALSE)#
	names(counts_df) = c("time_tops", "time_bots", "node_counts", "branchlength_sums")#
	counts_df#
	} # END sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
#
timeperiods = c(0.5, 1.9, 3.7, 5.1, 10)#
trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
tr = read.tree(trfn)#
counts_df = sum_nodes_branchlengths_by_timeperiod(tr, timeperiods)#
counts_df#
#
sum(tr$edge.length)#
sum(counts_df$branchlength_sums)#
tr$Nnode#
sum(counts_df$node_counts)
extdata_dir = np(system.file("extdata", package="BioGeoBEARS"))#
#
times_fn = np(paste(addslash(extdata_dir), "examples/Psychotria_M3strat/BGB/timeperiods.txt", sep=""))
read_times_fn
read_times_fn(inputs$times_fn)
library(BioGeoBEARS)#
#
extdata_dir = np(system.file("extdata", package="BioGeoBEARS"))#
#
# Load timeperiods from file#
times_fn = np(paste(addslash(extdata_dir), "examples/Psychotria_M3strat/BGB/timeperiods.txt", sep=""))#
inputs = NULL#
inputs$times_fn = times_fn#
timeperiods = read_times_fn(inputs$times_fn)#
#
# Or set the timeperiods manually#
timeperiods = c(0.5, 1.9, 3.7, 5.1, 10)#
#
# Load tree from file#
trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
tr = read.tree(trfn)#
counts_df = sum_nodes_branchlengths_by_timeperiod(tr, timeperiods)#
counts_df#
sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
	{#
	# Check if "tr" is useable#
	if (("phylo" %in% class(tr)) == TRUE)#
		{#
		trfn = "temp_tree.newick"#
		write.tree(tr, file=trfn)#
		} else if (("character" %in% class(tr)) == TRUE) {#
		trfn = tr#
		} else {#
		txt = "STOP ERROR in sum_nodes_branchlengths_by_timeperiod(): argument 'tr' must be either a 'phylo' tree object, or a 'character' filename."#
		stop(txt)#
		}#
	inputs = NULL#
	inputs$trfn = trfn#
	inputs$timeperiods = timeperiods#
#
	tree_pieces = section_the_tree(inputs, make_master_table=TRUE, plot_pieces=FALSE, cut_fossils=FALSE)#
#
	tree_pieces#
#
	node_counts = NULL#
	branchlength_sums = NULL#
#
	for (i in 1:length(tree_pieces$tree_sections_list))#
		{#
		tree_piece = tree_pieces$tree_sections_list[[i]]#
		branchlength_sum = 0#
		nodes_sum = 0#
		for (j in 1:length(tree_piece$return_pieces_list))#
			{#
			if ("numeric" %in% class(tree_piece$return_pieces_list[[j]]))#
				{#
				branchlength_sum = branchlength_sum + tree_piece$return_pieces_list[[j]]#
				} else {#
				tmptr = tree_piece$return_pieces_list[[j]]#
				nodes_sum = nodes_sum + tmptr$Nnode#
				branchlength_sum = branchlength_sum + sum(tmptr$edge.length)#
				} # END if ("numeric" %in% class#
			} # END for (j in 1:length(tree_piece$return_pieces_list))#
		node_counts = c(node_counts, nodes_sum)#
		branchlength_sums = c(branchlength_sums, branchlength_sum)	#
		} # END for (i in 1:length(tree_pieces))#
	time_tops = c(0.0, timeperiods[1:(length(timeperiods)-1)])#
	time_bots = timeperiods#
	tmpmat = cbind(time_tops, time_bots, node_counts, branchlength_sums)#
	counts_df = as.data.frame(tmpmat, stringsAsFactors=FALSE)#
	names(counts_df) = c("time_tops", "time_bots", "node_counts", "branchlength_sums")#
	counts_df#
	} # END sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
#
timeperiods = c(0.5, 1.9, 3.7, 5.1, 10)#
trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
tr = read.tree(trfn)#
counts_df = sum_nodes_branchlengths_by_timeperiod(tr, timeperiods)#
counts_df#
#
sum(tr$edge.length)#
sum(counts_df$branchlength_sums)#
tr$Nnode#
sum(counts_df$node_counts)
library(ape)#
library(BioGeoBEARS)#
#
extdata_dir = np(system.file("extdata", package="BioGeoBEARS"))#
#
# Load timeperiods from file#
times_fn = np(paste(addslash(extdata_dir), "examples/Psychotria_M3strat/BGB/timeperiods.txt", sep=""))#
inputs = NULL#
inputs$times_fn = times_fn#
timeperiods = read_times_fn(inputs$times_fn)#
#
# Or set the timeperiods manually#
timeperiods = c(0.5, 1.9, 3.7, 5.1, 10)#
#
# Load tree from file#
trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
tr = read.tree(trfn)#
counts_df = sum_nodes_branchlengths_by_timeperiod(tr, timeperiods)#
counts_df#
sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
	{#
	# Check if "tr" is useable#
	if (("phylo" %in% class(tr)) == TRUE)#
		{#
		trfn = "temp_tree.newick"#
		write.tree(tr, file=trfn)#
		} else if (("character" %in% class(tr)) == TRUE) {#
		trfn = tr#
		} else {#
		txt = "STOP ERROR in sum_nodes_branchlengths_by_timeperiod(): argument 'tr' must be either a 'phylo' tree object, or a 'character' filename."#
		stop(txt)#
		}#
	inputs = NULL#
	inputs$trfn = trfn#
	inputs$timeperiods = timeperiods#
#
	tree_pieces = section_the_tree(inputs, make_master_table=TRUE, plot_pieces=FALSE, cut_fossils=FALSE)#
#
	tree_pieces#
#
	node_counts = NULL#
	branchlength_sums = NULL#
#
	for (i in 1:length(tree_pieces$tree_sections_list))#
		{#
		tree_piece = tree_pieces$tree_sections_list[[i]]#
		branchlength_sum = 0#
		nodes_sum = 0#
		for (j in 1:length(tree_piece$return_pieces_list))#
			{#
			if ("numeric" %in% class(tree_piece$return_pieces_list[[j]]))#
				{#
				branchlength_sum = branchlength_sum + tree_piece$return_pieces_list[[j]]#
				} else {#
				tmptr = tree_piece$return_pieces_list[[j]]#
				nodes_sum = nodes_sum + tmptr$Nnode#
				branchlength_sum = branchlength_sum + sum(tmptr$edge.length)#
				} # END if ("numeric" %in% class#
			} # END for (j in 1:length(tree_piece$return_pieces_list))#
		node_counts = c(node_counts, nodes_sum)#
		branchlength_sums = c(branchlength_sums, branchlength_sum)	#
		} # END for (i in 1:length(tree_pieces))#
	time_tops = c(0.0, timeperiods[1:(length(timeperiods)-1)])#
	time_bots = timeperiods#
	tmpmat = cbind(time_tops, time_bots, node_counts, branchlength_sums)#
	counts_df = as.data.frame(tmpmat, stringsAsFactors=FALSE)#
	names(counts_df) = c("time_tops", "time_bots", "node_counts", "branchlength_sums")#
	counts_df#
	} # END sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
#
timeperiods = c(0.5, 1.9, 3.7, 5.1, 10)#
trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
tr = read.tree(trfn)#
counts_df = sum_nodes_branchlengths_by_timeperiod(tr, timeperiods)#
counts_df#
#
sum(tr$edge.length)#
sum(counts_df$branchlength_sums)#
tr$Nnode#
sum(counts_df$node_counts)
sum(tr$edge.length)-sum(counts_df$branchlength_sums)
library(ape)#
library(BioGeoBEARS)#
#
extdata_dir = np(system.file("extdata", package="BioGeoBEARS"))#
#
# Load timeperiods from file#
times_fn = np(paste(addslash(extdata_dir), "examples/Psychotria_M3strat/BGB/timeperiods.txt", sep=""))#
inputs = NULL#
inputs$times_fn = times_fn#
timeperiods = read_times_fn(inputs$times_fn)#
#
# Or set the timeperiods manually#
timeperiods = c(0.5, 1.9, 3.7, 5.1, 10)#
#
# Load tree from file#
trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
tr = read.tree(trfn)#
counts_df = sum_nodes_branchlengths_by_timeperiod(tr, timeperiods)#
counts_df#
sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
	{#
	# Check if "tr" is useable#
	if (("phylo" %in% class(tr)) == TRUE)#
		{#
		trfn = "temp_tree.newick"#
		write.tree(tr, file=trfn)#
		} else if (("character" %in% class(tr)) == TRUE) {#
		trfn = tr#
		} else {#
		txt = "STOP ERROR in sum_nodes_branchlengths_by_timeperiod(): argument 'tr' must be either a 'phylo' tree object, or a 'character' filename."#
		stop(txt)#
		}#
	inputs = NULL#
	inputs$trfn = trfn#
	inputs$timeperiods = timeperiods#
#
	tree_pieces = section_the_tree(inputs, make_master_table=TRUE, plot_pieces=FALSE, cut_fossils=FALSE)#
#
	tree_pieces#
#
	node_counts = NULL#
	branchlength_sums = NULL#
#
	for (i in 1:length(tree_pieces$tree_sections_list))#
		{#
		tree_piece = tree_pieces$tree_sections_list[[i]]#
		branchlength_sum = 0#
		nodes_sum = 0#
		for (j in 1:length(tree_piece$return_pieces_list))#
			{#
			if ("numeric" %in% class(tree_piece$return_pieces_list[[j]]))#
				{#
				branchlength_sum = branchlength_sum + tree_piece$return_pieces_list[[j]]#
				} else {#
				tmptr = tree_piece$return_pieces_list[[j]]#
				nodes_sum = nodes_sum + tmptr$Nnode#
				branchlength_sum = branchlength_sum + sum(tmptr$edge.length) = tmptr$root.edge#
				} # END if ("numeric" %in% class#
			} # END for (j in 1:length(tree_piece$return_pieces_list))#
		node_counts = c(node_counts, nodes_sum)#
		branchlength_sums = c(branchlength_sums, branchlength_sum)	#
		} # END for (i in 1:length(tree_pieces))#
	time_tops = c(0.0, timeperiods[1:(length(timeperiods)-1)])#
	time_bots = timeperiods#
	tmpmat = cbind(time_tops, time_bots, node_counts, branchlength_sums)#
	counts_df = as.data.frame(tmpmat, stringsAsFactors=FALSE)#
	names(counts_df) = c("time_tops", "time_bots", "node_counts", "branchlength_sums")#
	counts_df#
	} # END sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
#
timeperiods = c(0.5, 1.9, 3.7, 5.1, 10)#
trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
tr = read.tree(trfn)#
counts_df = sum_nodes_branchlengths_by_timeperiod(tr, timeperiods)#
counts_df#
#
sum(tr$edge.length)#
sum(counts_df$branchlength_sums)#
tr$Nnode#
sum(counts_df$node_counts)
library(ape)#
library(BioGeoBEARS)#
#
extdata_dir = np(system.file("extdata", package="BioGeoBEARS"))#
#
# Load timeperiods from file#
times_fn = np(paste(addslash(extdata_dir), "examples/Psychotria_M3strat/BGB/timeperiods.txt", sep=""))#
inputs = NULL#
inputs$times_fn = times_fn#
timeperiods = read_times_fn(inputs$times_fn)#
#
# Or set the timeperiods manually#
timeperiods = c(0.5, 1.9, 3.7, 5.1, 10)#
#
# Load tree from file#
trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
tr = read.tree(trfn)#
counts_df = sum_nodes_branchlengths_by_timeperiod(tr, timeperiods)#
counts_df#
sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
	{#
	# Check if "tr" is useable#
	if (("phylo" %in% class(tr)) == TRUE)#
		{#
		trfn = "temp_tree.newick"#
		write.tree(tr, file=trfn)#
		} else if (("character" %in% class(tr)) == TRUE) {#
		trfn = tr#
		} else {#
		txt = "STOP ERROR in sum_nodes_branchlengths_by_timeperiod(): argument 'tr' must be either a 'phylo' tree object, or a 'character' filename."#
		stop(txt)#
		}#
	inputs = NULL#
	inputs$trfn = trfn#
	inputs$timeperiods = timeperiods#
#
	tree_pieces = section_the_tree(inputs, make_master_table=TRUE, plot_pieces=FALSE, cut_fossils=FALSE)#
#
	tree_pieces#
#
	node_counts = NULL#
	branchlength_sums = NULL#
#
	for (i in 1:length(tree_pieces$tree_sections_list))#
		{#
		tree_piece = tree_pieces$tree_sections_list[[i]]#
		branchlength_sum = 0#
		nodes_sum = 0#
		for (j in 1:length(tree_piece$return_pieces_list))#
			{#
			if ("numeric" %in% class(tree_piece$return_pieces_list[[j]]))#
				{#
				branchlength_sum = branchlength_sum + tree_piece$return_pieces_list[[j]]#
				} else {#
				tmptr = tree_piece$return_pieces_list[[j]]#
				nodes_sum = nodes_sum + tmptr$Nnode#
				branchlength_sum = branchlength_sum + sum(tmptr$edge.length) + tmptr$root.edge#
				} # END if ("numeric" %in% class#
			} # END for (j in 1:length(tree_piece$return_pieces_list))#
		node_counts = c(node_counts, nodes_sum)#
		branchlength_sums = c(branchlength_sums, branchlength_sum)	#
		} # END for (i in 1:length(tree_pieces))#
	time_tops = c(0.0, timeperiods[1:(length(timeperiods)-1)])#
	time_bots = timeperiods#
	tmpmat = cbind(time_tops, time_bots, node_counts, branchlength_sums)#
	counts_df = as.data.frame(tmpmat, stringsAsFactors=FALSE)#
	names(counts_df) = c("time_tops", "time_bots", "node_counts", "branchlength_sums")#
	counts_df#
	} # END sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
#
timeperiods = c(0.5, 1.9, 3.7, 5.1, 10)#
trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
tr = read.tree(trfn)#
counts_df = sum_nodes_branchlengths_by_timeperiod(tr, timeperiods)#
counts_df#
#
sum(tr$edge.length)#
sum(counts_df$branchlength_sums)#
tr$Nnode#
sum(counts_df$node_counts)
sum(counts_df$branchlength_sums)-sum(tr$edge.length)
timeperiods
time_tops = c(0.0, timeperiods[1:(length(timeperiods)-1)])
time_tops
tmptr$root.edge
# Check if "tr" is useable#
	if (("phylo" %in% class(tr)) == TRUE)#
		{#
		trfn = "temp_tree.newick"#
		write.tree(tr, file=trfn)#
		} else if (("character" %in% class(tr)) == TRUE) {#
		trfn = tr#
		} else {#
		txt = "STOP ERROR in sum_nodes_branchlengths_by_timeperiod(): argument 'tr' must be either a 'phylo' tree object, or a 'character' filename."#
		stop(txt)#
		}#
	inputs = NULL#
	inputs$trfn = trfn#
	inputs$timeperiods = timeperiods#
#
	tree_pieces = section_the_tree(inputs, make_master_table=TRUE, plot_pieces=FALSE, cut_fossils=FALSE)#
#
	tree_pieces#
#
	node_counts = NULL#
	branchlength_sums = NULL#
#
	for (i in 1:length(tree_pieces$tree_sections_list))#
		{#
		tree_piece = tree_pieces$tree_sections_list[[i]]#
		branchlength_sum = 0#
		nodes_sum = 0#
		for (j in 1:length(tree_piece$return_pieces_list))#
			{#
			if ("numeric" %in% class(tree_piece$return_pieces_list[[j]]))#
				{#
				branchlength_sum = branchlength_sum + tree_piece$return_pieces_list[[j]]#
				} else {#
				tmptr = tree_piece$return_pieces_list[[j]]#
				nodes_sum = nodes_sum + tmptr$Nnode#
				branchlength_sum = branchlength_sum + sum(tmptr$edge.length) + tmptr$root.edge#
				} # END if ("numeric" %in% class#
			} # END for (j in 1:length(tree_piece$return_pieces_list))#
		node_counts = c(node_counts, nodes_sum)#
		branchlength_sums = c(branchlength_sums, branchlength_sum)	#
		} # END for (i in 1:length(tree_pieces))#
	time_tops = c(0.0, timeperiods[1:(length(timeperiods)-1)])#
	time_bots = timeperiods#
	tmpmat = cbind(time_tops, time_bots, node_counts, branchlength_sums)#
	counts_df = as.data.frame(tmpmat, stringsAsFactors=FALSE)#
	names(counts_df) = c("time_tops", "time_bots", "node_counts", "branchlength_sums")#
	counts_df
node_counts
branchlength_sums
6+NULL
tmptr$root.edge
library(ape)#
library(BioGeoBEARS)#
#
extdata_dir = np(system.file("extdata", package="BioGeoBEARS"))#
#
# Load timeperiods from file#
times_fn = np(paste(addslash(extdata_dir), "examples/Psychotria_M3strat/BGB/timeperiods.txt", sep=""))#
inputs = NULL#
inputs$times_fn = times_fn#
timeperiods = read_times_fn(inputs$times_fn)#
#
# Or set the timeperiods manually#
timeperiods = c(0.5, 1.9, 3.7, 5.1, 10)#
#
# Load tree from file#
trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
tr = read.tree(trfn)#
counts_df = sum_nodes_branchlengths_by_timeperiod(tr, timeperiods)#
counts_df#
sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
	{#
	# Check if "tr" is useable#
	if (("phylo" %in% class(tr)) == TRUE)#
		{#
		trfn = "temp_tree.newick"#
		write.tree(tr, file=trfn)#
		} else if (("character" %in% class(tr)) == TRUE) {#
		trfn = tr#
		} else {#
		txt = "STOP ERROR in sum_nodes_branchlengths_by_timeperiod(): argument 'tr' must be either a 'phylo' tree object, or a 'character' filename."#
		stop(txt)#
		}#
	inputs = NULL#
	inputs$trfn = trfn#
	inputs$timeperiods = timeperiods#
#
	tree_pieces = section_the_tree(inputs, make_master_table=TRUE, plot_pieces=FALSE, cut_fossils=FALSE)#
#
	tree_pieces#
#
	node_counts = NULL#
	branchlength_sums = NULL#
#
	for (i in 1:length(tree_pieces$tree_sections_list))#
		{#
		tree_piece = tree_pieces$tree_sections_list[[i]]#
		branchlength_sum = 0#
		nodes_sum = 0#
		for (j in 1:length(tree_piece$return_pieces_list))#
			{#
			if ("numeric" %in% class(tree_piece$return_pieces_list[[j]]))#
				{#
				branchlength_sum = branchlength_sum + tree_piece$return_pieces_list[[j]]#
				} else {#
				tmptr = tree_piece$return_pieces_list[[j]]#
				nodes_sum = nodes_sum + tmptr$Nnode#
				if (is.null(tmptr$root.edge))#
					{#
					branchlength_sum = branchlength_sum + sum(tmptr$edge.length) + 0.0#
					} else {#
					branchlength_sum = branchlength_sum + sum(tmptr$edge.length) + tmptr$root.edge#
					}#
				} # END if ("numeric" %in% class#
			} # END for (j in 1:length(tree_piece$return_pieces_list))#
		node_counts = c(node_counts, nodes_sum)#
		branchlength_sums = c(branchlength_sums, branchlength_sum)	#
		} # END for (i in 1:length(tree_pieces))#
	time_tops = c(0.0, timeperiods[1:(length(timeperiods)-1)])#
	time_bots = timeperiods#
	tmpmat = cbind(time_tops, time_bots, node_counts, branchlength_sums)#
	counts_df = as.data.frame(tmpmat, stringsAsFactors=FALSE)#
	names(counts_df) = c("time_tops", "time_bots", "node_counts", "branchlength_sums")#
	counts_df#
	} # END sum_nodes_branchlengths_by_timeperiod <- function(tr, timeperiods)#
#
timeperiods = c(0.5, 1.9, 3.7, 5.1, 10)#
trfn = np(paste(addslash(extdata_dir), "Psychotria_5.2.newick", sep=""))#
tr = read.tree(trfn)#
counts_df = sum_nodes_branchlengths_by_timeperiod(tr, timeperiods)#
counts_df#
#
sum(tr$edge.length)#
sum(counts_df$branchlength_sums)#
tr$Nnode#
sum(counts_df$node_counts)
res
# Load the package (after installation, see above).#
library(optimx)   # optimx seems better than R's default optim()#
library(GenSA)    # GenSA seems better than optimx (but slower) on 5+ parameters, #
                  # seems to sometimes fail on simple problems (2-3 parameters)#
library(FD)       # for FD::maxent() (make sure this is up-to-date)#
library(snow)     # (if you want to use multicore functionality; some systems/R versions prefer library(parallel), try either)#
library(parallel)#
#
########################################################
# 2018-10-10 update: I have been putting the #
# updates on CRAN/GitHub#
# You should use:#
# rexpokit version 0.26.6 from CRAN#
# cladoRcpp version 0.15 from CRAN#
# BioGeoBEARS version 1.1 from GitHub, install with:#
# library(devtools)#
# devtools::install_github(repo="nmatzke/BioGeoBEARS")#
########################################################
library(rexpokit)#
library(cladoRcpp)#
library(BioGeoBEARS)#
#
########################################################
# CUT: The old instructions to source() online upgrade .R files have been deleted,#
#         all updates are now on the GitHub version of the package, version 1.1+#
########################################################
#
########################################################
# (This local-sourcing is mostly useful for Nick, while actively developing)#
# Local source()-ing method -- uses BioGeoBEARS sourceall() function #
# on a directory of .R files, so you don't have to type them out.#
# The directories here are on my machine, you would have to make a #
# directory, save the .R files there, and refer to them.#
##
# NOTE: it's best to source the "cladoRcpp.R" update first, to avoid warnings like this:#
###
## Note: possible error in 'rcpp_calc_anclikes_sp_COOweights_faster(Rcpp_leftprobs = tmpca_1, ': #
##         unused arguments (m = m, m_null_range = include_null_range, jts_matrix = jts_matrix) #
###
##
# TO USE: Delete or comment out the 'source("http://...")' commands above, and un-comment#
#              the below...#
#########################################################################
# Un-comment (and fix directory paths) to use:#
#library(BioGeoBEARS)#
#source("/drives/Dropbox/_njm/__packages/cladoRcpp_setup/cladoRcpp.R")#
#sourceall("/drives/Dropbox/_njm/__packages/BioGeoBEARS_setup/")#
#calc_loglike_sp = compiler::cmpfun(calc_loglike_sp_prebyte)    # crucial to fix bug in uppass calculations#
#calc_independent_likelihoods_on_each_branch = compiler::cmpfun(calc_independent_likelihoods_on_each_branch_prebyte)#
#########################################################################
#
########################################################
# SETUP: YOUR WORKING DIRECTORY#
########################################################
# You will need to set your working directory to match your local system#
#
# Note these very handy functions!#
# Command "setwd(x)" sets your working directory#
# Command "getwd()" gets your working directory and tells you what it is.#
# Command "list.files()" lists the files in your working directory#
# To get help on any command, use "?".  E.g., "?list.files"#
#
# Set your working directory for output files#
# default here is your home directory ("~")#
# Change this as you like#
wd = "/GitHub/PhyBEARS.jl/test/apes_SSE/"#
setwd(wd)#
#
# Double-check your working directory with getwd()#
getwd()#
#
########################################################
# SETUP: Extension data directory#
########################################################
# When R packages contain extra files, they are stored in the "extdata" directory #
# inside the installed package.#
##
# BioGeoBEARS contains various example files and scripts in its extdata directory.#
# #
# Each computer operating system might install BioGeoBEARS in a different place, #
# depending on your OS and settings. #
# #
# However, you can find the extdata directory like this:#
extdata_dir = np(system.file("extdata", package="BioGeoBEARS"))#
extdata_dir#
list.files(extdata_dir)#
#
# "system.file" looks in the directory of a specified package (in this case BioGeoBEARS)#
# The function "np" is just a shortcut for normalizePath(), which converts the #
# path to the format appropriate for your system (e.g., Mac/Linux use "/", but #
# Windows uses "\\", if memory serves).#
#
# Even when using your own data files, you should KEEP these commands in your #
# script, since the plot_BioGeoBEARS_results function needs a script from the #
# extdata directory to calculate the positions of "corners" on the plot. This cannot#
# be made into a straight up BioGeoBEARS function because it uses C routines #
# from the package APE which do not pass R CMD check for some reason.#
#
########################################################
# SETUP: YOUR TREE FILE AND GEOGRAPHY FILE#
########################################################
# Example files are given below. To run your own data,#
# make the below lines point to your own files, e.g.#
# trfn = "/mydata/frogs/frogBGB/tree.newick"#
# geogfn = "/mydata/frogs/frogBGB/geog.data"#
#
########################################################
# Phylogeny file#
# Notes: #
# 1. Must be binary/bifurcating: no polytomies#
# 2. No negative branchlengths (e.g. BEAST MCC consensus trees sometimes have negative branchlengths)#
# 3. Be careful of very short branches, as BioGeoBEARS will interpret ultrashort branches as direct ancestors#
# 4. You can use non-ultrametric trees, but BioGeoBEARS will interpret any tips significantly below the #
#    top of the tree as fossils!  This is only a good idea if you actually do have fossils in your tree,#
#    as in e.g. Wood, Matzke et al. (2013), Systematic Biology.#
# 5. The default settings of BioGeoBEARS make sense for trees where the branchlengths are in units of #
#    millions of years, and the tree is 1-1000 units tall. If you have a tree with a total height of#
#    e.g. 0.00001, you will need to adjust e.g. the max values of d and e, or (simpler) multiply all#
#    your branchlengths to get them into reasonable units.#
# 6. DON'T USE SPACES IN SPECIES NAMES, USE E.G. "_"#
########################################################
# This is the example Newick file for Hawaiian apes#
# (from Ree & Smith 2008)#
# "trfn" = "tree file name"#
trfn = "tree.newick"#
#
# Look at the raw Newick file:#
moref(trfn)#
#
# Look at your phylogeny (plots to a PDF, which avoids issues with multiple graphics in same window):#
pdffn = "tree.pdf"#
pdf(file=pdffn, width=9, height=12)#
#
tr = read.tree(trfn)#
tr#
plot(tr)#
title("Example apes phylogeny")#
axisPhylo() # plots timescale#
#
dev.off()#
cmdstr = paste0("open ", pdffn)#
system(cmdstr)#
#
########################################################
# Geography file#
# Notes:#
# 1. This is a PHYLIP-formatted file. This means that in the #
#    first line, #
#    - the 1st number equals the number of rows (species)#
#    - the 2nd number equals the number of columns (number of areas)#
#    - after a tab, put the areas in parentheses, with spaces: (A B C D)#
##
# 1.5. Example first line:#
#    10    4    (A B C D)#
# #
# 2. The second line, and subsequent lines:#
#    speciesA    0110#
#    speciesB    0111#
#    speciesC    0001#
#         ...#
# #
# 2.5a. This means a TAB between the species name and the area 0/1s#
# 2.5b. This also means NO SPACE AND NO TAB between the area 0/1s.#
# #
# 3. See example files at:#
#    http://phylo.wikidot.com/biogeobears#files#
# #
# 4. Make you understand what a PLAIN-TEXT EDITOR is:#
#    http://phylo.wikidot.com/biogeobears#texteditors#
##
# 3. The PHYLIP format is the same format used for C++ LAGRANGE geography files.#
##
# 4. All names in the geography file must match names in the phylogeny file.#
##
# 5. DON'T USE SPACES IN SPECIES NAMES, USE E.G. "_"#
##
# 6. Operational taxonomic units (OTUs) should ideally be phylogenetic lineages, #
#    i.e. genetically isolated populations.  These may or may not be identical #
#    with species.  You would NOT want to just use specimens, as each specimen #
#    automatically can only live in 1 area, which will typically favor DEC+J #
#    models.  This is fine if the species/lineages really do live in single areas,#
#    but you wouldn't want to assume this without thinking about it at least. #
#    In summary, you should collapse multiple specimens into species/lineages if #
#    data indicates they are the same genetic population.#
#######################################################
#
# This is the example geography file for Hawaiian apes#
# (from Ree & Smith 2008)#
geogfn = "geog.data"#
#
# Look at the raw geography text file:#
moref(geogfn)#
#
# Look at your geographic range data:#
tipranges = getranges_from_LagrangePHYLIP(lgdata_fn=geogfn)#
tipranges#
#
# Maximum range size observed:#
max(rowSums(dfnums_to_numeric(tipranges@df)))#
#
# Set the maximum number of areas any species may occupy; this cannot be larger #
# than the number of areas you set up, but it can be smaller.#
max_range_size = 2#
#
#####################################################
#####################################################
# KEY HINT: The number of states (= number of different possible geographic ranges)#
# depends on (a) the number of areas and (b) max_range_size.#
# If you have more than about 500-600 states, the calculations will get REALLY slow,#
# since the program has to exponentiate a matrix of e.g. 600x600.  Often the computer#
# will just sit there and crunch, and never get through the calculation of the first#
# likelihood.#
# #
# (this is also what is usually happening when LAGRANGE hangs: you have too many states!)#
##
# To check the number of states for a given number of ranges, try:#
numstates_from_numareas(numareas=4, maxareas=4, include_null_range=TRUE)#
numstates_from_numareas(numareas=4, maxareas=4, include_null_range=FALSE)#
numstates_from_numareas(numareas=4, maxareas=3, include_null_range=TRUE)#
numstates_from_numareas(numareas=4, maxareas=2, include_null_range=TRUE)#
#
# Large numbers of areas have problems:#
numstates_from_numareas(numareas=10, maxareas=10, include_null_range=TRUE)#
#
# ...unless you limit the max_range_size:#
numstates_from_numareas(numareas=10, maxareas=2, include_null_range=TRUE)#
#####################################################
#####################################################
#
########################################################
########################################################
# DEC AND DEC+J ANALYSIS#
########################################################
########################################################
# NOTE: The BioGeoBEARS "DEC" model is identical with #
# the Lagrange DEC model, and should return identical#
# ML estimates of parameters, and the same #
# log-likelihoods, for the same datasets.#
##
# Ancestral state probabilities at nodes will be slightly #
# different, since BioGeoBEARS is reporting the #
# ancestral state probabilities under the global ML#
# model, and Lagrange is reporting ancestral state#
# probabilities after re-optimizing the likelihood#
# after fixing the state at each node. These will #
# be similar, but not identical. See Matzke (2014),#
# Systematic Biology, for discussion.#
##
# Also see Matzke (2014) for presentation of the #
# DEC+J model.#
########################################################
########################################################
#
########################################################
########################################################
#
########################################################
# Run DEC#
########################################################
#
# Intitialize a default model (DEC model)#
BioGeoBEARS_run_object = define_BioGeoBEARS_run()#
#
# Give BioGeoBEARS the location of the phylogeny Newick file#
BioGeoBEARS_run_object$trfn = trfn#
#
# Give BioGeoBEARS the location of the geography text file#
BioGeoBEARS_run_object$geogfn = geogfn#
#
# Input the maximum range size#
BioGeoBEARS_run_object$max_range_size = max_range_size#
#
BioGeoBEARS_run_object$min_branchlength = 0.000001    # Min to treat tip as a direct ancestor (no speciation event)#
BioGeoBEARS_run_object$include_null_range = TRUE    # set to FALSE for e.g. DEC* model, DEC*+J, etc.#
# (For DEC* and other "*" models, please cite: Massana, Kathryn A.; Beaulieu, #
#  Jeremy M.; Matzke, Nicholas J.; OMeara, Brian C. (2015). Non-null Effects of #
#  the Null Range in Biogeographic Models: Exploring Parameter Estimation in the #
#  DEC Model. bioRxiv,  http://biorxiv.org/content/early/2015/09/16/026914 )#
# Also: search script on "include_null_range" for other places to change#
#
# Set up a time-stratified analysis:#
# 1. Here, un-comment ONLY the files you want to use.#
# 2. Also un-comment "BioGeoBEARS_run_object = section_the_tree(...", below.#
# 3. For example files see (a) extdata_dir, #
#  or (b) http://phylo.wikidot.com/biogeobears#files#
#  and BioGeoBEARS Google Group posts for further hints)#
##
# Uncomment files you wish to use in time-stratified analyses:#
#BioGeoBEARS_run_object$timesfn = "timeperiods.txt"#
#BioGeoBEARS_run_object$dispersal_multipliers_fn = "manual_dispersal_multipliers.txt"#
#BioGeoBEARS_run_object$areas_allowed_fn = "areas_allowed.txt"#
#BioGeoBEARS_run_object$areas_adjacency_fn = "areas_adjacency.txt"#
#BioGeoBEARS_run_object$distsfn = "distances_matrix.txt"#
# See notes on the distances model on PhyloWiki's BioGeoBEARS updates page.#
#
# Speed options and multicore processing if desired#
BioGeoBEARS_run_object$on_NaN_error = -1e50    # returns very low lnL if parameters produce NaN error (underflow check)#
BioGeoBEARS_run_object$speedup = TRUE          # shorcuts to speed ML search; use FALSE if worried (e.g. >3 params)#
BioGeoBEARS_run_object$use_optimx = TRUE    # if FALSE, use optim() instead of optimx();#
# if "GenSA", use Generalized Simulated Annealing, which seems better on high-dimensional#
# problems (5+ parameters), but seems to sometimes fail to optimize on simple problems#
BioGeoBEARS_run_object$num_cores_to_use = 1#
# (use more cores to speed it up; this requires#
# library(parallel) and/or library(snow). The package "parallel" #
# is now default on Macs in R 3.0+, but apparently still #
# has to be typed on some Windows machines. Note: apparently #
# parallel works on Mac command-line R, but not R.app.#
# BioGeoBEARS checks for this and resets to 1#
# core with R.app)#
#
# Sparse matrix exponentiation is an option for huge numbers of ranges/states (600+)#
# I have experimented with sparse matrix exponentiation in EXPOKIT/rexpokit,#
# but the results are imprecise and so I haven't explored it further.#
# In a Bayesian analysis, it might work OK, but the ML point estimates are#
# not identical.#
# Also, I have not implemented all functions to work with force_sparse=TRUE.#
# Volunteers are welcome to work on it!!#
BioGeoBEARS_run_object$force_sparse = FALSE    # force_sparse=TRUE causes pathology & isn't much faster at this scale#
#
# This function loads the dispersal multiplier matrix etc. from the text files into the model object. Required for these to work!#
# (It also runs some checks on these inputs for certain errors.)#
BioGeoBEARS_run_object = readfiles_BioGeoBEARS_run(BioGeoBEARS_run_object)#
#
# Divide the tree up by timeperiods/strata (uncomment this for stratified analysis)#
#BioGeoBEARS_run_object = section_the_tree(inputs=BioGeoBEARS_run_object, make_master_table=TRUE, plot_pieces=FALSE)#
# The stratified tree is described in this table:#
#BioGeoBEARS_run_object$master_table#
#
# Good default settings to get ancestral states#
BioGeoBEARS_run_object$return_condlikes_table = TRUE#
BioGeoBEARS_run_object$calc_TTL_loglike_from_condlikes_table = TRUE#
BioGeoBEARS_run_object$calc_ancprobs = TRUE    # get ancestral states from optim run#
#
# Set up DEC model#
# (nothing to do; defaults)#
#
# Look at the BioGeoBEARS_run_object; it's just a list of settings etc.#
BioGeoBEARS_run_object#
#
# This contains the model object#
BioGeoBEARS_run_object$BioGeoBEARS_model_object#
#
# This table contains the parameters of the model #
BioGeoBEARS_run_object$BioGeoBEARS_model_object@params_table#
#
# Run this to check inputs. Read the error messages if you get them!#
check_BioGeoBEARS_run(BioGeoBEARS_run_object)#
#
# For a slow analysis, run once, then set runslow=FALSE to just #
# load the saved result.#
runslow = TRUE#
resfn = "apes_DEC_M0_unconstrained_v1.Rdata"#
if (runslow)#
    {#
    res = bears_optim_run(BioGeoBEARS_run_object)#
    res    #
#
    save(res, file=resfn)#
    resDEC = res#
    } else {#
    # Loads to "res"#
    load(resfn)#
    resDEC = res#
    }
res=resDEC
names(res)
res$relative_probs_of_each_state_at_branch_bottom_below_node_UPPASS
res$ML_marginal_prob_each_state_at_branch_top_AT_node
res$relative_probs_of_each_state_at_branch_bottom_below_node_DOWNPASS
library(ape)#
#trfn = "tree_small.newick"#
trstr = "((chimp:1,human:1):1,gorilla:2);"#
tr = read.tree(file="", text=trstr)#
# plot(tr)#
# axisPhylo()#
# title("Tree with 3 taxa")#
########################################################
########################################################
# 1. Calculation of tip state likelihoods under DEC, in #
#    BioGeoBEARS#
########################################################
########################################################
library(diversitree)#
library(deSolve)		# for lsoda
library(optimx)#
library(FD)#
library(snow)#
library(parallel)#
library(BioGeoBEARS)
SPCmod <- function(t, x, parms)#
	{#
	with(data=#
	as.list(c(parms, x)), #
		{ # expr#
		import <- sigimp(t)#
		dS <- import - b*S*P + g*C     # substrate#
		dP <- c*S*P  - d*C*P           # producer#
		dC <- e*P*C  - f*C             # consumer#
		res <- c(dS, dP, dC)#
		list(res)#
		}#
	)#
	}#
#
## Parameters #
parms  <- c(b = 0.0, c = 0.1, d = 0.1, e = 0.1, f = 0.1, g = 0.0)#
#
## vector of timesteps#
times  <- seq(0, 100, length = 101)#
#
## external signal with rectangle impulse#
signal <- as.data.frame(list(times = times,#
                            import = rep(0,length(times))))#
#
signal$import[signal$times >= 10 & signal$times <= 11] <- 0.2#
#
sigimp <- approxfun(signal$times, signal$import, rule = 2)#
## Start values for steady state#
y <- xstart <- c(S = 1, P = 1, C = 1)#
#
## Solving#
out <-  lsoda(xstart, times, SPCmod, parms) #
########################################################
########################################################
# 2. Calculation of tree likelihood under a BirthDeath process#
########################################################
########################################################
library(ape)#
#trfn = "tree_small.newick"#
trstr = "((chimp:1,human:1):1,gorilla:2);"#
tr = read.tree(file="", text=trstr)#
plot(tr)#
axisPhylo()#
#
# Get the ML estimates of birthRate (speciation rate) #
# and deathRate (extinction rate)#
# ...using ape::birthdeath#
BD =  birthdeath(tr)#
BD#
names(BD)#
#
# Calculate the birthRate and deathRate from the outputs#
x1 = unname(BD$para["d/b"])#
x2 = unname(BD$para["b-d"])#
deathRate = (x2*x1) / (1-x1)#
birthRate = deathRate+x2#
c(birthRate, deathRate)#
#
# You should get:#
# c(birthRate, deathRate)#
# [1] 0.2222222 0.0000000#
#
# Get the log-likelihood of the tree under the ML parameters#
# Convert the deviance to likelihood#
BD_LnL = -1 * BD$dev / 2#
BD_LnL#
# -3.216395#
# Set birthRate#
#birthRate = 0.22222222222222222222#
########################################################
# Likelihood equation in the birthdeath function#
########################################################
N <- length(tr$tip.label)#
nb_node = tr$Nnode - 1#
sptimes <- c(NA, branching.times(tr)) # NA so the number of times equals number of tips?#
# a = "d/b"#
# r = "b-d"#
#
dev <- function(a=0.1, r=0.2, N, x, return_deviance=FALSE)#
	{#
	if (r < 0 || a > 1) #
		return(1e+100)#
	lnl_topology = lfactorial(tr$Nnode)#
	lnl_numBirths = nb_node * log(r)#
	lnl_Births_above_root = r * sum(sptimes[3:N])#
	lnl_numtips_wOneMinusDeathRate = N * log(1 - a)#
	# Interpretation: more tips are less likely, if relativeDeathRate is >0#
	# If relativeDeathRate = 1, a=0, and lnl=-Inf... #
	#    CLEARLY WRONG EXCEPT IN A MAXIMUM LIKELIHOOD CONTEXT!!!#
	# If relativeDeathRate = 0, a=0, and lnl=0, i.e. any number of tips is equiprobable#
	lnl_branching_times = -2 * sum(log(exp(r * sptimes[2:N]) - a))#
	# For each observed branching,#
	# netDiversificationRate * timeOfEvent <- take exponent of that ; this means recorded events are less likely in the past#
	# <- subtract "a", a constant (relativeDeathRate)#
	##
	# This would be a straight likelihood as:#
	# 1/#
	# (exp(r*branching_time)-a)^2#
	##
	# Sum the logs of these#
	##
	# If deathRate = 0#
	# lnl_branching_times = -2 * sum(log(exp(birthRate*sptimes[2:N]) - 0))#
	# lnl_branching_times = -2 * sum(log( exp(birthRate*sptimes[2:N]) )#
	# lnl_branching_times = -2 * sum( birthRate*sptimes[2:N] )#
	##
	# Note: sum(X) = 9 = total branchlength of tr#
	# In BD:#
	# -2*sum(sptimes[2:N]) = -12#
	# sum(sptimes[3:N]) = 3#
	# So, lnl_branching_times + lnl_Births_above_root = yule's -lambda * X#
	if (return_deviance == TRUE)#
		{#
		result = -2 * (lnl_topology + lnl_numBirths + lnl_Births_above_root + #
			lnl_numtips_wOneMinusDeathRate + lnl_branching_times)#
		} else {#
		result = lnl_topology + lnl_numBirths + lnl_Births_above_root + #
			lnl_numtips_wOneMinusDeathRate + lnl_branching_times#
		}#
	return(result)#
	}#
dev(a=0.1, r=0.2, N=N, x=x)#
# -4.063987#
#
dev(a=x1, r=x2, N=N, x=x)#
# -3.216395#
#
dev(a=deathRate/birthRate, r=birthRate-deathRate, N=N, x=x)#
# -3.216395#
#
dev(a=0/birthRate, r=birthRate-0, N=N, x=x)#
#
birthRate = 0.222222222222#
dev(a=0/birthRate, r=birthRate-0, N=N, x=x)#
S <- rep(1, times=length(tr$tip.label))#
bd.ext(phy=tr, S=S, conditional=TRUE)#
#       d / b = 2.883955e-07   StdErr = NaN #
#       b - d = 0.2656457   StdErr = NaN #
#
bd.ext(phy=tr, S=S, conditional=FALSE) # same than older versions#
#      d / b = 4.949791e-06   StdErr = NaN #
#      b - d = 0.242636   StdErr = NaN #
########################################################
########################################################
# 3. Calculation of tree likelihood under a Yule process#
########################################################
########################################################
yule(phy=tr)#
# $lambda#
# [1] 0.2222222#
# #
# $se#
# [1] 0.1571348#
# #
# $loglik#
# [1] -3.216395#
# #
# attr(,"class")#
# [1] "yule"#
########################################################
# Yule likelihood#
########################################################
yule_lik <- function(tr)#
	{#
	# Total length of tree#
	X <- sum(tr$edge.length)#
#
	# Number of internal nodes#
	tr$Nnode#
	# Number of internal nodes, minus root#
	nb_node <- tr$Nnode - 1#
  # ML estimate of lambda (birthrate)#
  lambda <- nb_node/X#
  # Standard error of ML estimate#
  se <- lambda/sqrt(nb_node)#
  # Log-likelihood#
  lnl_topology = lfactorial(tr$Nnode)#
  lnl_numBirths = nb_node * log(lambda)#
  loglik = -lambda * X + lnl_topology + lnl_numBirths#
  loglik#
  # -3.216395#
  res = NULL#
  res$lambda = lambda#
  res$se = se#
  res$loglik = loglik#
  return(res)#
  }#
#
yule_lik(tr=tr)#
# -3.216395 #
########################################################
########################################################
# 3. Set up of equivalent BiSSE model for DEC, starting values#
########################################################
########################################################
#
# ClaSSE helper functions#
library(diversitree)#
source("/GitHub/PhyBEARS.jl/Rsrc/ClaSSE_mods_v2.R")#
source("/GitHub/PhyBEARS.jl/Rsrc/ClaSSE_functions_v3.R")
states = c(0, 1, 1) # 3 states for 3 tips#
names(states) = tr$tip.label#
states#
# Proportion of species in each state; for 2 states#
# (Let's assume we have all species)#
sampling.f = c(1,1)  # length(sampling.f) = 2 states#
#
# Number of states#
k = 2#
#
# Make the BiSSE likelihood function. #
# (strict=FALSE means that some states in the state space can be absent from the tips)#
bisse_2areas = diversitree::make.bisse(tree=tr, states=states, sampling.f=sampling.f, strict=FALSE)#
#
# Look at all the parameters of this model!#
# lambdas = speciation rates#
# mus = extinction rates#
# qs = anagenetic transition rates#
birthRate = 0.222222222#
deathRate = 1.2#
#
lambda0 = birthRate*2#
lambda1 = birthRate#
mu0 = deathRate#
mu1 = deathRate*2#
q01 = 0.4#
q10 = 0.2#
parms = c(lambda0, lambda1, mu0, mu1, q01, q10)#
names(parms) = c("lambda0", "lambda1", "mu0", "mu1", "q01", "q10")#
parms#
bisse_params = parms
res1 = bisse_2areas(pars=bisse_params, root=ROOT.OBS, root.p=NULL, intermediates=TRUE, condition.surv=FALSE)#
res2 = bisse_2areas(pars=bisse_params, root=ROOT.FLAT, root.p=NULL, intermediates=TRUE, condition.surv=FALSE)#
root_probs = c(0, 1)#
res3 = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=FALSE)#
root_probs = c(0.25, 0.75)#
res4 = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=FALSE)#
root_probs = c(0.5,0.5)#
res5 = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=FALSE)#
res6 = bisse_2areas(pars=bisse_params, root=ROOT.EQUI, root.p=NULL, intermediates=TRUE, condition.surv=FALSE)#
#
# Res1t is BiSSE default#
res1t = bisse_2areas(pars=bisse_params, root=ROOT.OBS, root.p=NULL, intermediates=TRUE, condition.surv=TRUE)#
res2t = bisse_2areas(pars=bisse_params, root=ROOT.FLAT, root.p=NULL, intermediates=TRUE, condition.surv=TRUE)#
root_probs = c(0, 1)#
res3t = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=TRUE)#
root_probs = c(0.25, 0.75)#
res4t = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=TRUE)#
root_probs = c(0.5,0.5)#
res5t = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=TRUE)#
res6t = bisse_2areas(pars=bisse_params, root=ROOT.EQUI, root.p=NULL, intermediates=TRUE, condition.surv=TRUE)#
#
# get_classe_LnLs returns the total log-likelihood, and #
# the total of the branch likelihoods#
LnLs1 = get_classe_LnLs(res1)#
LnLs2 = get_classe_LnLs(res2)#
LnLs3 = get_classe_LnLs(res3)#
LnLs4 = get_classe_LnLs(res4)#
LnLs5 = get_classe_LnLs(res5)#
LnLs6 = get_classe_LnLs(res6)#
LnLs1t = get_classe_LnLs(res1t)#
LnLs2t = get_classe_LnLs(res2t)#
LnLs3t = get_classe_LnLs(res3t)#
LnLs4t = get_classe_LnLs(res4t)#
LnLs5t = get_classe_LnLs(res5t)#
LnLs6t = get_classe_LnLs(res6t)#
#
LnLst = as.data.frame(rbind(LnLs1, LnLs2, LnLs3, LnLs4, LnLs5, LnLs6, LnLs1t, LnLs2t, LnLs3t, LnLs4t, LnLs5t, LnLs6t), stringsAsFactors=FALSE)#
names(LnLst) = c("ttl_LnL", "branch_LnL")#
ObsDiff = (LnLst$ttl_LnL - LnLst$branch_LnL)#
exp_ObsDiff = exp((LnLst$ttl_LnL - LnLst$branch_LnL))#
LnLdiff = round((LnLst$ttl_LnL - LnLst$branch_LnL - log(birthRate)), digits=4)#
exp_LnLdiff = exp((LnLst$ttl_LnL - LnLst$branch_LnL - log(birthRate)))#
LnLst2 = cbind(LnLst, ObsDiff, LnLdiff, exp_ObsDiff, exp_LnLdiff)#
cft(LnLst2, numdigits_inbetween_have_fixed_digits=8)
lik
library(diversitree)#
#
## Start with a simple tree evolved under a BiSSE with all rates#
## asymmetric:#
# lambda0, lambda1, mu0, mu1, q01, q10#
#
# Gives weird result -- green nodes, blue truth#
set.seed(47) # Rare to have a transition, but observed here#
pars <- c(0.222222222, 0.222222222, 0.111111111, 0.222222222, 0.1, 0.05)#
#
# Seems accurate#
pars <- c(0.222222222, 0.222222222, 0.111111111, 0.05, 0.1, 0.15)#
#
set.seed(48) # Rare to have a transition, but observed here#
phy <- trees(pars, "bisse", max.taxa=4, max.t=Inf, x0=0)[[1]]#
cols = c("blue", "green3")#
lower = rep(0.0, times=length(pars))#
upper = rep(10.0, times=length(pars))#
#
## Here is the true history#
h <- history.from.sim.discrete(phy, 0:1)#
diversitree:::plot.history(h, phy, main="True history", cols=cols)#
#
lik <- make.bisse(phy, phy$tip.state)#
bisse_2areas = lik
fit <- find.mle(func=lik, x.init=pars, method="subplex", fail.value=-1e10, lower=lower, upper=upper)#
coef(fit)#
st <- asr.marginal(lik, coef(fit))#
nodelabels(thermo=t(st), piecol=cols, cex=0.5)
res1 = bisse_2areas(pars=bisse_params, root=ROOT.OBS, root.p=NULL, intermediates=TRUE, condition.surv=FALSE)#
res2 = bisse_2areas(pars=bisse_params, root=ROOT.FLAT, root.p=NULL, intermediates=TRUE, condition.surv=FALSE)#
root_probs = c(0, 1)#
res3 = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=FALSE)#
root_probs = c(0.25, 0.75)#
res4 = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=FALSE)#
root_probs = c(0.5,0.5)#
res5 = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=FALSE)#
res6 = bisse_2areas(pars=bisse_params, root=ROOT.EQUI, root.p=NULL, intermediates=TRUE, condition.surv=FALSE)#
#
# Res1t is BiSSE default#
res1t = bisse_2areas(pars=bisse_params, root=ROOT.OBS, root.p=NULL, intermediates=TRUE, condition.surv=TRUE)#
res2t = bisse_2areas(pars=bisse_params, root=ROOT.FLAT, root.p=NULL, intermediates=TRUE, condition.surv=TRUE)#
root_probs = c(0, 1)#
res3t = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=TRUE)#
root_probs = c(0.25, 0.75)#
res4t = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=TRUE)#
root_probs = c(0.5,0.5)#
res5t = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=TRUE)#
res6t = bisse_2areas(pars=bisse_params, root=ROOT.EQUI, root.p=NULL, intermediates=TRUE, condition.surv=TRUE)
library(diversitree)#
source("/GitHub/PhyBEARS.jl/Rsrc/ClaSSE_mods_v2.R")#
source("/GitHub/PhyBEARS.jl/Rsrc/ClaSSE_functions_v3.R")
quadratic.roots
library(diversitree)#
source("/GitHub/PhyBEARS.jl/Rsrc/ClaSSE_mods_v2.R")#
source("/GitHub/PhyBEARS.jl/Rsrc/ClaSSE_functions_v3.R")
res1 = bisse_2areas(pars=bisse_params, root=ROOT.OBS, root.p=NULL, intermediates=TRUE, condition.surv=FALSE)#
res2 = bisse_2areas(pars=bisse_params, root=ROOT.FLAT, root.p=NULL, intermediates=TRUE, condition.surv=FALSE)#
root_probs = c(0, 1)#
res3 = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=FALSE)#
root_probs = c(0.25, 0.75)#
res4 = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=FALSE)#
root_probs = c(0.5,0.5)#
res5 = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=FALSE)#
res6 = bisse_2areas(pars=bisse_params, root=ROOT.EQUI, root.p=NULL, intermediates=TRUE, condition.surv=FALSE)#
#
# Res1t is BiSSE default#
res1t = bisse_2areas(pars=bisse_params, root=ROOT.OBS, root.p=NULL, intermediates=TRUE, condition.surv=TRUE)#
res2t = bisse_2areas(pars=bisse_params, root=ROOT.FLAT, root.p=NULL, intermediates=TRUE, condition.surv=TRUE)#
root_probs = c(0, 1)#
res3t = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=TRUE)#
root_probs = c(0.25, 0.75)#
res4t = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=TRUE)#
root_probs = c(0.5,0.5)#
res5t = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=TRUE)#
res6t = bisse_2areas(pars=bisse_params, root=ROOT.EQUI, root.p=NULL, intermediates=TRUE, condition.surv=TRUE)
# get_classe_LnLs returns the total log-likelihood, and #
# the total of the branch likelihoods#
LnLs1 = get_classe_LnLs(res1)#
LnLs2 = get_classe_LnLs(res2)#
LnLs3 = get_classe_LnLs(res3)#
LnLs4 = get_classe_LnLs(res4)#
LnLs5 = get_classe_LnLs(res5)#
LnLs6 = get_classe_LnLs(res6)#
LnLs1t = get_classe_LnLs(res1t)#
LnLs2t = get_classe_LnLs(res2t)#
LnLs3t = get_classe_LnLs(res3t)#
LnLs4t = get_classe_LnLs(res4t)#
LnLs5t = get_classe_LnLs(res5t)#
LnLs6t = get_classe_LnLs(res6t)#
#
LnLst = as.data.frame(rbind(LnLs1, LnLs2, LnLs3, LnLs4, LnLs5, LnLs6, LnLs1t, LnLs2t, LnLs3t, LnLs4t, LnLs5t, LnLs6t), stringsAsFactors=FALSE)#
names(LnLst) = c("ttl_LnL", "branch_LnL")#
ObsDiff = (LnLst$ttl_LnL - LnLst$branch_LnL)#
exp_ObsDiff = exp((LnLst$ttl_LnL - LnLst$branch_LnL))#
LnLdiff = round((LnLst$ttl_LnL - LnLst$branch_LnL - log(birthRate)), digits=4)#
exp_LnLdiff = exp((LnLst$ttl_LnL - LnLst$branch_LnL - log(birthRate)))#
LnLst2 = cbind(LnLst, ObsDiff, LnLdiff, exp_ObsDiff, exp_LnLdiff)#
cft(LnLst2, numdigits_inbetween_have_fixed_digits=8)
pars <- c(0.222222222, 0.222222222, 0.111111111, 0.222222222, 0.1, 0.05)#
#
# Seems accurate#
#pars <- c(0.222222222, 0.222222222, 0.111111111, 0.05, 0.1, 0.15)#
#
set.seed(48) # Rare to have a transition, but observed here#
phy <- trees(pars, "bisse", max.taxa=4, max.t=Inf, x0=0)[[1]]#
cols = c("blue", "green3")#
lower = rep(0.0, times=length(pars))#
upper = rep(10.0, times=length(pars))#
#
## Here is the true history#
h <- history.from.sim.discrete(phy, 0:1)#
diversitree:::plot.history(h, phy, main="True history", cols=cols)#
#
lik <- make.bisse(phy, phy$tip.state)#
bisse_2areas = lik#
#fit <- find.mle(lik, pars, method="subplex")#
fit <- find.mle(func=lik, x.init=pars, method="subplex", fail.value=-1e10, lower=lower, upper=upper)#
coef(fit)#
st <- asr.marginal(lik, coef(fit))#
nodelabels(thermo=t(st), piecol=cols, cex=0.5)#
res1 = bisse_2areas(pars=bisse_params, root=ROOT.OBS, root.p=NULL, intermediates=TRUE, condition.surv=FALSE)#
res2 = bisse_2areas(pars=bisse_params, root=ROOT.FLAT, root.p=NULL, intermediates=TRUE, condition.surv=FALSE)#
root_probs = c(0, 1)#
res3 = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=FALSE)#
root_probs = c(0.25, 0.75)#
res4 = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=FALSE)#
root_probs = c(0.5,0.5)#
res5 = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=FALSE)#
res6 = bisse_2areas(pars=bisse_params, root=ROOT.EQUI, root.p=NULL, intermediates=TRUE, condition.surv=FALSE)#
#
# Res1t is BiSSE default#
res1t = bisse_2areas(pars=bisse_params, root=ROOT.OBS, root.p=NULL, intermediates=TRUE, condition.surv=TRUE)#
res2t = bisse_2areas(pars=bisse_params, root=ROOT.FLAT, root.p=NULL, intermediates=TRUE, condition.surv=TRUE)#
root_probs = c(0, 1)#
res3t = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=TRUE)#
root_probs = c(0.25, 0.75)#
res4t = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=TRUE)#
root_probs = c(0.5,0.5)#
res5t = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=TRUE)#
res6t = bisse_2areas(pars=bisse_params, root=ROOT.EQUI, root.p=NULL, intermediates=TRUE, condition.surv=TRUE)#
# get_classe_LnLs returns the total log-likelihood, and #
# the total of the branch likelihoods#
LnLs1 = get_classe_LnLs(res1)#
LnLs2 = get_classe_LnLs(res2)#
LnLs3 = get_classe_LnLs(res3)#
LnLs4 = get_classe_LnLs(res4)#
LnLs5 = get_classe_LnLs(res5)#
LnLs6 = get_classe_LnLs(res6)#
LnLs1t = get_classe_LnLs(res1t)#
LnLs2t = get_classe_LnLs(res2t)#
LnLs3t = get_classe_LnLs(res3t)#
LnLs4t = get_classe_LnLs(res4t)#
LnLs5t = get_classe_LnLs(res5t)#
LnLs6t = get_classe_LnLs(res6t)#
#
LnLst = as.data.frame(rbind(LnLs1, LnLs2, LnLs3, LnLs4, LnLs5, LnLs6, LnLs1t, LnLs2t, LnLs3t, LnLs4t, LnLs5t, LnLs6t), stringsAsFactors=FALSE)#
names(LnLst) = c("ttl_LnL", "branch_LnL")#
ObsDiff = (LnLst$ttl_LnL - LnLst$branch_LnL)#
exp_ObsDiff = exp((LnLst$ttl_LnL - LnLst$branch_LnL))#
LnLdiff = round((LnLst$ttl_LnL - LnLst$branch_LnL - log(birthRate)), digits=4)#
exp_LnLdiff = exp((LnLst$ttl_LnL - LnLst$branch_LnL - log(birthRate)))#
LnLst2 = cbind(LnLst, ObsDiff, LnLdiff, exp_ObsDiff, exp_LnLdiff)#
cft(LnLst2, numdigits_inbetween_have_fixed_digits=8)
write.tree(phy, file="")
phy$tip.state
library(diversitree)#
#
## Start with a simple tree evolved under a BiSSE with all rates#
## asymmetric:#
# lambda0, lambda1, mu0, mu1, q01, q10#
#
# Gives weird result -- green nodes, blue truth#
set.seed(47) # Rare to have a transition, but observed here#
pars <- c(0.222222222, 0.222222222, 0.111111111, 0.222222222, 0.1, 0.05)#
#
# Seems accurate#
#pars <- c(0.222222222, 0.222222222, 0.111111111, 0.05, 0.1, 0.15)#
#
set.seed(48) # Rare to have a transition, but observed here#
phy <- trees(pars, "bisse", max.taxa=4, max.t=Inf, x0=0)[[1]]#
write.tree(phy, file="")#
#
cols = c("blue", "green3")#
lower = rep(0.0, times=length(pars))#
upper = rep(10.0, times=length(pars))#
#
## Here is the true history#
h <- history.from.sim.discrete(phy, 0:1)#
diversitree:::plot.history(h, phy, main="True history", cols=cols)#
#
lik <- make.bisse(phy, phy$tip.state)#
phy$tip.state
library(diversitree)#
#
## Start with a simple tree evolved under a BiSSE with all rates#
## asymmetric:#
# lambda0, lambda1, mu0, mu1, q01, q10#
#
# Gives weird result -- green nodes, blue truth#
set.seed(47) # Rare to have a transition, but observed here#
#pars <- c(0.222222222, 0.222222222, 0.111111111, 0.222222222, 0.1, 0.05)#
#
# Seems accurate#
pars <- c(0.222222222, 0.222222222, 0.111111111, 0.05, 0.1, 0.15)#
#
set.seed(48) # Rare to have a transition, but observed here#
phy <- trees(pars, "bisse", max.taxa=4, max.t=Inf, x0=0)[[1]]#
write.tree(phy, file="")#
#
cols = c("blue", "green3")#
lower = rep(0.0, times=length(pars))#
upper = rep(10.0, times=length(pars))#
#
## Here is the true history#
h <- history.from.sim.discrete(phy, 0:1)#
diversitree:::plot.history(h, phy, main="True history", cols=cols)#
#
lik <- make.bisse(phy, phy$tip.state)#
phy$tip.state
library(diversitree)#
#
## Start with a simple tree evolved under a BiSSE with all rates#
## asymmetric:#
# lambda0, lambda1, mu0, mu1, q01, q10#
#
# Gives weird result -- green nodes, blue truth#
set.seed(47) # Rare to have a transition, but observed here#
pars <- c(0.222222222, 0.222222222, 0.111111111, 0.05, 0.1, 0.15)#
#
set.seed(48) # Rare to have a transition, but observed here#
phy <- trees(pars, "bisse", max.taxa=4, max.t=Inf, x0=0)[[1]]#
write.tree(phy, file="")#
#
cols = c("blue", "green3")#
lower = rep(0.0, times=length(pars))#
upper = rep(10.0, times=length(pars))#
#
## Here is the true history#
h <- history.from.sim.discrete(phy, 0:1)#
diversitree:::plot.history(h, phy, main="True history", cols=cols)#
#
lik <- make.bisse(phy, phy$tip.state)#
phy$tip.state#
#
bisse_2areas = lik#
#fit <- find.mle(lik, pars, method="subplex")#
fit <- find.mle(func=lik, x.init=pars, method="subplex", fail.value=-1e10, lower=lower, upper=upper)#
coef(fit)#
st <- asr.marginal(lik, coef(fit))#
nodelabels(thermo=t(st), piecol=cols, cex=0.5)#
res1 = bisse_2areas(pars=bisse_params, root=ROOT.OBS, root.p=NULL, intermediates=TRUE, condition.surv=FALSE)#
res2 = bisse_2areas(pars=bisse_params, root=ROOT.FLAT, root.p=NULL, intermediates=TRUE, condition.surv=FALSE)#
root_probs = c(0, 1)#
res3 = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=FALSE)#
root_probs = c(0.25, 0.75)#
res4 = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=FALSE)#
root_probs = c(0.5,0.5)#
res5 = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=FALSE)#
res6 = bisse_2areas(pars=bisse_params, root=ROOT.EQUI, root.p=NULL, intermediates=TRUE, condition.surv=FALSE)#
#
# Res1t is BiSSE default#
res1t = bisse_2areas(pars=bisse_params, root=ROOT.OBS, root.p=NULL, intermediates=TRUE, condition.surv=TRUE)#
res2t = bisse_2areas(pars=bisse_params, root=ROOT.FLAT, root.p=NULL, intermediates=TRUE, condition.surv=TRUE)#
root_probs = c(0, 1)#
res3t = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=TRUE)#
root_probs = c(0.25, 0.75)#
res4t = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=TRUE)#
root_probs = c(0.5,0.5)#
res5t = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=TRUE)#
res6t = bisse_2areas(pars=bisse_params, root=ROOT.EQUI, root.p=NULL, intermediates=TRUE, condition.surv=TRUE)#
# get_classe_LnLs returns the total log-likelihood, and #
# the total of the branch likelihoods#
LnLs1 = get_classe_LnLs(res1)#
LnLs2 = get_classe_LnLs(res2)#
LnLs3 = get_classe_LnLs(res3)#
LnLs4 = get_classe_LnLs(res4)#
LnLs5 = get_classe_LnLs(res5)#
LnLs6 = get_classe_LnLs(res6)#
LnLs1t = get_classe_LnLs(res1t)#
LnLs2t = get_classe_LnLs(res2t)#
LnLs3t = get_classe_LnLs(res3t)#
LnLs4t = get_classe_LnLs(res4t)#
LnLs5t = get_classe_LnLs(res5t)#
LnLs6t = get_classe_LnLs(res6t)#
#
LnLst = as.data.frame(rbind(LnLs1, LnLs2, LnLs3, LnLs4, LnLs5, LnLs6, LnLs1t, LnLs2t, LnLs3t, LnLs4t, LnLs5t, LnLs6t), stringsAsFactors=FALSE)#
names(LnLst) = c("ttl_LnL", "branch_LnL")#
ObsDiff = (LnLst$ttl_LnL - LnLst$branch_LnL)#
exp_ObsDiff = exp((LnLst$ttl_LnL - LnLst$branch_LnL))#
LnLdiff = round((LnLst$ttl_LnL - LnLst$branch_LnL - log(birthRate)), digits=4)#
exp_LnLdiff = exp((LnLst$ttl_LnL - LnLst$branch_LnL - log(birthRate)))#
LnLst2 = cbind(LnLst, ObsDiff, LnLdiff, exp_ObsDiff, exp_LnLdiff)#
cft(LnLst2, numdigits_inbetween_have_fixed_digits=8)
tr
phy
lik
pars
phy
phy$tip.state
write.tree(phy,file="")
plot(phy)
library(diversitree)#
#
## Start with a simple tree evolved under a BiSSE with all rates#
## asymmetric:#
# lambda0, lambda1, mu0, mu1, q01, q10#
#
# Gives weird result -- green nodes, blue truth#
set.seed(47) # Rare to have a transition, but observed here#
pars <- c(0.222222222, 0.222222222, 0.111111111, 0.05, 0.1, 0.15)#
#
set.seed(48) # Rare to have a transition, but observed here#
phy <- trees(pars, "bisse", max.taxa=4, max.t=Inf, x0=0)[[1]]#
write.tree(phy, file="")#
#
cols = c("blue", "green3")#
lower = rep(0.0, times=length(pars))#
upper = rep(10.0, times=length(pars))#
#
## Here is the true history#
h <- history.from.sim.discrete(phy, 0:1)#
diversitree:::plot.history(h, phy, main="True history", cols=cols)#
#
lik <- make.bisse(phy, phy$tip.state)#
phy$tip.state#
#
bisse_2areas = lik#
#fit <- find.mle(lik, pars, method="subplex")#
fit <- find.mle(func=lik, x.init=pars, method="subplex", fail.value=-1e10, lower=lower, upper=upper)#
coef(fit)#
st <- asr.marginal(lik, coef(fit))#
nodelabels(thermo=t(st), piecol=cols, cex=0.5)#
res1 = bisse_2areas(pars=bisse_params, root=ROOT.OBS, root.p=NULL, intermediates=TRUE, condition.surv=FALSE)#
res2 = bisse_2areas(pars=bisse_params, root=ROOT.FLAT, root.p=NULL, intermediates=TRUE, condition.surv=FALSE)#
root_probs = c(0, 1)#
res3 = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=FALSE)#
root_probs = c(0.25, 0.75)#
res4 = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=FALSE)#
root_probs = c(0.5,0.5)#
res5 = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=FALSE)#
res6 = bisse_2areas(pars=bisse_params, root=ROOT.EQUI, root.p=NULL, intermediates=TRUE, condition.surv=FALSE)#
#
# Res1t is BiSSE default#
res1t = bisse_2areas(pars=bisse_params, root=ROOT.OBS, root.p=NULL, intermediates=TRUE, condition.surv=TRUE)#
res2t = bisse_2areas(pars=bisse_params, root=ROOT.FLAT, root.p=NULL, intermediates=TRUE, condition.surv=TRUE)#
root_probs = c(0, 1)#
res3t = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=TRUE)#
root_probs = c(0.25, 0.75)#
res4t = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=TRUE)#
root_probs = c(0.5,0.5)#
res5t = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=TRUE)#
res6t = bisse_2areas(pars=bisse_params, root=ROOT.EQUI, root.p=NULL, intermediates=TRUE, condition.surv=TRUE)#
# get_classe_LnLs returns the total log-likelihood, and #
# the total of the branch likelihoods#
LnLs1 = get_classe_LnLs(res1)#
LnLs2 = get_classe_LnLs(res2)#
LnLs3 = get_classe_LnLs(res3)#
LnLs4 = get_classe_LnLs(res4)#
LnLs5 = get_classe_LnLs(res5)#
LnLs6 = get_classe_LnLs(res6)#
LnLs1t = get_classe_LnLs(res1t)#
LnLs2t = get_classe_LnLs(res2t)#
LnLs3t = get_classe_LnLs(res3t)#
LnLs4t = get_classe_LnLs(res4t)#
LnLs5t = get_classe_LnLs(res5t)#
LnLs6t = get_classe_LnLs(res6t)#
#
LnLst = as.data.frame(rbind(LnLs1, LnLs2, LnLs3, LnLs4, LnLs5, LnLs6, LnLs1t, LnLs2t, LnLs3t, LnLs4t, LnLs5t, LnLs6t), stringsAsFactors=FALSE)#
names(LnLst) = c("ttl_LnL", "branch_LnL")#
ObsDiff = (LnLst$ttl_LnL - LnLst$branch_LnL)#
exp_ObsDiff = exp((LnLst$ttl_LnL - LnLst$branch_LnL))#
LnLdiff = round((LnLst$ttl_LnL - LnLst$branch_LnL - log(birthRate)), digits=4)#
exp_LnLdiff = exp((LnLst$ttl_LnL - LnLst$branch_LnL - log(birthRate)))#
LnLst2 = cbind(LnLst, ObsDiff, LnLdiff, exp_ObsDiff, exp_LnLdiff)#
cft(LnLst2, numdigits_inbetween_have_fixed_digits=8)
res1t
write.tree(phy)
names(res1t)
res1t
attr(res1t,"intermediates")$init
t(attr(res1t,"intermediates")$init)
phy$tip.state
library(diversitree)#
#
## Start with a simple tree evolved under a BiSSE with all rates#
## asymmetric:#
# lambda0, lambda1, mu0, mu1, q01, q10#
#
# Gives weird result -- green nodes, blue truth#
set.seed(47) # Rare to have a transition, but observed here#
pars <- c(0.222222222, 0.222222222, 0.111111111, 0.05, 0.1, 0.15)#
#
set.seed(48) # Rare to have a transition, but observed here#
phy <- trees(pars, "bisse", max.taxa=4, max.t=Inf, x0=0)[[1]]#
write.tree(phy, file="")#
#
cols = c("blue", "green3")#
lower = rep(0.0, times=length(pars))#
upper = rep(10.0, times=length(pars))#
#
## Here is the true history#
h <- history.from.sim.discrete(phy, 0:1)#
diversitree:::plot.history(h, phy, main="True history", cols=cols)#
#
lik <- make.bisse(phy, phy$tip.state)#
phy$tip.state#
#
bisse_2areas = lik#
#fit <- find.mle(lik, pars, method="subplex")#
fit <- find.mle(func=lik, x.init=pars, method="subplex", fail.value=-1e10, lower=lower, upper=upper)#
coef(fit)#
st <- asr.marginal(lik, coef(fit))#
nodelabels(thermo=t(st), piecol=cols, cex=0.5)#
res1 = bisse_2areas(pars=bisse_params, root=ROOT.OBS, root.p=NULL, intermediates=TRUE, condition.surv=FALSE)#
res2 = bisse_2areas(pars=bisse_params, root=ROOT.FLAT, root.p=NULL, intermediates=TRUE, condition.surv=FALSE)#
root_probs = c(0, 1)#
res3 = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=FALSE)#
root_probs = c(0.25, 0.75)#
res4 = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=FALSE)#
root_probs = c(0.5,0.5)#
res5 = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=FALSE)#
res6 = bisse_2areas(pars=bisse_params, root=ROOT.EQUI, root.p=NULL, intermediates=TRUE, condition.surv=FALSE)#
#
# Res1t is BiSSE default#
res1t = bisse_2areas(pars=bisse_params, root=ROOT.OBS, root.p=NULL, intermediates=TRUE, condition.surv=TRUE)#
res2t = bisse_2areas(pars=bisse_params, root=ROOT.FLAT, root.p=NULL, intermediates=TRUE, condition.surv=TRUE)#
root_probs = c(0, 1)#
res3t = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=TRUE)#
root_probs = c(0.25, 0.75)#
res4t = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=TRUE)#
root_probs = c(0.5,0.5)#
res5t = bisse_2areas(pars=bisse_params, root=ROOT.GIVEN, root.p=root_probs, intermediates=TRUE, condition.surv=TRUE)#
res6t = bisse_2areas(pars=bisse_params, root=ROOT.EQUI, root.p=NULL, intermediates=TRUE, condition.surv=TRUE)#
# get_classe_LnLs returns the total log-likelihood, and #
# the total of the branch likelihoods#
LnLs1 = get_classe_LnLs(res1)#
LnLs2 = get_classe_LnLs(res2)#
LnLs3 = get_classe_LnLs(res3)#
LnLs4 = get_classe_LnLs(res4)#
LnLs5 = get_classe_LnLs(res5)#
LnLs6 = get_classe_LnLs(res6)#
LnLs1t = get_classe_LnLs(res1t)#
LnLs2t = get_classe_LnLs(res2t)#
LnLs3t = get_classe_LnLs(res3t)#
LnLs4t = get_classe_LnLs(res4t)#
LnLs5t = get_classe_LnLs(res5t)#
LnLs6t = get_classe_LnLs(res6t)#
#
LnLst = as.data.frame(rbind(LnLs1, LnLs2, LnLs3, LnLs4, LnLs5, LnLs6, LnLs1t, LnLs2t, LnLs3t, LnLs4t, LnLs5t, LnLs6t), stringsAsFactors=FALSE)#
names(LnLst) = c("ttl_LnL", "branch_LnL")#
ObsDiff = (LnLst$ttl_LnL - LnLst$branch_LnL)#
exp_ObsDiff = exp((LnLst$ttl_LnL - LnLst$branch_LnL))#
LnLdiff = round((LnLst$ttl_LnL - LnLst$branch_LnL - log(birthRate)), digits=4)#
exp_LnLdiff = exp((LnLst$ttl_LnL - LnLst$branch_LnL - log(birthRate)))#
LnLst2 = cbind(LnLst, ObsDiff, LnLdiff, exp_ObsDiff, exp_LnLdiff)#
cft(LnLst2, numdigits_inbetween_have_fixed_digits=8)
